---
layout: distill
title: RLHF - IPL
description: 'A blog for Inverse Preference Learning: Preference-based RL without a Reward Function'
tags: RLHF, Robotics
date: 2024-06-08
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

<d-cite key="IPL"></d-cite>

- 1
- 2
- 3

## Preliminaries

### RL

Contractive Bellman operator

$$
\begin{equation}
\label{eq:bellman}
  (\mathcal{B}^\pi_r Q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot\mid s,a)}[V^\pi(s')],
\end{equation}
$$

## Method

### Inverse Soft-Bellman Operator

$$
\begin{equation}
\label{eq:inverse_bellman}
(\mathcal{T}^\pi Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'\sim p(\cdot\mid s,a)}[V^\pi(s')].
\end{equation}
$$

记 $$r_{Q^\pi}$$ 为使用 $$Q^\pi$$ 导出的隐式奖励函数 (implicit reward function)，即 $$r_{Q^\pi}(s,a) = Q^\pi(s,a)$$，则我们可以得到使用 $$Q^\pi$$ 导出的隐式奖励函数的偏好分布 (preference distribution) $$P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}]$$:

$$
\begin{equation}
\label{eq:q_preference}
P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) }{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) + \exp \sum_t (\mathcal{T}^\pi Q)(s_t^{2}, a_t^{2})}.
\end{equation}
$$

### Optimal Inverse Bellman Operator

$$
\begin{equation}
  (\mathcal{T}^* Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'}[V^{\text{targ}}(s')], \ \text{ where } V^\text{targ}(s) \text{ is estimated as in } \mathcal{B}^*_r.
\end{equation}
$$

### Regularization

$$
\begin{equation}
  \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{1}, \sigma^{2},y \sim \mathcal{D}_p} \left[y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}])\right].
\end{equation}
$$

其中 $$P_{Q^*}$$ 由将 $$\mathcal{T}^*Q$$ 替换到 \eqref{eq:q_preference} 得到。

但是仅仅使用此损失函数会导致较差的结果，因为此目标函数是不受约束的，由于 BT model 对于 shift 是不变的，即对于所有的奖励函数 $$r(s,a)$$，$$Q(s,a)$$ 都是一样的。为了解决这个问题，IPL 引入一个凸正则项 $$\psi(\cdot)$$，对隐式奖励函数 $$r_{Q^\pi} = \mathcal{T^\pi}Q$$ 进行正则化，得到正则化的偏好损失函数：

$$
\begin{equation}
\label{eq:ipl}
\mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{(1)}, \sigma^{(2)},y \sim \mathcal{D}_p} \left[ y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}]) \right] + \lambda \psi(\mathcal{T}^*Q)
\end{equation}
$$



$$
\begin{equation}
\label{eq:loss}
  \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right].
\end{equation}
$$



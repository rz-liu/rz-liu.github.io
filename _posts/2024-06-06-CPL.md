---
layout: distill
title: RLHF - CPL
description: 'A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning'
tags: RLHF, Robotics
date: 2024-06-06
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

<d-cite key="CPL"></d-cite>

- 1
- 2
- 3


## Preliminaries


## Method

### 

$$
\begin{equation}
\label{eq:adv_to_pol1}
  A^*(s,a) = \alpha \log \pi^*(a|s).
\end{equation}
$$

$$
\begin{equation}
\begin{aligned}
  A(s_t,a_t) &= Q(s_t,a_t) - V(s_t) \\
  &= r_t + \gamma V(s_{t+1}) - V(s_t).\\
  \sum_{h=t}^{H-1} A(s_h,a_h) &= \sum_{h=t}^{H-1} \left[ r_h + \gamma V(s_{h+1}) - V(s_h) \right] \\
  &= \sum_{h=t}^{H-1} r_h + \gamma V(s_H) - V(s_t) \\
\end{aligned}
\end{equation}
$$

$$
\begin{equation}
  P_{A^*}\left[\sigma^+ \succ \sigma^- \right] = \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t)}{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha  \log \pi^*(a^-_t|s^-_t)}.
\end{equation}
$$


$$
\begin{equation}
\label{eq:loss}
  \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right].
\end{equation}
$$

### Practical Implementation

Regularization term

$$
\begin{equation}
\label{eq:reg_loss}
  \mathcal{L}_{\text{CPL}({\color{red}{\lambda}})}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{\mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp {\color{red}{\lambda}} \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right].
\end{equation}
$$

Python 实现如下，有点没搞懂为什么要用这个形式，是为了数值稳定性？
```python
def biased_bce_with_logits(adv1, adv2, y, bias=1.0):
    # Apply the log-sum-exp trick.
    # y = 1 if we prefer x2 to x1
    # We need to implement the numerical stability trick.

    logit21 = adv2 - bias * adv1  # (B,)
    logit12 = adv1 - bias * adv2  # (B,)
    max21 = torch.clamp(-logit21, min=0, max=None)  # (B,)
    max12 = torch.clamp(-logit12, min=0, max=None)  # (B,)
    nlp21 = torch.log(torch.exp(-max21) + torch.exp(-logit21 - max21)) + max21  # (B,)
    nlp12 = torch.log(torch.exp(-max12) + torch.exp(-logit12 - max12)) + max12  # (B,)
    loss = y * nlp21 + (1 - y) * nlp12  # (B,)
    loss = loss.mean()

    # Now compute the accuracy
    with torch.no_grad():
        accuracy = ((adv2 > adv1) == torch.round(y)).float().mean()

    return loss, accuracy
```

正常来说，不加这个 trick 的话，应该是这样的：
```python
def biased_bce_with_logits(adv1, adv2, y, bias=1.0):
    # y = 1 if we prefer x2 to x1

    logit21 = adv2 - bias * adv1  # (B,)
    logit12 = adv1 - bias * adv2  # (B,)
    loss = y * torch.log(1 + torch.exp(-logit21)) + (1 - y) * torch.log(1 + torch.exp(-logit12))  # (B,)
    loss = loss.mean()

    # Now compute the accuracy
    with torch.no_grad():
        accuracy = ((adv2 > adv1) == torch.round(y)).float().mean()

    return loss, accuracy
```


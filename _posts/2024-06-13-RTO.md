---
layout: distill
title: RLHF - RTO
description: 'A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF'
tags: RLHF, LLM
date: 2024-06-13
featured: false
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Formulation
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

- 1
- 2
- 3

## Preliminaries

在本章，我们介绍标准的 RLHF 范式。设 $$x \in \mathcal{X}$$ 表示从分布 $$\rho \in \Delta(\mathcal{X})$$ 中采样的提示，$$y= (y_1,y_2,\ldots,y_h,\ldots)$$ 表示由 LLM 生成的回答，其中 $$y_i$$ 表示第 $$i$$ 个 token。在实践中，通常假设 <d-cite key="christiano2017deep"></d-cite> 奖励信号根据 Bradley-Terry (BT) 模型 <d-cite key="BTModel"></d-cite> 生成：

$$
\begin{equation}
\label{eqn:bt}
\begin{aligned}
  P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big),
 \end{aligned}
\end{equation}
$$

其中 $$\sigma(z) = 1/(1+\exp(-z))$$ 是 sigmoid 函数, $$r$$ 是定义在 **sentence-level** 的 ground-truth reward function，用于评价整个回复 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和基于奖励训练 RL。在第一步中，学习者被给定一个数据集 $$\mathcal{D} = \{(x, y^w, y^l)\}$$，其中 $$y^w$$ 表示更好的回复，$$y^l$$ 表示更差的回复。奖励函数通过最大似然估计（MLE）在数据集 $$\mathcal{D}$$ 上学习：

$$
\begin{equation}
\label{eqn:mle_old}
r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big].
\end{equation}
$$

在第二步，优化学习到的奖励 $$r_{\mathrm{MLE}}$$，同时确保更新后的 LLM 不会与参考模型 $$\pi_{\mathrm{ref}}$$ 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 $$r_{\mathrm{MLE}}$$ 进行优化，带有 KL 正则化项：

$$
\begin{equation}
\hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg],
\end{equation}
$$

其中，$$\beta > 0$$ 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 $$x$$ 和一个包含 $$H$$ 个 token 的回复 $$y = y_{1:H}$$，其中 $$H$$ 是 token 的数量，现有的 PPO 开源实现将句子级奖励 $$r_{\mathrm{MLE}}(x, y)$$ 分配给最后一个 token，并优化以下奖励：

$$
\begin{equation}
\label{eq:ppo:reward}
\begin{aligned}
{r}_{\mathrm{ppo}}(x, y_{1:h}) =
\begin{cases}
  {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h \le H-1, \\
  {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h = H,
\end{cases}
\end{aligned}
\end{equation}
$$

其中，$$\pi$$ 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。


## Formulation

### MDP Formulation for RLHF

我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 $$\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)$$，其中 $$\mathcal{S}$$ 是状态空间，$$\mathcal{A}$$ 是动作空间，$$\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$$ 是环境动态，$$r$$ 表示奖励函数，$$\rho$$ 表示初始状态分布，$$H$$ 是交互步数的最大值。MDP 中的策略 $$\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$$ 是从状态到动作分布的映射。环境 $$\mathcal{M}$$ 与代理之间的交互可以描述如下。首先，从初始分布 $$\rho$$ 中抽取起始状态 $$s_1$$。在第 $$h$$ 步，代理观察到状态 $$s_h$$ 并根据其策略选择动作 $$a_h$$。然后，环境从分布 $$\mathcal{P}(\cdot \mid s_h, a_h)$$ 中抽取下一个状态 $$s_{h+1}$$。这种交互会持续到满足某个结束条件，该条件将在 $$H$$ 步内触发。

在大型语言模型（LLM）的标准文本生成过程中，每个状态 $$s_h = (x, y_{1:h-1})$$ 包括提示 $$x$$ 和到目前为止生成的所有响应 token。每个动作 $$a_h = y_{h}$$ 表示词汇表中的一个 token。环境动态 $$\mathcal{P}$$ 通常是已知的且确定的，这意味着给定 tokens $$s_h = (x, y_{1:h-1})$$ 和 $$a_h = y_{h}$$，环境将转移到 $$s_{h+1} = (x, y_{1:h})$$。策略 $$\pi$$ 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 $$h$$，$$\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})$$。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，$$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$ 表示 token-wise 奖励。生成的文本以特殊的句子结束 token $$\texttt{EoS}$$ 结束，该 token 终止生成过程。

在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 $$\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H$$ 和 $$\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H$$，偏好由以下公式计算：

$$
\begin{equation}
\label{eq:BT:mdp}
  P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg).
\end{equation}
$$

相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。


### Learning Objective

与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 $$\pi$$，我们定义其对应的正则化价值函数为

$$
\begin{equation}
\label{eq:q:v}
\begin{aligned}
V_\beta^{\pi}(s; r) &= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg],
\end{aligned}
\end{equation}
$$

其中，期望 $$\mathbb{E}_{\pi}$$ 是针对策略 $$\pi$$ 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 $$H$$，因此 \eqref{eq:q:v} 中的求和最多在 $$H$$ 步结束。在本文的其余部分，我们可能会将 $$\sum_{h=1}^\infty$$ 和 $$\sum_{h=1}^H$$ 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 $$Q_\beta^\pi$$ 是策略 $$\pi$$ 的正则化价值函数的关联函数，定义如下

$$
\begin{equation}
\label{eq:bellman}
Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)],
\end{equation}
$$


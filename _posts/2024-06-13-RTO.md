---
layout: distill
title: RLHF - RTO
description: 'A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF'
tags: RLHF, LLM
date: 2024-06-13
featured: false
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

toc:
  - name: Summary
    subsections:
      - name: Motivation
      - name: Key Idea
      - name: Contributions
  - name: Preliminaries
  - name: Formulation
  - name: Method
  - name: Implementation
  - name: Experiments
  - name: References

_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

Code: 

这篇论文提出了 Reinforced Token Optimization (RTO) <d-cite key="RTO"></d-cite>，使用 MDP 框架解决 RLHF 问题。RTO 算法包括两个主要步骤：(i) token-wise reward learning，其中 RTO 基于偏好数据学习 token-wise 奖励；(ii) optimizing token-wise reward，通过 RL 训练方法（如 PPO）优化 token-wise 奖励。

### Motivation

- 之前的方法使用的是 sentence-level 奖励，这种奖励 sparse，不利于学习。

### Key Idea

- 利用 fine-grained token-wise information 学习奖励函数

### Contributions

1. 
2. 
3. 

## Preliminaries

设 $$x \in \mathcal{X}$$ 表示从分布 $$\rho \in \Delta(\mathcal{X})$$ 中采样的提示 (prompt)，$$y= (y_1,y_2,\ldots,y_h,\ldots)$$ 表示由 LLM 生成的回答，其中 $$y_i$$ 表示第 $$i$$ 个 token。在 RLHF 中<d-cite key="christiano2017deep"></d-cite>，通常使用 Bradley-Terry (BT) 模型<d-cite key="BTModel"></d-cite>建模偏好：

$$
\begin{equation}
\label{eqn:bt}
\begin{aligned}
  P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big),
 \end{aligned}
\end{equation}
$$

其中 $$\sigma(z) = 1/(1+\exp(-z))$$ 是 sigmoid 函数, $$r$$ 是定义在 **sentence-level** 的真实奖励函数 (ground-truth reward function)，用于评价整条回答 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和根据奖励训练 RL。在第一步，需要给定一个数据集 $$\mathcal{D} = \{(x, y^w, y^l)\}$$，其中 $$y^w$$ 表示更好的回答，$$y^l$$ 表示更差的回答。奖励函数通过最大似然估计（MLE）在数据集 $$\mathcal{D}$$ 上学习：

$$
\begin{equation}
\label{eqn:mle_old}
r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big].
\end{equation}
$$

在第二步，优化学习到的奖励 $$r_{\mathrm{MLE}}$$，同时确保更新后的 LLM 不会与参考模型 $$\pi_{\mathrm{ref}}$$ 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 $$r_{\mathrm{MLE}}$$ 进行优化，带有 KL 正则化项：

$$
\begin{equation}
\hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg],
\end{equation}
$$

其中，$$\beta > 0$$ 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 $$x$$ 和一个包含 $$H$$ 个 token 的回答 $$y = y_{1:H}$$，其中 $$H$$ 是 token 的数量，现有的 PPO 开源实现将句子级奖励 $$r_{\mathrm{MLE}}(x, y)$$ 分配给最后一个 token，并优化以下奖励：

$$
\begin{equation}
\label{eq:ppo:reward}
\begin{aligned}
{r}_{\mathrm{ppo}}(x, y_{1:h}) =
\begin{cases}
  {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h \le H-1, \\
  {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h = H,
\end{cases}
\end{aligned}
\end{equation}
$$

其中，$$\pi$$ 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。


## Formulation

### MDP Formulation for RLHF

我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 $$\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)$$，其中 $$\mathcal{S}$$ 是状态空间，$$\mathcal{A}$$ 是动作空间，$$\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$$ 是环境动态，$$r$$ 表示奖励函数，$$\rho$$ 表示初始状态分布，$$H$$ 是交互步数的最大值。MDP 中的策略 $$\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$$ 是从状态到动作分布的映射。环境 $$\mathcal{M}$$ 与代理之间的交互可以描述如下。首先，从初始分布 $$\rho$$ 中抽取起始状态 $$s_1$$。在第 $$h$$ 步，代理观察到状态 $$s_h$$ 并根据其策略选择动作 $$a_h$$。然后，环境从分布 $$\mathcal{P}(\cdot \mid s_h, a_h)$$ 中抽取下一个状态 $$s_{h+1}$$。这种交互会持续到满足某个结束条件，该条件将在 $$H$$ 步内触发。

在大型语言模型（LLM）的标准文本生成过程中，每个状态 $$s_h = (x, y_{1:h-1})$$ 包括提示 $$x$$ 和到目前为止生成的所有响应 token。每个动作 $$a_h = y_{h}$$ 表示词汇表中的一个 token。环境动态 $$\mathcal{P}$$ 通常是已知的且确定的，这意味着给定 tokens $$s_h = (x, y_{1:h-1})$$ 和 $$a_h = y_{h}$$，环境将转移到 $$s_{h+1} = (x, y_{1:h})$$。策略 $$\pi$$ 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 $$h$$，$$\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})$$。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，$$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$ 表示 token-wise 奖励。生成的文本以特殊的句子结束 token $$\texttt{EoS}$$ 结束，该 token 终止生成过程。

在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 $$\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H$$ 和 $$\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H$$，偏好由以下公式计算：

$$
\begin{equation}
\label{eq:BT:mdp}
  P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg).
\end{equation}
$$

相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。


### Learning Objective

与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 $$\pi$$，我们定义其对应的正则化价值函数为

$$
\begin{equation}
\label{eq:q:v}
\begin{aligned}
V_\beta^{\pi}(s; r) &= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg],
\end{aligned}
\end{equation}
$$

其中，期望 $$\mathbb{E}_{\pi}$$ 是针对策略 $$\pi$$ 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 $$H$$，因此 \eqref{eq:q:v} 中的求和最多在 $$H$$ 步结束。在本文的其余部分，我们可能会将 $$\sum_{h=1}^\infty$$ 和 $$\sum_{h=1}^H$$ 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 $$Q_\beta^\pi$$ 是策略 $$\pi$$ 的正则化价值函数的关联函数，定义如下

$$
\begin{equation}
\label{eq:bellman}
Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)],
\end{equation}
$$

where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. 
The regularized optimal policy $\pi_\beta^*$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^*$ and $V_\beta^*$, respectively. By \eqref{eq:bellman}, it can be shown that
\# \label{eq:optimal:policy}
\pi_\beta^*(a \mid s) = \exp\{ (Q_\beta^*(s, a) - V_\beta^*(s))/\beta \}.
\#
Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap:
\# \label{eq:def:subopt}
\SubOpt(\hat{\pi}) = \mathbb{E}_{s \sim \rho} [V_{\beta}^*(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^*(\rho) - V_\beta^{\hat{\pi}}(\rho),
\#
where we use the shorthand $V_\beta^\pi(\rho) = \mathbb{E}_{s \sim \rho}[V_\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure 
    $d^\pi(s) = \mathbb{E}_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \mathbb{E}_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.


### Advantages of Token-Wise MDP over Sentence-Wise Bandit
\label{sec:token:reward}

直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A = |\mathcal{A}|$$ 的确定性 MDP。我们使用自回归策略 $$\pi^*$$ 来表示强大的 LLM 策略，例如 GPT-4。固定提示 $$x$$，给定回答 $$(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)$$，由 $$\pi^*$$ 提供的评估为

$$
\begin{equation}
P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}.
\end{equation}
$$

通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 $$r_s$$ 和 token 级奖励 $$r_t$$ 可以分别由以下公式得到

$$
\begin{equation}
\label{eq:reward:example}
r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}).
\end{equation}
$$

直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 $$r_s(x, y) = \sum_{h = 1}^H r_t( (x, y_{1:h-1}), y_h)$$。


### Method
\label{sec:alg_theory}

Motivated by Section~\ref{sec:formulation}, we tackle RLHF by treating it as an MDP problem. Under this MDP framework, we aim to develop an algorithmic framework that fully utilizes the token-level information. To this end, we develop the Reinforced Token Optimization (\texttt{RTO}) algorithm. At a high level, \texttt{RTO} consists of two main steps: {\color{bluee}(i) token-wise reward learning}, where \texttt{RTO} learns a token-wise reward based on the preference data; and {\color{bluee} (ii) optimizing token-wise reward} through RL training methods such as PPO. In Section~\ref{sec:theory:version}, we provide a theoretically grounded version of \texttt{RTO} with guaranteed sample complexity. To align more closely with practice, we present a practical implementation of \texttt{RTO} in Section~\ref{sec:practical:version}.

\subsection{Theoretical Version with Sample Complexity Guarantee} \label{sec:theory:version}

   
We focus on the offline setting and assume the access to an offline dataset $\cD  = \{(\tau^w, \tau^l)\}$ that contains several trajectory pairs, where $\tau^w = \{(s_h^w, a_h^w)\}_{h=1}^H$ is preferred over $\tau^l = \{(s_h^l, a_h^l)\}_{h=1}^H$. Each pair of trajectories shares the same initial state/prompt (i.e., $s_1^w = s_1^l$), but differs in the subsequent tokens.
    We also assume that the reward function is linear, and our following results are ready to be extended to general function approximation \citep{chen2022human,wang2023rlhf,zhan2023provable}.
    \begin{assumption}[Linear Reward] \label{assumption:linear}
    We assume that the reward function $r$ is linear, i.e., $r(s, a) = \phi(s, a)^\top \theta^*$ for some known feature $\phi: \cS \times \cA \rightarrow \RR^d$ and unknown vector $\theta^* \in \RR^d$. We also assume that $\|\phi(\cdot, \cdot)\|_2 \le L$ and $\| \theta^* \|_2 \le B$.
    \end{assumption}
    Following the standard reward learning pipeline \citep{ouyang2022training}, we learn the reward function via maximum likelihood estimation (MLE). Specifically, if we parametrize the reward function by $\theta$, then the MLE is given by 
    \# \label{eq:mle}
    \theta_{\mle} = \argmax_{\|\theta\|_2 \le B} \cL_{\cD}(\theta), \quad \text{where } \cL_\cD(\theta) = \sum_{(\tau^w, \tau^l) \in \cD} & \bigg[  \log \Big( \sigma  \big(\sum_{h=1}^H r_\theta(s_h^w, a_h^w) - \sum_{h=1}^H r_\theta(s_h^l, a_h^l) \big) \Big)  \bigg].
    \#
    Inspired by previous literature in offline RL \citep{jin2021pessimism,rashidinejad2021bridging,xiong2022nearly,zhu2023principled,zhan2023provable}, given the MLE $\theta_{\mle}$, we construct the pessimistic token-wise reward estimation as
    \# \label{eq:pessimistic:reward}
    \hat{r}(s, a) = \phi(s, a)^\top \theta_{\mle} - \varrho \cdot \|\phi(s, a)\|_{\Sigma_\cD^{-1}},
    \#
    where $\Sigma_\cD = \sum_{(\tau^1, \tau^2) \in \cD} [\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ) (\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ))^\top] + \lambda I_d$, $\lambda > 0$ is a tuning parameter, and $\varrho$ is a problem-dependent coefficient will be specified in Theorem~\ref{thm:offline} and \eqref{eq:varrho}.  Finally, \texttt{RTO} outputs the optimal policy $\hat{\pi}$ with respect to $\hat{r}$, i.e., $\hat{\pi} = \argmax_\pi V_\beta^\pi(s; \hat{r})$ for any $s \in \cS$. The pseudocode of \texttt{RTO} is given in Algorithm~\ref{alg:offline}.
    
   
    


\begin{algorithm}[t]
\caption{Reinforced Token Optimization (Theoretical Version)}
\label{alg:offline}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} Offline dataset $\cD$, $\lambda > 0$, $\beta > 0$, and problem dependent coefficient $\varrho$.
    \STATE Compute $\theta_{\mle}$ based on $\cD$ by maximizing the loglikelihood given in \eqref{eq:mle}. 
    \STATE Calculate the pessimistic reward $\hat{r}$ via \eqref{eq:pessimistic:reward}. {\color{bluee}\COMMENT{token-wise reward learning}}
    \STATE Compute the corresponding optimal policy $\hat{\pi}$ with respect to $\hat{r}$. {\color{bluee}\COMMENT{optimizing token-wise reward}}
    \STATE \textbf{Output:} policy $\hat{\pi}$.
\end{algorithmic}
\end{algorithm}

\begin{theorem} \label{thm:offline}
    Suppose Assumption~\ref{assumption:linear} holds. For $\beta > 0$, $\lambda > 0$, $\delta \in (0, 1)$, if we choose $\varrho = \tilde{\cO}(\sqrt{d})$ (see~\eqref{eq:varrho}), then the output policy $\hat{\pi}$ of Algorithm~\ref{alg:offline} satisfies
    \$
    \SubOpt(\hat{\pi}) \le 2 \varrho \cdot  \mathbb{E}_{(s, a) \sim d^*} \big[\|\phi(s, a) \|_{\Sigma_\cD^{-1}} \big] - \beta \cdot \mathbb{E}_{s \sim d^*} \big[\mathrm{KL}\big(\pi_\beta^*(\cdot \mid s) \| \hat{\pi}(\cdot \mid s) \big) \big].
    \$
\end{theorem}

\begin{proof}
    See Appendix~\ref{appendix:pf:thm:offline} for a detailed proof.
\end{proof}


The first term in Theorem \ref{thm:offline} measures how well the offline dataset covers the trajectory generated by the policy $\pi_\beta^*$. Typically, this term decreases at a rate of $|\cD|^{-1/2}$ under the mild partial coverage assumption~\citep{jin2021pessimism,uehara2021pessimistic,xiong2022nearly,zhu2023principled,zhan2023provable}, where $|\cD|$ is the size of the offline dataset. The second KL term is always negative, and it arises from the goal of learning a regularized value. We also remark that our algorithm relies on the known transition kernel to compute the exact optimal policy with respect to $\hat{r}$. While this is natural in the context of large language models, we provide insights on how to extend our findings to stochastic regularized MDPs and the variant of our \texttt{RTO} algorithm in Appendix~\ref{appendix:variant}.


There have also been previous works \citep{pacchiano2021dueling,chen2022human,wang2023rlhf,li2023reinforcement,zhan2023provable} studying RLHF under the MDP framework, also known as dueling RL and preference-based RL. However, these works do not consider the KL constraint, which is an essential component of RLHF. Furthermore, they do not explicitly emphasize the superiority of the MDP framework over the contextual dueling bandit problem in the context of LLMs, and their proposed algorithms lack practical implementation. In contrast, we will provide a practical implementation of our algorithm, demonstrating the practicality of our approach.




\subsection{Practical Implementation} \label{sec:practical:version}
In this subsection, we shift our focus to developing a practical version of \texttt{RTO}. The key challenge in implementing \texttt{RTO} in Algorithm~\ref{alg:offline} lies in learning the token-wise reward to be optimized from the offline data. In the most popular frameworks outlined in Instruct-GPT \citep{ouyang2022training}, Claude \citep{bai2022training}, and LLaMA2 \citep{touvron2023llama} projects replace the last layer of the LLM with a linear layer for a scalar output and maximize the log-likelihood as in \eqref{eqn:mle_old}. However, this approach gives only a sentence-level reward. To bridge the gap in the literature, we present our practical version of \texttt{RTO} in Algorithm~\ref{alg:offline:practical}, which features a novel calculation of token-wise reward. Our key observation is that, given a trajectory $\tau = \{(s_h, a_h)\}_{h=1}^H$, we have
\# \label{eq:prac:1}
\sum_{h=1}^H \beta \log \frac{\pi_\beta^*(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} &= \sum_{h = 1}^H \big(Q_\beta^*(s_h, a_h) - V_\beta^*(s_h) - \log \pi_{\mathrm{ref}}(a_h \mid s_h) \big) \notag \\
& = \sum_{h = 1}^H r(s_h, a_h) - V_\beta^*(s_1) + \underbrace{\sum_{h = 1}^{H-1} \big( \mathbb{E}_{s' \sim \cP(\cdot \mid s_h, a_h)}[V_\beta^*(s')] - V_\beta^*(s_{h+1}) \big)}_{(\star)},
\# 
where the first equality uses the closed-form of optimal policy $\pi_\beta^*(a \mid s) = \exp\{ (Q_\beta^*(s, a) - V_\beta^*(s))/\beta \}$ in \eqref{eq:optimal:policy}, and the second equality follows from the fact that $Q_\beta^{\pi}(s, a) = r_\beta(s, a) + \mathbb{E}_{s' \sim \cP(\cdot \mid s, a)}[V_\beta^\pi(s')]$ in \eqref{eq:bellman} with $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. We focus on the typical LLM generation scenario where the transition kernel is deterministic. Then we have $(\star) = 0$ in \eqref{eq:prac:1}, yielding that 
\$
\sum_{h = 1}^H r(s_h, a_h) = \sum_{h=1}^H \beta \log \frac{\pi_\beta^*(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} + V_\beta^*(s_1) .
\$
Building upon this result and combining it with the definition of the BT model in \eqref{eq:BT:mdp}, for any trajectory pair $\{\tau^j =  \{(s_{h}^j, a_{h}^j)\}_{h=1}^H\}_{j=1}^2$ satisfying $s_1^1 = s_1^2$, we have
\# \label{eq:prac:2}
    \PP(\tau^1 \succ \tau^2) = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg) = \sigma \bigg( \sum_{h=1}^H \beta \log \frac{\pi_\beta^*(a_h^1 \mid s_h^1)}{\pi_{\mathrm{ref}}(a_h^1 \mid s_h^1)} - \sum_{h=1}^H \beta \log \frac{\pi_\beta^*(a_h^2 \mid s_h^2)}{\pi_{\mathrm{ref}}(a_h^2 \mid s_h^2)} \bigg).
\#
An interesting observation is that, based on the autoregressive nature of policies, \eqref{eq:prac:2} aligns with the learning objective of DPO proposed by \citet{rafailov2023direct}, but under the token-level MDP instead of the sentence-level bandit setup. Similar to the bandit setting where the learning objective is equivalent to a BT model with sentence-wise reward $r^*(x, y) = \beta \log \frac{\pi_{\beta}^*(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$ \citep{rafailov2023direct}, \eqref{eq:prac:2} shows that the learning objective in token-wise MDP equivalents to a BT model with a token-wise reward function 
\# \label{eq:prac:3}
r^*(s_h = (x, y_{1:h-1}), a_h = y_h) = \beta \log \frac{\pi_\beta^*(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} = \beta \log \frac{\pi_\beta^*(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})},
\# 
where $x$ is the prompt, $y_{1:h-1}$ is the tokens generated so far, and $y_h$ is the token chosen at the current step. 
In contrast to the previous PPO implementation with sparse reward in \eqref{eq:ppo:reward}, we will assign the token-wise reward function defined in~\eqref{eq:prac:3} to each step. Formally, for any $h$, we define
\begin{equation}
    \begin{aligned}\label{eq:prac:5}
&\beta_1 \log \frac{\pi_\beta^*(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})}  \\
& \qquad \approx \beta_1 \log \frac{\pi_{\mathrm{dpo}}(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} := r_{\mathrm{rto}}((x, y_{1:h-1}),y_h)
    \end{aligned}
\end{equation}
as the token-wise reward used by \texttt{RTO},
where $\beta_1$ and $\beta_2$ are tuning parameters, and $\pi$ is the current policy to be updated. In the last step of \eqref{eq:prac:5}, we use $\pi_{\mathrm{dpo}}$, the policy learned by DPO, as a proxy for the unknown $\pi_\beta^*$. Finally, we employ PPO to optimize the token-wise reward $r_{\mathrm{rto}}$ in \eqref{eq:prac:5}. 






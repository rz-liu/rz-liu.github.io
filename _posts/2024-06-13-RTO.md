---
layout: distill
title: RLHF - RTO
description: 'A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF'
tags: RLHF, LLM
date: 2024-06-13
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Formulation
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

- 1
- 2
- 3

## Preliminaries

In this section, we introduce the standard RLHF paradigm. Let $x \in \cX$ denote the prompt sampled from a distribution $\rho \in \Delta(\cX)$, and $y= (y_1,y_2,\dots,y_h,\dots)$ be the corresponding response, which is a sequence of tokens generated by LLMs, where $y_i$ represents the $i$-th token. In practice, it is widely assumed \citep{christiano2017deep, ziegler2019fine, bai2022training, ouyang2022training, touvron2023llama} that the preference signal is generated according to the Bradley-Terry (BT) model \citep{bradley1952rank}:

$$
\begin{equation}
\label{eqn:bt}
\begin{aligned}
  P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big),
 \end{aligned}
\end{equation}
$$

其中 $$\sigma(z) = 1/(1+\exp(-z))$$ 是 sigmoid 函数, $$r$$ 是定义在 **sentence-level** 的 ground-truth reward function，用于评价整个回复 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和基于奖励训练 RL。在第一步中，学习者被给定一个数据集 $$\mathcal{D} = \{(x, y^w, y^l)\}$$，其中 $$y^w$$ 表示更好的回复，$$y^l$$ 表示更差的回复。奖励函数通过最大似然估计（MLE）在数据集 $$\mathcal{D}$$ 上学习：

$$
\begin{equation}
\label{eqn:mle_old}
r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big].
\end{equation}
$$

在第二步，优化学习到的奖励 $$r_{\mathrm{MLE}}$$，同时确保更新后的 LLM 不会与参考模型 $$\pi_{\mathrm{ref}}$$ 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 $$r_{\mathrm{MLE}}$$ 进行优化，带有 KL 正则化项：

$$
\begin{equation}
\hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg],
\end{equation}
$$

其中，$$\beta > 0$$ 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 $$x$$ 和一个包含 $$H$$ 个 token 的回复 $$y = y_{1:H}$$，其中 $$H$$ 是 token 的数量，现有的 PPO 开源实现将句子级奖励 $$r_{\mathrm{MLE}}(x, y)$$ 分配给最后一个 token，并优化以下奖励：

$$
\begin{equation}
\label{eq:ppo:reward}
\begin{aligned}
{r}_{\mathrm{ppo}}(x, y_{1:h}) =
\begin{cases}
  {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h \le H-1, \\
  {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} & \text{ if } h = H,
\end{cases}
\end{aligned}
\end{equation}
$$

其中，$$\pi$$ 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。


## Formulation

### MDP Formulation for RLHF

我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 $$\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)$$，其中 $$\mathcal{S}$$ 是状态空间，$$\mathcal{A}$$ 是动作空间，$$\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$$ 是环境动态，$$r$$ 表示奖励函数，$$\rho$$ 表示初始状态分布，$$H$$ 是交互步数的最大值。MDP 中的策略 $$\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$$ 是从状态到动作分布的映射。环境 $$\mathcal{M}$$ 与代理之间的交互可以描述如下。首先，从初始分布 $$\rho$$ 中抽取起始状态 $$s_1$$。在第 $$h$$ 步，代理观察到状态 $$s_h$$ 并根据其策略选择动作 $$a_h$$。然后，环境从分布 $$\mathcal{P}(\cdot \mid s_h, a_h)$$ 中抽取下一个状态 $$s_{h+1}$$。这种交互会持续到满足某个结束条件，该条件将在 $$H$$ 步内触发。

在大型语言模型（LLM）的标准文本生成过程中，每个状态 $$s_h = (x, y_{1:h-1})$$ 包括提示 $$x$$ 和到目前为止生成的所有响应 token。每个动作 $$a_h = y_{h}$$ 表示词汇表中的一个 token。环境动态 $$\mathcal{P}$$ 通常是已知的且确定的，这意味着给定 tokens $$s_h = (x, y_{1:h-1})$$ 和 $$a_h = y_{h}$$，环境将转移到 $$s_{h+1} = (x, y_{1:h})$$。策略 $$\pi$$ 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 $$h$$，$$\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})$$。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，$$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$ 表示 token-wise 奖励。生成的文本以特殊的句子结束 token $$\texttt{EoS}$$ 结束，该 token 终止生成过程。

在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 $$\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H$$ 和 $$\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H$$，偏好由以下公式计算：

$$
\begin{equation}
\label{eq:BT:mdp}
  P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg).
\end{equation}
$$

相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。


### Learning Objective

与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 $$\pi$$，我们定义其对应的正则化价值函数为

$$
\begin{equation}
\label{eq:q:v}
\begin{aligned}
V_\beta^{\pi}(s; r) &= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg],
\end{aligned}
\end{equation}
$$

其中，期望 $$\mathbb{E}_{\pi}$$ 是针对策略 $$\pi$$ 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 $$H$$，因此 \eqref{eq:q:v} 中的求和最多在 $$H$$ 步结束。在本文的其余部分，我们可能会将 $$\sum_{h=1}^\infty$$ 和 $$\sum_{h=1}^H$$ 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 $$Q_\beta^\pi$$ 是策略 $$\pi$$ 的正则化价值函数的关联函数，定义如下

$$
\begin{equation}
\label{eq:bellman}
Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)],
\end{equation}
$$

where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. 
The regularized optimal policy $\pi_\beta^*$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^*$ and $V_\beta^*$, respectively. By \eqref{eq:bellman}, it can be shown that
\# \label{eq:optimal:policy}
\pi_\beta^*(a \mid s) = \exp\{ (Q_\beta^*(s, a) - V_\beta^*(s))/\beta \}.
\#
Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap:
\# \label{eq:def:subopt}
\SubOpt(\hat{\pi}) = \EE_{s \sim \rho} [V_{\beta}^*(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^*(\rho) - V_\beta^{\hat{\pi}}(\rho),
\#
where we use the shorthand $V_\beta^\pi(\rho) = \EE_{s \sim \rho}[V_\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure 
    $d^\pi(s) = \EE_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \EE_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.


### Advantages of Token-Wise MDP over Sentence-Wise Bandit
\label{sec:token:reward}

直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A = |\mathcal{A}|$$ 的确定性 MDP。我们使用自回归策略 $$\pi^*$$ 来表示强大的 LLM 策略，例如 GPT-4。固定提示 $$x$$，给定回复 $$(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)$$，由 $$\pi^*$$ 提供的评估为

$$
\begin{equation}
P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}.
\end{equation}
$$

通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 $$r_s$$ 和 token 级奖励 $$r_t$$ 可以分别由以下公式得到

$$
\begin{equation}
\label{eq:reward:example}
r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}).
\end{equation}
$$

直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 $$r_s(x, y) = \sum_{h = 1}^H r_t( (x, y_{1:h-1}), y_h)$$。


### Method
\label{sec:alg_theory}





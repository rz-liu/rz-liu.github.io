---
layout: distill
title: RLHF - DPO-Q*
description: 'A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)'
tags: RLHF, LLM
date: 2024-06-13
featured: false
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

toc:
  - name: Summary
    subsections:
      - name: Motivation
      - name: Key Idea
      - name: Contributions
  - name: Preliminaries
  - name: Method
  - name: Implementation
  - name: Experiments
  - name: References

_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

Code: 

这篇论文提出了 <d-cite key="DPO-Q*"></d-cite>

### Motivation

- 

### Key Idea

- 

### Contributions

1. 
2. 
3. 

## Preliminaries

在
In this section we first define the per-token MDP for large language models, and then describe how it relates to classic RLHF approaches and direct alignment algorithms, specifically DPO. We operate in the typical RLHF setting where we have a dataset $$\mathcal{D}=\{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N$$ of language prompts $$\mathbf{x}$$ and target answers $$\mathbf{y}$$, which can each individually be broken down into a sequence of tokens, for example $$\mathbf{x}=(x_0, \ldots, x_m)$$, from a fixed discrete vocabulary $$\mathcal{A}$$. Throughout this section we will use the $$\mathbf{x}$$, $$\mathbf{y}$$ notation for the contextual bandit framing where the entire response $$\mathbf{y}$$ is the action, but will use state $$\mathbf{s}$$ and action $$\mathbf{a}$$ notation from RL literature for describing sequences at the token-level.

### The Token-level MDP for Large Language Models

We define the token level MDP as a tuple $$\mathcal{M} = (\mathcal{S}, \mathcal{A}, f, r, \rho_0)$$, where the state space $$\mathcal{S}$$ consists of all tokens generated so far (i.e. $$\mathbf{s}_t=\{x_0, \ldots, x_m, y_0, \ldots, y_t\}$$) and the action space is the vocabulary of tokens $$\mathcal{A}$$. The dynamics $$f$$ are the deterministic transition model between tokens $$f(\mathbf{s},\mathbf{a}) = \mathbf{s} \mid \mathbf{a}$$, where $$\mid$$ is concatenation. The initial state distribution $$\rho_0$$ is a distribution over prompts $$\mathbf{x}$$, where an initial state $$\mathbf{s}_0$$ is comprised of the tokens from $$\mathbf{x}$$. In RLHF, the reward function is learned from human feedback over preferences between responses which we will denote using trajectories $$\tau$$ at the token level. As is typically done \citep{ziegler2020finetuning, stiennon2022learning}, we assume that preference trajectories start at the same state (initial propmpt) and end in a terminal state (\textbf{EOS} token), from which future rewards are zero. 
% While the reward function $$r$$ will ultimately be learned from feedback, we assume that within the token-MDP the reward $$r$$ at terminal states must be zero. Note that this is not a restrictive assumption, as $$r(\mathbf{s}_t, \mathbf{a}_t)$$ can fully represent any reward dependent on the next state $$\mathbf{s}_{t+1}$$ per the deterministic dynamics. 
In this token level MDP, the corresponding Bradley-Terry preference model \cite{bradley1952rankanalysis, christiano2017deep} is 

$$
\begin{equation}\label{eq:dense-bradley-terry}
  p^*(\tau^w \succeq \tau^l)=\frac{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)}{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)+ \exp\left(\sum_{i=1}^M r(\mathbf{s}_i^l, \mathbf{a}_i^l)\right)}.
\end{equation}
$$

which gives the probability that the ``win'' trajectory $$\tau^w$$ of length $$N$$ is preferred to the ``loss'' trajectory $$\tau^l$$ of length $$M$$. Now that we have defined the token level MDP, we can show how it relates to both classic and direct alignment RLHF methods.

### The Classical RLHF Methods

Most classical RLHF approaches \citep{ziegler2020finetuning, bai2022constitutional, ouyang2022training} first learn a reward function from human feedback on prompt and response pairs $$(\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)$$, then optimize it with a policy gradient-based method like PPO \citep{schulman2017proximal} with an entropy-bonus using the following KL-constrained RL objective

<!-- $$
\begin{equation}\label{eq:multi_step_RL}
\max_{\pi_{\theta}}  \E_{a_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_t)}\left[\sum_{t=0}^N \left(r(\mathbf{s}_t, \mathbf{a}_t) - \beta\log\frac{\pi_{\theta}(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}\right) \bigg \mid \mathbf{s}_0 \sim \rho(\mathbf{s}_0) \right]
\end{equation}
$$ -->

$$
\begin{equation}\label{eq:multi_step_RL}
\max_{\pi_{\theta}} \mathbb{E}_{a_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_t)}\left[\sum_{t=0}^T (r(\mathbf{s}_t, \mathbf{a}_t) + \underbrace{\beta\log\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}_{\text{KL penalty}}) + \beta\mathcal{H}(\pi_{\theta}) \mid \mathbf{s}_0\sim\rho(\mathbf{s}_0)\right]
\end{equation}
$$

where $$\pi_{\mathrm{ref}}$$ is a reference policy, often resulting from supervised finetuning, from which the learned policy should not significantly deviate. However, in classic RLHF methods the reward function is learned as a contextual bandit with the preference model

$$
\begin{equation}\label{eq:bandit_pref}
  p^*(\mathbf{y}^w \succeq \mathbf{y}^l) = \frac{\exp r(\mathbf{x},\mathbf{y}^w)} {\exp r(\mathbf{x},\mathbf{y}^w) + \exp r(\mathbf{x},\mathbf{y}^l)}
\end{equation}
$$

and is thus only applied at the final timestep for the last action where $$\mathbf{a}$$ is \textbf{EOS}. In practice the actual reward used in the token-level PPO is 

$$
\begin{equation}\label{eq:token_reward}
  r(\mathbf{s}_t, \mathbf{a}_t) =
  \begin{cases}
    \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), & \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\
    r(\mathbf{x}, \mathbf{y}) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), & \text{if } \mathbf{s}_{t+1}=\mathbf{y} \text{ is terminal} 
  \end{cases}
\end{equation}
$$

in a maximum entropy formulation. This leads to an interesting contradiction where the reward function $$r$$ is treated like a bandit, but the actual RL value function and optimization is done per-token in practice.

### Direct Preference Optimization

Unlike classical RLHF, DPO, as derived in \citet{rafailov2023direct}, stays entirely within the contextual bandits setting entirely and also uses the bandit-based preference model in \cref{eq:bandit_pref}. To circumvent the need for an RL algorithm, DPO uses the well-known closed form solution to the KL-contextual bandit version of the RL problem posed in \cref{eq:multi_step_RL} \citep{ziebart2008maximum, levine2018reinforcement}:

$$
\begin{equation}
  \pi^*(\mathbf{y} \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})}\pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})e^{r(\mathbf{x}, \mathbf{y})}
\end{equation}
$$

where $$\pi^*$$ is the optimal policy and $$Z(\mathbf{x})$$ is the partition function that normalizes it. DPO re-arranges this equation to solve for reward as

$$
\begin{equation}
  r(\mathbf{x},\mathbf{y}) = \beta \log \pi^*(\mathbf{y} \mid \mathbf{x}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x}) - Z(\mathbf{x}).
\end{equation}
$$

Substituting this relationship into the standard binary cross-entropy loss function used for reward modeling yields the DPO loss equation as the partition function $$Z(\mathbf{x})$$ cancels from the Bradley Terry model.

$$
\begin{equation}\label{eq:optimum_model}
  \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\mathrm{ref}}) = -\mathbb{E}_{(\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(\mathbf{y}^w \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^w \mid \mathbf{x})} - \beta \log \frac{\pi_{\theta}(\mathbf{y}^l \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^l \mid \mathbf{x})}\right)\right]
\end{equation}
$$

For brevity we use $$\sigma$$ to denote the logistic function. In the next section, we show how an alternative derivation of DPO can also cast its optimization within the token-level MDP.


## Method

In this section we explore how DPO can theoretically be cast into the token-level MDP, and explore the consequences of doing so. First, we provide a token level derivation of DPO under the assumptions in \cref{section:tokenMDP}. Next, we show that even in the token MDP, DPO is able to fit any reward function in the multi-step Bradley Terry preference model \cref{eq:dense-bradley-terry}. Ultimately, this shows that DPO can potentially be used for more sequential optimization tasks, like multi-turn interactions or even multi-modal generation.

### DPO as a $$Q$$-function in the Token Level MDP

\noindent \textbf{RL in the Token-level MDP.} While the original derivation of DPO relies on the fact that $$Q^*(\mathbf{x}, \mathbf{y}) = r(\mathbf{x}, \mathbf{y})$$, this relationship does not hold in the token-level MDP. To resolve this, we need to develop new mathematical results that will allow us to relate the reward function in the Token-level Bradley Terry model \cref{eq:dense-bradley-terry} to the corresponding optimal policy $$\pi^*$$. In the general maximum entropy RL setting, the fixed point solution of \cref{eq:multi_step_RL} is given by \citep{ziebart2010modeling} as

$$
\begin{equation}\label{eq:policy}
  \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = e^{(Q^*(\mathbf{s}_t, \mathbf{a}_t)-V^*(\mathbf{s}_t))/\beta}
\end{equation}
$$

where $$\pi^*(\mathbf{a} \mid \mathbf{s})$$ is the optimal policy and $$Q^*(\mathbf{s}, \mathbf{a})$$ is the optimal Q-function which models the total future reward from $$(\mathbf{s}, \mathbf{a})$$ under $$\pi^*$$. The optimal value function $$V^*$$ is a function of $$Q^*$$,

$$
\begin{equation}\label{eq:value}
  V^*(\mathbf{s}_t) = \beta\log\int_{\mathcal{A}} e^{Q^*(\mathbf{s}_t, \mathbf{a})/\beta}d\mathbf{a}    
\end{equation}
$$

such that the policy $$\pi^*$$ integrates to one. Unfortunately unlike in the bandits setting this relationship gives us no specific information about the reward function $$r$$ at a single state action pair since the optimal policy optimizes for total future returns as estimated by $$Q$$. To do so, we will need to consider the relationship between $$Q^*$$ and $$r$$. 

\noindent \textbf{From $$r$$ to $$Q^*$$.}
The relationship between future returns and the current timestep is captured by the belmman equaitons which are satisifed by any valid Q-function. We write this below for the optimal policy $$\pi^*$$ under the reward $$r$$ with a KL divergence penalty:

$$
\begin{equation}\label{eq:critic}
  Q^*(\mathbf{s}_t, \mathbf{a}_t) =
  \begin{cases}
    r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) + V^*(\mathbf{s}_{t+1}), & \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\
    r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), & \text{if } \mathbf{s}_{t+1} \text{ is terminal}
  \end{cases}
\end{equation}
$$

We can then rearrange the bellman equation for the optimal $$Q$$-function in terms of the reward. This style of relationship was first explored by \citet{garg2022iqlearn} in imitation learning and later in \citet{hejna2024inverse} for preference-based RL. However, these works \emph{require} the use of a discount factor $$\gamma < 1$$ which is typically not used in RLHF. In the appendix we prove the following  Lemma which shows that this relationship is indeed one-to-one in the token MDP as well.

\begin{lemma} \label{lemma:r_to_q} Under mild assumptions, there is a bijection between reward functions $$r(\mathbf{s}_t, \mathbf{a}_t)$$ and corresponding optimal Q-functions $$Q^*(\mathbf{s}_t, \mathbf{a}_t)$$ in the token MDP.
\end{lemma}

This leads us to a rather interesting conclusion -- that an LLM is \emph{always} the optimal soft Q-functions for \emph{some} reward function in the token MDP. Consider any LLM which outputs logits $$l_\theta$$ and temperature parameter $$\beta$$. As is common practice, we take the sampling policy $$\pi$$ to be the softmax over tokens modulated by temperature parameter $$\beta$$ -- which is precisely \cref{eq:policy} where $$Q^* = l_\theta$$ because the value optimal function $$V^*$$ is precisely $$\beta \log Z(\mathbf{s}_t)$$, normalizing the distribution. The corresponding reward function may not be smooth or well-behaved. Notably, the logits have a free parameter due to the softmax. While this free-parameter results in the same optimal policy per later arguments, it means the sequence of values may not be smooth. The question then becomes how to finetune the LLM such that it is the optimal Q-function for a reward function $$r$$ that aligns with human preferences. To do so, we will complete our derivation of DPO in the token MDP.

\textbf{DPO learns our best estimate of $$Q^*$$.} Now that we have established a bijection between $$r$$ and $$Q^*$$, we can derive a token-level version of DPO to align the implicit reward, induced by the $$Q$$ function represented by the language model, with that of the best estimate of reward, according to Bradley-Terry model in \cref{eq:dense-bradley-terry}. To do so, we need to represent the sum of rewards first in terms of the $$Q$$-function $$Q^*$$, and then in terms of the policy $$\pi^*$$. We complete the first step by inverting the Bellman equation in \cref{eq:critic} and substituting it into the sum of rewards over a trajectory $$\tau = \{\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{a}_{T-1}, \mathbf{s}_T\}$$.

$$
\begin{equation}
\begin{aligned}
  \sum_{t=0}^{T-1}r(\mathbf{s}_t, \mathbf{a}_t) 
  &= \sum_{t=0}^{T-1}\left(Q^*(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) - V^*(\mathbf{s}_{t+1})\right) = \\ 
  &= Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1}Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_{t}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)
\end{aligned}
\end{equation}
$$

The equality follows from $$V^*(\mathbf{s}_T)=0$$ and re-arranging the sum to isolate $$t=0$$. As $$V^*$$ is written entirely in terms of $$Q^*$$ and $$\beta$$ per \cref{eq:value}, we have expressed the sum of return over the sequence just in terms of $$Q^*$$. Next, we exchange $$Q^*$$ for $$\pi^*$$. We can log-linearize \cref{eq:policy} as $$\beta \log \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_t)$$. This is equivalent to stating that the language model probabilities are just the softmax over $$l_\theta = Q^*$$ with temperature $$\beta$$. Continuing from the above, with this substitution we get

$$
\begin{equation}
  = Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid\mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} =V^*(\mathbf{s}_0) + \sum_{t=0}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}
\end{equation}
$$

where the final step results from adding and subtracting $$V^*(\mathbf{s}_0)$$ and applying the substitution again. Now, this representation for the sum of rewards in terms of the optimal policy can be directly substituted into the preference model in \cref{eq:dense-bradley-terry}, where the $$V^*(\mathbf{s}_0)$$ term will cancel just as $$Z(\mathbf{x})$$ did in the original DPO derivation assuming $$\tau^w$$ and $$\tau^l$$ start at the same state $$\mathbf{s}_0$$, giving us the policy-induced preference model

$$
\begin{equation}\label{eq:policy_pref}
  p_{\pi^*}(\tau^w \succeq \tau^l) = \sigma \left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)} - \sum_{t=0}^{M-1} \beta \log \frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right).
\end{equation}
$$

To derive the final DPO loss function, we can take the KL-divergence between the empirical preference model of our dataset $$p_\mathcal{D}$$ and the preference model implied by a learned policy $$p_{\pi_\theta}$$, $$\mathbb{D}_{\mathrm{KL}} (p_\mathcal{D} \mid\mid p_{\pi_\theta})$$. This results in 

$$
\begin{equation}\label{eq:DPO}
  \mathcal{L}(\pi_{\theta}, \mathcal{D}) = -\mathbb{E}_{(\tau_w, \tau_l)\sim \mathcal{D}}\left[\log \sigma\left(\left( \sum_{t=0}^{N-1}\beta \log\frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}\right)- \left( \sum_{t=0}^{M-1}\beta \log\frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right)\right)\right]
\end{equation}
$$

In the next section we demonstrate that DPO can learn any dense reward function in the token-level MDP. 




### Token-Level DPO Can Parameterize Any Dense Reward Function.

In the previous section we derived DPO using the bijection between reward functions and optimal $$Q$$-functions uniquely available in the token-level MDP. An alternative view of DPO casts it as restricting the learned reward function such that it belongs to the class optimal advantage functions $$A^*(\mathbf{s},\mathbf{a}) = Q^*(\mathbf{s},\mathbf{a}) - V^*(\mathbf{s})$$ from which an optimal policy is readily obtained per \cref{eq:policy}. Here we show that this restriction does not limit the class of reward functions we can represent. We begin by expanding the definition of equivalency used in \citet{rafailov2023direct} to the broader class of potential-based reward shaping functions:

\begin{definition}\label{def:equivalence}
Two reward functions $$r(\mathbf{s}_t, \mathbf{a}_t)$$ and $$r'(\mathbf{s}_t, \mathbf{a}_t)$$ are equivalent if there exists a potential function $$\Phi(\mathbf{s})$$, such that $$r'(\mathbf{s}_t, \mathbf{a}_t) =r(\mathbf{s}_t, \mathbf{a}_t) + \Phi(\mathbf{s}_{t+1})  - \Phi(\mathbf{s}_{t})$$.
\end{definition}

In \citet{ng1999policy}'s seminal work, the authors proved that two equivalent reward functions defined per \cref{def:equivalence} have the same optimal policy. By log-linearizing the optimal policy fixed point in \cref{eq:policy} and substituting in the Bellman equation from \cref{eq:critic}, we have

$$
\begin{equation}\label{eq:advantage}
  \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{t+1}) - V^*(\mathbf{s}_{t}).
\end{equation}
$$

This is precisely the optimal advantage function, where $$V^*$$ directly follows the form of a potential shaping function. As also noted by contemporary works, using the advantage as reward preserves the optimal policy \citep{knox2024learning, hejna2024contrastive}. Unlike prior works, however, we demonstrate that this re-parameterization also leads to the same exact preference distribution as $$r$$. 

\begin{theorem}\label{theorem:equiv} Given a reference policy $$\pi_{\mathrm{ref}}$$ and a parameter $$\beta>0$$ all reward classes consistent with the Plackett-Luce (and Bradley-Terry) models in \cref{eq:dense-bradley-terry} can be represented with the a re-parameterization of the form

$$
\begin{equation}\label{eq:reward_param}
  r(\mathbf{s}, \mathbf{a}) = \beta \log \pi(\mathbf{s} \mid \mathbf{a}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{s} \mid \mathbf{a})
\end{equation}
$$

within the token MDP where $$V^*(\mathbf{s}_t) = 0$$ for all terminal states.
\end{theorem}

\begin{proof}
Above we derived the invariance of the optimal policy under the re-parameterization. The preference model can be shown to be invariant by following the same steps used to arrive at \cref{eq:policy_pref} in the last section.
% as $$\sum_{t=0}^{N-1} \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = \sum_{t=0}^{N-1} r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{N}) - V^*(\mathbf{s}_{0}).$$ Assuming we reach a terminal state, $$V^*(\mathbf{s}_{N}) = 0$$ and assuming all responses begin at the same state $$V^*(\mathbf{s}_{0})$$ is cancelled from the preference model. \looseness=-1
\end{proof}
Interestingly, in practice, the potential function $$\Phi(\mathbf{s}_t)$$ represents the free parameter in the logits of the language model. An equal shift along all logits yields the same policy, but different Q-functions and corresponding rewards. The above Theorem proves that all of these are in the same equivalence class and induce the same set of preferences. 

Moreover, this Theorem implies that we can use DPO to learn the optimal policy for any per-token reward function, provided preference queries start at the same state and end at a terminal state. In addition, DPO \emph{always} fits an optimal advantage function for \emph{some} reward which is responsible for credit assignment. Thus, the training data determines how close the learned advantage corresponds to that of the true reward. This is in contrast to methods that estimate the reward function and then additionally employ some policy improvement mechanism. Which algorithm performs better remains largely an open or empirical question. 

The above derivations cast a language model as a Q function in the discrete token-level MDP. While this interpretation does not generally hold in continuous spaces, we can extend many of our results to other specially structured MDPs, like those present in diffusion. See Appendix \ref{appendix:diffusion} for more thorough treatment.




## Implementation

```python

```

## Experiments

---
layout: distill
title: RLHF - SELM
description: 'A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)'
paper_url: https://arxiv.org/abs/2405.19332
tags: RLHF, LLM
date: 2024-06-16
featured: false
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

toc:
  - name: Summary
    subsections:
      - name: Motivation
      - name: Key Idea
      - name: Contributions
  - name: Preliminaries
  - name: Method
  - name: Implementation
  - name: Experiments
  - name: References

_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

Code: 

这篇论文提出了 <d-cite key="SELM"></d-cite>

### Motivation

- Online RLHF 可以主动迭代式探索 out-of-distribution (OOD) 区域，但是随机采样的方式可能会导致采样效率低下，高奖励的区域可能会被忽略，因此需要一种更加高效的采样方式。
- 

### Key Idea

- 加一个 optimism bonus，鼓励 agent 探索未知区域，同时也能够保证 agent 在已知区域的收敛性。

### Contributions

1. 
2. 
3. 

## Preliminaries

### Large Language Models
A language model $$\pi\in\Delta_\mathcal{Y}^\mathcal{X}$$ typically takes the prompt $$x\in\mathcal{X}$$ as input and outputs the response $$y\in\mathcal{Y}$$. Here, $$\mathcal{X}$$ and $$\mathcal{Y}$$ are finite spaces of prompts and responses, respectively. Given the prompt $$x\in\mathcal{X}$$, a discrete probability distribution $$\pi(\cdot \mid x)\in\Delta_\mathcal{Y}$$ is generated, where $$\Delta_\mathcal{Y}$$ is the set of discrete distributions over $$\mathcal{Y}$$. Modern recipes for training LLMs consist of pre-training and post-training procedures, where during pre-training, LLMs learn to predict the next word on a huge and diverse dataset of text sequences in order to understand the underlying patterns and structures of natural language in an unsupervised manner. The post-training procedure aims to align better to end tasks and human preferences with two phases happening in order: Supervised Fine-Tuning (SFT) and human preference alignment. Here, SFT fine-tunes the pre-trained LLM with supervised learning on high-quality data to follow instructions on downstream tasks and obtain a model $$\pi^{\text{SFT}}$$. In the following of this paper, we focus mainly on preference alignment.

\subsection{Reward Modeling and Preference Optimization}
\paragraph{Reinforcement Learning from Human Feedback (RLHF).}
Standard RLHF frameworks consist of learning a reward model and then optimizing the LLM policy using the learned reward. 

Specifically, a point-wise reward $$r(x, y): \mathcal{X}\times\mathcal{Y}\rightarrow \mathcal{R}$$ represents the Elo score \citep{elo1978rating} of the response $$y$$ given the prompt $$x$$. Then the preference distribution can be expressed by the Bradley-Terry model that distinguishes between the preferred response $$y_w$$ and the dispreferred response $$y_l$$ given prompt $$x$$, denoted as $$y_w\succ y_l \mid x$$, using the logistic function $$\sigma$$:

$$
\begin{equation}\label{bt_model}
\begin{aligned}
  p(y_w\succ y_l \mid x) &:= \mathbb{E}_{h}\bigl[\mathbb{1}(h \text{ prefers } y_w \text{ over } y_l \text{ given } x)\bigr] \notag\\
  &\,= \sigma\bigl(r(x, y_w) - r(x, y_l)\bigr) = \frac{\exp\bigl(r(x, y_w)\bigr)}{\exp\bigl(r(x, y_w)\bigr) + \exp\bigl(r(x, y_l)\bigr)},
\end{aligned}
\end{equation}
$$

where $$h$$ denotes the human rater and the expectation is over $$h$$ to account for the randomness of the choices of human raters we ask for their preference. When provided a static dataset of $$N$$ comparisons $$\mathcal{D}=\{x_i, y_{w,i}, y_{l,i}\}_{i=1}^N$$, the parameterized reward model can be learned by minimizing the following logistic regression loss:

$$
\begin{equation}\label{lr_loss}
\mathcal{L}_{\text{lr}}(r; \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\bigl[\log\sigma\bigl(r(x, y_w) - r(x, y_l)\bigr)\bigr].
\end{equation}
$$

Using the learned reward, the LLM policy $$\pi\in\Delta_\mathcal{Y}^\mathcal{X}$$ is optimized with reinforcement learning (RL) to maximize the expected reward while maintaining a small deviation from some base reference policy $$\pi_{\text{ref}}$$, i.e., maximizing the following objective

$$
\begin{equation}\label{rlhf_kl}
\mathcal{J}(\pi; \mathcal{D}) = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi(\cdot \mid x)}\bigl[r(x, y)\bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}),
\end{equation}
$$

where $$\beta$$ is a hyperparameter and $$\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}) := \mathbb{E}_{x\sim\mathcal{D}} [\text{KL}(\pi(\cdot \mid x) \, \mid \mid \pi_{\text{ref}}(\cdot  \mid x))]$$ is the expected Kullback-Leibler (KL) divergence. An ideal $$\pi_{\text{ref}}$$ is the policy that helps mitigate the distribution shift issue \citep{rafailov2024direct,guo2024direct} between the true preference distribution and the policy $$\pi$$ during the off-policy RL training. Since we only have access to the dataset $$\mathcal{D}$$ sampled from the unavailable true preference distribution, $$\pi_{\text{ref}}$$ can be obtained by fine-tuning on the preferred responses in $$\mathcal{D}$$ or simply setting $$\pi_{\text{ref}}=\pi^{\text{SFT}}$$ and performing RLHF based on the SFT model.

### Direct Alignment from Preference

With the motivation to get rid of a separate reward model, which is computationally costly to train, recent works \citep{rafailov2024direct,azar2023general,zhao2023slic,tunstall2023zephyr,ethayarajh2024kto} derived the preference loss as a function of the policy by changing of variables. Among them, DPO \citep{rafailov2024direct} shows that when the BT model in \eqref{bt_model} can perfectly fit the preference, the global optimizers of the RLHF objective in \eqref{rlhf_kl} and the following loss are equivalent:

$$
\begin{equation}
\mathcal{L}_{\text{DPO}}(\pi;\mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\biggl[\log\sigma\biggl(\beta\log\frac{\pi(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta\log\frac{\pi(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\biggr)\biggr].
\end{equation}
$$


## Method

### RM-Free Objective for Active Exploration
\label{sec_der}

In this section, we present several modifications to the optimistically biased objective \eqref{eq_intro} motivated in the introduction. Then we derive an RM-free objective for the LLM policy and analyze how active exploration works by examining its gradient.

First, we consider the equivalence of \eqref{eq_intro}: $$\max_r-\mathcal{L}_{\text{lr}}(r; \mathcal{D}) + \alpha\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]$$, where the inner $$\pi$$ is deterministic when optimal. To account for the change of $$\pi$$ relative to the reference policy $$\pi_{\text{ref}}$$, we introduce two modifications: (1) replacing the optimistic bias term $$\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]$$ with $$\max_\pi\mathbb{E}_{y\sim\pi, y'\sim\pi_{\text{ref}}}[r(x, y)-r(x, y')]$$, and (2) incorporating a KL-divergence loss term between $$\pi$$ and $$\pi_{\text{ref}}$$. These changes ensure that the resulting optimistic RM elicits responses with high potential unknown to the reference policy $$\pi_{\text{ref}}$$ while minimizing the deviation between $$\pi$$ and $$\pi_{\text{ref}}$$.

Formally, for the reward function $$r$$, the bilevel optimization problem with optimism is formulated as:

$$
\begin{equation}\label{our_obj}
\max_r  -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) +\alpha\max_\pi\Biggl(\underbrace{\mathbb{E}_{\substack{x\sim\mathcal{D}_t, y\sim\pi(\cdot \mid x)\\ y'\sim\pi_{\text{ref}}(\cdot \mid x)}}\Bigl[r(x, y) - r(x, y')\Bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}})}_{\mathcal{F}(\pi; r)}\Biggr),
\end{equation}
$$

where $$\mathcal{D}_t=\{x_i, y_{w,i}^t, y_{l,i}^t\}_{i=1}^N$$ is the associated dataset at iteration $$t$$ and $$\mathcal{L}_{\text{lr}}$$ is the logistic regression loss defined in \eqref{lr_loss}. The nested optimization in \eqref{our_obj} can be handled by first solving the inner optimization $$\mathcal{F}(\pi; r)$$ to obtain $$\pi_r$$ that is optimal under $$r$$. The solution is as follows and we defer all the derivations in this section to Appendix \ref{derivation_1}.

$$
\begin{equation}
\pi_r(y \mid x) := \mathop{\mathrm{argmax}}_\pi\mathcal{F}(\pi; r) = \frac{1}{Z(x)}\pi_{\text{ref}}(y \mid x)\exp\bigl(r(x, y) / \beta\bigr),
\end{equation}
$$

where the partition function $$Z(x) = \sum_y\pi_{\text{ref}}(y \mid x)\exp(r(x, y)/\beta)$$. By substituting $$\pi=\pi_r$$ into $$\mathcal{F}(\pi; r)$$, we can rewrite the bilevel objective in \eqref{our_obj} as a single-level one:

$$
\begin{equation}
  \max_r -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) + \alpha\mathcal{F}(\pi_r; r).
\end{equation}
$$

Following the implicit reward formulation in DPO, we reparameterize the reward function with $$\theta\in\Theta$$ as $$\hat{r}_\theta(x, y)=\beta(\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x))$$, which is the optimal solution of \eqref{rlhf_kl} and can express \textit{all} reward classes consistent with the BT model as proved in \citep{rafailov2024direct}. With this change of variable, we obtain the RM-free objective for direct preference alignment with optimism:

$$
\begin{equation}\label{final_obj}
\max_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr].
\end{equation}
$$

We now analyze how this new objective encourages active exploration. Specifically, we derive the gradient of \eqref{final_obj} with respect to $$\theta$$ as

$$
\begin{equation}\label{eq_grad}
\begin{aligned}
  &\underbrace{-\beta\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}_t}\Bigl[\sigma\bigl(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\bigr)\bigl(\nabla_\theta\log\pi_\theta(y_w \mid x) - \nabla_\theta\log\pi_\theta(y_l \mid x)\bigr)\Bigr]}_{\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)} \notag\\
  &\qquad\qquad\qquad\qquad\qquad\qquad - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_\theta(\cdot \mid x)}\bigl[\exp\bigl(-\hat{r}_\theta(x, y)/\beta\bigr)\nabla_\theta\log\pi_\theta(y \mid x)\bigr].
\end{aligned}
\end{equation}
$$

We note that the second line, corresponding to the gradient of the optimism term, decreases the log-likelihood of response $$y$$ generated by $$\pi_\theta$$ that has a low value of $$\exp(-\hat{r}_\theta(x, y)/\beta)$$. Therefore, the added optimism term biases the gradient toward parameter regions that can elicit responses $$y$$ with high implicit reward $$\hat{r}_\theta$$, consistent with our intuition outlined in Figure \ref{urm_illu}.

This also explains why $$\mathbb{E}_{\pi_{\text{ref}}}[\log\pi_\theta]$$ is minimized in our objective \eqref{final_obj}, which is equivalent to maximizing the KL divergence between $$\pi_{\text{ref}}$$ and $$\pi_\theta$$, while the reverse KL in the policy optimization objective \eqref{rlhf_kl} is minimized. For the DPO gradient $$\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)$$, the degree of deviation of policy $$\pi_\theta$$ from $$\pi_{\text{ref}}$$ only affects the preference estimated with $$\hat{r}_\theta$$. In other words, $$\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))$$ is a scalar value and the policy deviation only determines the \textit{step size} of the policy gradient, instead of its \textit{direction}. On the other hand, our added exploration term directly controls the direction of the gradient toward potentially more rewarding areas while still fitting the preference data in $$\mathcal{D}_t$$. As more feedback data is collected iteratively, deviating from the unbiasedly fitted model incurs a higher DPO loss, which ultimately dominates our objective at convergence. This mechanism ensures that the resulting LLM effectively balances between exploring novel responses and exploiting previously observed ones, leading to a more accurate and aligned model.

### Algorithm
\label{sec_algo}

With the optimistically biased objective derived above, the language model can actively generate OOD responses worth exploring. Human or AI feedback follows to reduce the uncertainty in these regions. These two steps are executed iteratively to get a more and more aligned model. 

In practice, we split the offline preference dataset into three portions with equal sizes, one for each iteration. Besides, we use AI rankers, such as external RMs, to provide feedback on the model-generated response and the original chosen, rejected responses. The complete pseudocode of our algorithm, named \textit{Self-Exploring Language Models} (SELM), is outlined in Algorithm \ref{alg_se}.
\begin{algorithm}[H]
\caption{Self-Exploring Language Models (SELM)}
\begin{algorithmic}[1]\label{alg_se}
\REQUIRE Reference model $$\pi_{\text{ref}}$$, preference dataset $$\mathcal{D}$$, online iterations $$T$$, optimism coefficient $$\alpha$$.
\FOR{iteration $$t = 1, 2, \ldots, T$$}
\STATE Set $$\mathcal{D}_{t}$$ as the $$t$$-th portion of $$\mathcal{D}$$ and generate $$y\sim\pi_{\text{ref}}(\cdot \mid x)$$ for each prompt $$x$$ in $$\mathcal{D}_t$$.
\STATE Rank $$\{y, y_w, y_l\}$$ and update $$\mathcal{D}_t$$ to contain the best (chosen) and worst (rejected) responses.
\STATE Train the LLM $$\pi_{\theta_t} = \mathop{\mathrm{argmax}}_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha \mathbb{E}_{x\sim\mathcal{D}_t}[\log\pi_{\theta}(y \mid x)]$$ and let $$\pi_{\text{ref}}=\pi_{\theta_t}$$.
\ENDFOR 
\end{algorithmic}
\end{algorithm}
  

\subsection{Self-Exploration Reduces Indiscriminate Favor of Unseen Extrapolations}
It has been observed recently \citep{rafailov2024r,pal2024smaug,xu2024dpo} that DPO decreases the likelihood of responses generated by the reference policy. It is because for any prompt $$x$$, at convergence when $$\pi_\theta \neq \pi_{\text{ref}}$$, it holds that

$$
\begin{equation}
  \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\hat{r}_\theta(x, y)/\beta\bigr] = \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x)\bigr] = -\text{KL}\bigl(\pi_{\text{ref}}(\cdot \mid x) \mid \mid \pi_\theta(\cdot \mid x)\bigr) < 0,
\end{equation}
$$

while at the beginning of training when $$\pi_\theta = \pi_{\text{ref}}$$, the above terms are zero. Thus, the expected implicit reward $$\hat{r}_\theta$$ as well as the likelihood of $$\pi_\theta$$ will decrease on the reference model's responses. 
This indicates that DPO stimulates a biased distribution favoring unseen extrapolated responses. In the online iterative setting that we consider, the LLM policy generates responses and receives preference feedback alternately, where biasing towards OOD regions may sometimes help discover outstanding novel responses. However, DPO \textit{indiscriminately} favors unseen extrapolations and \textit{passively} explores based purely on the randomness inherent in sampling from the LLM. As a consequence, the vast space of natural language makes it almost impossible to exhaustively explore all the possible responses and identify those that most effectively benefit alignment.

Next, we demonstrate that SELM mitigates this issue by performing guided exploration. Specifically, consider the proposed self-exploration objective in \eqref{final_obj}, which, in addition to the standard DPO loss, also minimizes $$\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]$$. We now investigate how the probability distribution changes with this term incorporated.
\begin{theorem}
\label{thm}
For any $$\rho\in\Theta$$ in the policy parameter space, let $$\hat{r}_\rho(x, y) = \beta(\log\pi_\rho(y \mid x) - \log\pi_{\text{ref}}(y \mid x))$$ be the reparameterized implicit reward. Denote $$\pi^{\min}_\rho$$ as the policy that minimizes the expected implicit reward under the KL constraint, i.e.,

$$
\begin{equation}\label{eq_pi_rho}
\pi^{\min}_\rho(\cdot \mid x) := \mathop{\mathrm{argmin}}_\pi\mathbb{E}_{x, y\sim\pi(\cdot \mid x)}\bigl[\hat{r}_\rho(x, y)\bigr] + \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_\rho).
\end{equation}
$$

Then minimizing $$\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]$$ decreases the likelihood of responses sampled from $$\pi^{\min}_\rho$$:

$$
\begin{equation}
\min_{\pi_\theta}\mathbb{E}_{x, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr] = \min_{\pi_\theta}\mathbb{E}_{x,y\sim\pi^{\min}_\rho(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr].
\end{equation}
$$

\end{theorem}


The above theorem states that maximizing the divergence between $$\pi_\theta$$ and $$\pi_{\text{ref}}$$ is essentially reducing the probability of generating responses with low implicit rewards reparameterized by any policy parameter $$\rho$$ during training. In other words, the policy not only exploits the existing preference data but also learns to avoid generating the text $$y$$ that is assigned a low reward value. This process occurs in every iteration with updated reference models. Consequently, responses with high potential rewards are selectively preferred and many commonplace responses receive a small probability mass, thus mitigating the indiscriminate favoring of unseen responses and improving exploration efficiency.

## Implementation

```python

```

## Experiments

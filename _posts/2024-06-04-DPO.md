---
layout: distill
title: RLHF (1) - DPO
description: 'A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model'
tags: RLHF, LLM
date: 2024-06-04
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

- 1
- 2
- 3

## Preliminaries

### Dataset



### SFT

Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt $$x$$ 和高质量回答 $$y$$ 的数据集，对模型进行微调，使得模型在 prompt $$x$$ 上生成的回答接近 $$y$$，最终得到模型 $$\pi^{\text{SFT}}$$。

### 奖励建模

对于一个输入 $$x$$，经过 SFT 的模型可以产生成对的回答 $$(y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)$$。我们使用 $$y_w$$ 和 $$y_l$$ 分别表示 $$(y_1, y_2)$$ 中更好的回答和更差的回答，则偏好关系可以定义为 $$y_w \succ y_l \mid x$$。假设 ground-truth reward function 为 $$r^*(x, y)$$，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution $$p^*$$ 可以表示为

$$
\begin{equation}
  p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
$$

假设我们有一个偏好数据集 $$\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$$，其中 $$y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}$$，我们可以将 reward learning 建模为一个二分类问题：

$$
\begin{equation}
  \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
$$


### RL Fine-Tuning

使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。

$$
\begin{equation}
  \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi^{\text{ref}}(y \mid x)\bigr]
\end{equation}
$$


## Method

受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 **key insight** 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。

### Deriving the DPO objective

我们从与以前的工作相同的 RL 目标开始，公式如下：
We start with the same RL objective as prior work, Eq.~\ref{eq:RL}, under a general reward function $r$. Following prior work~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\ref{eq:RL} takes the form:

$$
\begin{equation}
\label{eq:op_policy}
\pi_r(y\mid x) = \frac{1}{Z(x)}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
$$

where $Z(x) =\sum_{y}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ is the partition function. See Appendix \ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ \citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\pi_r$, the reference policy $\pi^{\text{ref}}$, and the unknown partition function $Z(\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\ref{eq:op_policy} and then with some algebra we obtain:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi^{\text{ref}}(y\mid x)} + \beta \log Z(x).
\end{equation}
We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\pi^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in Eq.~\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\pi^*$ and reference policy $\pi^{\text{ref}}$. Thus, the optimal RLHF policy $\pi^*$ under the Bradley-Terry model satisfies the preference model:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi^{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi^{\text{ref}}(y_1\mid x)}\right)}
\end{equation}
The derivation is in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\ref{app:plackett_luce_models}.

Now that we have 
the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_\theta$. Analogous to the reward modeling approach (i.e. Eq.~\ref{eq:reward_model}), our policy objective becomes:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi^{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi^{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi^{\text{ref}}(y_l\mid x)}\right)\right].
\end{equation}
\rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.

\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the parameters $\theta$ can be written as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi^{\text{ref}}) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$}\bigg]\bigg],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi^{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi^{\text{ref}}$ (more in Section~\ref{sec:theory}). Intuitively, the gradient of the loss function $\mathcal{L}_\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}_\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{DPO outline.} 
The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \sim \pi^{\text{ref}}(\cdot \mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\}_{i=1}^N$ and 2) optimize the language model $\pi_\theta$ to minimize $\mathcal{L}_\text{DPO}$ for the given $\pi^{\text{ref}}$ and $\mathcal{D}$ and desired $\beta$. 
In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pisft$, we initialize $\pi^{\text{ref}} = \pisft$ whenever available. However, when $\pisft$ is not available, we initialize $\pi^{\text{ref}}$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\pi^{\text{ref}} = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi^{\text{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\ref{app:implementation}.
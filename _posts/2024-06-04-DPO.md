---
layout: distill
title: RLHF - DPO
description: 'A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)'
tags: RLHF, LLM
date: 2024-06-04
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

toc:
  - name: Summary
    subsections:
      - name: Motivation
      - name: Key Idea
      - name: Contributions
  - name: Preliminaries
  - name: Method
  - name: Implementation
  - name: Experiments
  - name: References

_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

这篇论文提出了 Direct Preference Optimization (DPO) <d-cite key="DPO"></d-cite>，使用偏好数据直接优化策略，而不需要先学奖励模型再学策略。

### Motivation

- 之前的方法流程复杂，先学 reward model，再学 policy，效率低
- RL (PPO) 难以训练

### Key Idea



### Contributions

1. D
2. D
3. D

## Preliminaries

### Dataset



### SFT

Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt $$x$$ 和高质量回答 $$y$$ 的数据集，对模型进行微调，使得模型在 prompt $$x$$ 上生成的回答接近 $$y$$，最终得到模型 $$\pi_{\text{SFT}}$$。

### 奖励建模

对于一个输入 $$x$$，经过 SFT 的模型可以产生成对的回答 $$(y_1, y_2) \sim \pi_{\text{SFT}}(y \mid x)$$。我们使用 $$y_w$$ 和 $$y_l$$ 分别表示 $$(y_1, y_2)$$ 中更好的回答和更差的回答，则偏好关系可以定义为 $$y_w \succ y_l \mid x$$。假设 ground-truth reward function 为 $$r^*(x, y)$$，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution $$p^*$$ 可以表示为

$$
\begin{equation}\label{eq:bradley-terry}
  p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
$$

假设我们有一个偏好数据集 $$\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$$，其中 $$y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}$$，我们可以将 reward learning 建模为一个二分类问题：

$$
\begin{equation}\label{eq:reward_model}
  \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
$$


### RL Fine-Tuning

使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。

$$
\begin{equation}\label{eq:RL}
  \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi_{\text{ref}}(y \mid x)\bigr]
\end{equation}
$$


## Method

受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 **key insight** 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。

### Deriving the DPO objective

我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 $$r$$ 是一个通用的函数。接下来推导KL 约束奖励最大化目标的最优解 (KL-constrained reward maximization objective) 的最优解。对于 \eqref{eq:RL}，我们可以将其重写为：

$$
\begin{equation}\label{eq:RL_proof}
\begin{aligned}
   & \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(y \mid x)} \Big[ r(x, y) \Big] - \beta \mathbb{D}_{\textrm{KL}} \Big[ \pi(y \mid x) \mid\mid \pi_{\text{ref}}(y \mid x) \Big] \\
  =& \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ r(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} \right] \\
  =& \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} - \frac{1}{\beta} r(x, y) \right] \\
  =& \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right)} - \log Z(x) \right],
\end{aligned}
\end{equation}
$$

其中

$$
\begin{equation}
  Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
$$

可以看出 partition function $$Z(x)$$ 只与 $$x$$ 和 $$\pi_{\text{ref}}$$ 有关，而与 $$\pi$$ 无关。我们定义：

$$
\begin{equation}
  \pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
$$

其中 $$\pi^*(y \mid x)$$ 满足 $$\pi^*(y \mid x) \ge 0$$ 且 $$\sum_y \pi^*(y \mid x) = 1$$。由于 $$Z(x)$$ 不是 $$y$$ 的函数，我们可以将 \eqref{eq:RL_proof} 重写为：

$$
\begin{equation}
\begin{aligned}
   & \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log\frac{\pi(y \mid x)}{\pi^*(y \mid x)} \right] - \log Z(x) \right] \\
  =& \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \Big[ \mathbb{D}_{\text{KL}}(\pi(y \mid x) \mid\mid \pi^*(y \mid x)) - \log Z(x) \Big]
\end{aligned}
\end{equation}
$$

由于 $$Z(x)$$ 不依赖于 $$\pi$$，上式的最小值在 KL 散度最小时取得，根据 Gibbs' inequality，当 $$\pi = \pi^*$$ 时，KL 散度为 $$0$$。因此，KL 约束奖励最大化目标的最优解形式为：

$$
\begin{equation}\label{eq:op_policy}
  \pi_r(y\mid x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
$$

其中 $$Z(x)$$ 是 partition function。即使我们使用奖励函数 $$r_{\phi}$$ 的 MLE 估计值，估计 partition function $$Z(x)$$ 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 $$\pi_r$$、参考策略 $$\pi_{\text{ref}}$$ 和未知的 partition function $$Z(\cdot)$$。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：

$$
\begin{equation}\label{eq:main_eq}
  r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \beta \log Z(x).
\end{equation}
$$

我们可以将这种重新参数化应用于 ground-truth reward $$r^*$$ 和相应的最优策略 $$\pi^*$$。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 $$p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))$$。将 \eqref{eq:main_eq} 中的重新参数化代入到 $$r^*(x,y)$$ 的偏好模型 \eqref{eq:bradley-terry} 中：

$$
\begin{equation}
\begin{aligned}
  p^*(y_1 \succ y_2 \mid x)
  &= \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right)}{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} + \beta \log Z(x)\right)} \\
  &= \frac{1}{1+\exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}-\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} \\
  &= \sigma \left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right),
\end{aligned}
\end{equation}
$$

其中 $$\sigma(z) = 1/(1+\exp(-z))$$ 是 sigmoid 函数。通过上式，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 $$\pi^*$$ 和参考策略 $$\pi_{\text{ref}}$$。因此，Bradley-Terry 模型下的最优 RLHF 策略 $$\pi^*$$ 满足偏好模型：

$$
\begin{equation}\label{eq:objective}
  p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi_{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi_{\text{ref}}(y_1\mid x)}\right)}
\end{equation}
$$

虽然 \eqref{eq:objective} 使用了 BT 模型，我们可以类似地推导出 Plackett-Luce 模型 <d-cite key="plackett1975analysis, luce2005individual"></d-cite> 的表达式，详见附录。

注意到我们现在可以将人类偏好数据的概率表示为最优策略而不是奖励模型，我们可以为参数化策略 $$\pi_{\theta}$$ 制定一个最大似然目标。类似于奖励建模方法（即 \eqref{eq:reward_model}），我们的策略目标变为：

$$
\begin{equation}\label{eq:optimum_model}
  \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}\right)\right].
\end{equation}
$$

这样，我们既绕过了显式奖励建模步骤，又避免了执行强化学习优化。此外，由于我们的过程等价于拟合重新参数化的 Bradley-Terry 模型，因此它具有某些理论性质，例如在适当假设下，偏好数据分布的一致性~\citep{bong2022generalized}。

### What does the DPO update do?

为了理解 DPO 的机制，我们可以分析损失函数 $$\mathcal{L}_\text{DPO}$$ 的梯度。对于参数 $$\theta$$ 的梯度可以写成：

$$
\begin{equation}\label{eq:gradient}
\begin{aligned}
  & \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi_{\text{ref}}) = \\
  &-\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[ \underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong} \bigg[ \underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$} \bigg] \bigg],
\end{aligned}
\end{equation}
$$

其中，$$\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$ 是由语言模型 $$\pi_\theta$$ 和参考模型 $$\pi_{\text{ref}}$$ 隐式定义的奖励。直观地说，损失函数 $$\mathcal{L}_\text{DPO}$$ 的梯度增加了更好回答 $$y_w$$ 的可能性，降低了更差回答 $$y_l$$ 的可能性。重要的是，这些样本的权重取决于隐式奖励模型 $$\hat{r}_\theta$$ 如何评价更差的回答，由 $$\beta$$ 缩放，即隐式奖励模型对回答的排序有多么不正确，考虑到 KL 约束的强度。我们的实验表明这种加权的重要性，因为没有加权系数的这种方法可能会导致语言模型退化。

### DPO outline

DPO 的一般流程如下：1) 为每个 prompt $$x$$ 采样完成 $$y_1, y_2 \sim \pi_{\text{ref}}(\cdot \mid x)$$，并使用人类偏好标记构建偏好离线数据集 $$\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$$；2) 为给定的 $$\pi_{\text{ref}}$$ 和 $$\mathcal{D}$$ 以及所需的 $$\beta$$，优化语言模型 $$\pi_\theta$$ 以最小化 $$\mathcal{L}_\text{DPO}$$。在实践中，我们希望重用公开可用的偏好数据集，而不是生成样本并收集人类偏好。由于偏好数据集是使用 $$\pi_{\text{ref}}$$ 进行采样的，我们在可能的情况下初始化 $$\pi_{\text{ref}} = \pi_{\text{SFT}}$$。然而，当 $$\pi_{\text{SFT}}$$ 不可用时，我们通过最大化更好回答的似然来初始化 $$\pi_{\text{ref}}$$，即 $$\pi_{\text{ref}} = \mathop{\mathrm{argmax}}_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]$$。这个过程有助于减轻真实参考分布和 DPO 使用的 $$\pi_{\text{ref}}$$ 之间的分布偏移。


## Implementation

```python
import torch.nn.functional as F

def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):
    """
    pi_logps: policy logprobs, shape (B,)
    ref_logps: reference model logprobs, shape (B,)
    yw_idxs: preferred completion indices in [0, B-1], shape (T,)
    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
    beta: temperature controlling strength of KL penalty

    Each pair of (yw_idxs[i], yl_idxs[i]) represents the
      indices of a single preference pair.
    """

    pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]
    ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]

    pi_logratios = pi_yw_logps - pi_yl_logps
    ref_logratios = ref_yw_logps - ref_yl_logps

    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))
    rewards = beta * (pi_logps - ref_logps).detach()

    return losses, rewards
```

## Experiments

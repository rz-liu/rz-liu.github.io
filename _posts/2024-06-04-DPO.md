---
layout: distill
title: RLHF (1) - DPO
description: 'A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model'
tags: RLHF, LLM
date: 2024-06-04
featured: true
related_publications: true

authors:
  - name: Runze Liu
    affiliations:
      name: Tsinghua University

bibliography: blogs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Summary
  - name: Preliminaries
  - name: Method
  - name: References

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  mjx-container[jax="CHTML"][display="true"] {
    margin-top: 0em !important;
    margin-bottom: 1em !important;
  }
---

## Summary

- 1
- 2
- 3

## Preliminaries

### Dataset



### SFT

Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt $$x$$ 和高质量回答 $$y$$ 的数据集，对模型进行微调，使得模型在 prompt $$x$$ 上生成的回答接近 $$y$$，最终得到模型 $$\pi^{\text{SFT}}$$。

### 奖励建模

对于一个输入 $$x$$，经过 SFT 的模型可以产生成对的回答 $$(y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)$$。我们使用 $$y_w$$ 和 $$y_l$$ 分别表示 $$(y_1, y_2)$$ 中更好的回答和更差的回答，则偏好关系可以定义为 $$y_w \succ y_l \mid x$$。假设 ground-truth reward function 为 $$r^*(x, y)$$，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution $$p^*$$ 可以表示为

$$
\begin{equation}
  p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
$$

假设我们有一个偏好数据集 $$\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$$，其中 $$y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}$$，我们可以将 reward learning 建模为一个二分类问题：

$$
\begin{equation}
  \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
$$


### RL Fine-Tuning

使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。

$$
\begin{equation}
\label{eq:RL}
  \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi^{\text{ref}}(y \mid x)\bigr]
\end{equation}
$$


## Method

受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 **key insight** 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。

### Deriving the DPO objective

我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 $$r$$ 是一个通用的函数。根据以前的工作~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}，KL 约束奖励最大化目标的最优解形式为：

$$
\begin{equation}
\label{eq:op_policy}
\pi_r(y\mid x) = \frac{1}{Z(x)}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
$$

其中 $$Z(x) =\sum_{y}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$$ 是 partition function。即使我们使用奖励函数 $$r_{\phi}$$ 的 MLE 估计值，估计 partition function $$Z(x)$$ 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 $$\pi_r$$、参考策略 $$\pi^{\text{ref}}$$ 和未知的 partition function $$Z(\cdot)$$。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：

$$
\begin{equation}
\label{eq:main_eq}
  r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi^{\text{ref}}(y\mid x)} + \beta \log Z(x).
\end{equation}
$$

我们可以将这种重新参数化应用于 ground-truth reward $$r^*$$ 和相应的最优策略 $$\pi^*$$。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 $$p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))$$。将 \eqref{eq:main_eq} 中的重新参数化代入到 $$r^*(x,y)$$ 的偏好模型 \eqref{eq:bradley-terry} 中，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 $$\pi^*$$ 和参考策略 $$\pi^{\text{ref}}$$。因此，Bradley-Terry 模型下的最优 RLHF 策略 $$\pi^*$$ 满足偏好模型：

$$
\begin{equation}
\label{eq:objective}
  p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi^{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi^{\text{ref}}(y_1\mid x)}\right)}
\end{equation}
$$

The derivation is in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\ref{app:plackett_luce_models}.

Now that we have 
the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_\theta$. Analogous to the reward modeling approach (i.e. Eq.~\ref{eq:reward_model}), our policy objective becomes:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi^{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi^{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi^{\text{ref}}(y_l\mid x)}\right)\right].
\end{equation}
\rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.

### What does the DPO update do?

For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the parameters $\theta$ can be written as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi^{\text{ref}}) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$}\bigg]\bigg],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi^{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi^{\text{ref}}$ (more in Section~\ref{sec:theory}). Intuitively, the gradient of the loss function $\mathcal{L}_\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}_\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\ref{tab:unlikelihood_generations}).

### DPO outline

The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \sim \pi^{\text{ref}}(\cdot \mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\}_{i=1}^N$ and 2) optimize the language model $\pi_\theta$ to minimize $\mathcal{L}_\text{DPO}$ for the given $\pi^{\text{ref}}$ and $\mathcal{D}$ and desired $\beta$. 
In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pisft$, we initialize $\pi^{\text{ref}} = \pisft$ whenever available. However, when $\pisft$ is not available, we initialize $\pi^{\text{ref}}$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\pi^{\text{ref}} = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi^{\text{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\ref{app:implementation}.
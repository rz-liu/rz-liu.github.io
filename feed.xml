<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rz-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rz-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-14T09:47:27+00:00</updated><id>https://rz-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RLHF - TDPO</title><link href="https://rz-liu.github.io/blog/2024/TDPO/" rel="alternate" type="text/html" title="RLHF - TDPO"/><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/TDPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/TDPO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Token-level Direct Preference Optimization (ICML 24)]]></summary></entry><entry><title type="html">RLHF - RTO</title><link href="https://rz-liu.github.io/blog/2024/RTO/" rel="alternate" type="text/html" title="RLHF - RTO"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/RTO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/RTO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 Reinforced Token Optimization (RTO) <d-cite key="RTO"></d-cite>，使用 MDP 框架解决 RLHF 问题。RTO 算法包括两个主要步骤：(i) token-wise reward learning，其中 RTO 基于偏好数据学习 token-wise 奖励；(ii) optimizing token-wise reward，通过 RL 训练方法（如 PPO）优化 token-wise 奖励。</p> <h3 id="motivation">Motivation</h3> <ul> <li>之前的方法使用的是 sentence-level 奖励，这种奖励 sparse，不利于学习。</li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>利用 fine-grained token-wise information 学习奖励函数</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>设 \(x \in \mathcal{X}\) 表示从分布 \(\rho \in \Delta(\mathcal{X})\) 中采样的提示 (prompt)，\(y= (y_1,y_2,\ldots,y_h,\ldots)\) 表示由 LLM 生成的回答，其中 \(y_i\) 表示第 \(i\) 个 token。在 RLHF 中<d-cite key="christiano2017deep"></d-cite>，通常使用 Bradley-Terry (BT) 模型<d-cite key="BTModel"></d-cite>建模偏好：</p> \[\begin{equation} \label{eqn:bt} \begin{aligned} P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数, \(r\) 是定义在 <strong>sentence-level</strong> 的真实奖励函数 (ground-truth reward function)，用于评价整条回答 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和根据奖励训练 RL。在第一步，需要给定一个数据集 \(\mathcal{D} = \{(x, y^w, y^l)\}\)，其中 \(y^w\) 表示更好的回答，\(y^l\) 表示更差的回答。奖励函数通过最大似然估计（MLE）在数据集 \(\mathcal{D}\) 上学习：</p> \[\begin{equation} \label{eqn:mle_old} r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big]. \end{equation}\] <p>在第二步，优化学习到的奖励 \(r_{\mathrm{MLE}}\)，同时确保更新后的 LLM 不会与参考模型 \(\pi_{\mathrm{ref}}\) 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 \(r_{\mathrm{MLE}}\) 进行优化，带有 KL 正则化项：</p> \[\begin{equation} \hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg], \end{equation}\] <p>其中，\(\beta &gt; 0\) 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 \(x\) 和一个包含 \(H\) 个 token 的回答 \(y = y_{1:H}\)，其中 \(H\) 是 token 的数量，现有的 PPO 开源实现将句子级奖励 \(r_{\mathrm{MLE}}(x, y)\) 分配给最后一个 token，并优化以下奖励：</p> \[\begin{equation} \label{eq:ppo:reward} \begin{aligned} {r}_{\mathrm{ppo}}(x, y_{1:h}) = \begin{cases} {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h \le H-1, \\ {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h = H, \end{cases} \end{aligned} \end{equation}\] <p>其中，\(\pi\) 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。</p> <h2 id="formulation">Formulation</h2> <h3 id="mdp-formulation-for-rlhf">MDP Formulation for RLHF</h3> <p>我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)\)，其中 \(\mathcal{S}\) 是状态空间，\(\mathcal{A}\) 是动作空间，\(\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) 是环境动态，\(r\) 表示奖励函数，\(\rho\) 表示初始状态分布，\(H\) 是交互步数的最大值。MDP 中的策略 \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\) 是从状态到动作分布的映射。环境 \(\mathcal{M}\) 与代理之间的交互可以描述如下。首先，从初始分布 \(\rho\) 中抽取起始状态 \(s_1\)。在第 \(h\) 步，代理观察到状态 \(s_h\) 并根据其策略选择动作 \(a_h\)。然后，环境从分布 \(\mathcal{P}(\cdot \mid s_h, a_h)\) 中抽取下一个状态 \(s_{h+1}\)。这种交互会持续到满足某个结束条件，该条件将在 \(H\) 步内触发。</p> <p>在大型语言模型（LLM）的标准文本生成过程中，每个状态 \(s_h = (x, y_{1:h-1})\) 包括提示 \(x\) 和到目前为止生成的所有响应 token。每个动作 \(a_h = y_{h}\) 表示词汇表中的一个 token。环境动态 \(\mathcal{P}\) 通常是已知的且确定的，这意味着给定 tokens \(s_h = (x, y_{1:h-1})\) 和 \(a_h = y_{h}\)，环境将转移到 \(s_{h+1} = (x, y_{1:h})\)。策略 \(\pi\) 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 \(h\)，\(\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})\)。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，\(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 表示 token-wise 奖励。生成的文本以特殊的句子结束 token \(\texttt{EoS}\) 结束，该 token 终止生成过程。</p> <p>在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 \(\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H\) 和 \(\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H\)，偏好由以下公式计算：</p> \[\begin{equation} \label{eq:BT:mdp} P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg). \end{equation}\] <p>相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。</p> <h3 id="learning-objective">Learning Objective</h3> <p>与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 \(\pi\)，我们定义其对应的正则化价值函数为</p> \[\begin{equation} \label{eq:q:v} \begin{aligned} V_\beta^{\pi}(s; r) &amp;= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg], \end{aligned} \end{equation}\] <p>其中，期望 \(\mathbb{E}_{\pi}\) 是针对策略 \(\pi\) 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 \(H\)，因此 \eqref{eq:q:v} 中的求和最多在 \(H\) 步结束。在本文的其余部分，我们可能会将 \(\sum_{h=1}^\infty\) 和 \(\sum_{h=1}^H\) 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 \(Q_\beta^\pi\) 是策略 \(\pi\) 的正则化价值函数的关联函数，定义如下</p> \[\begin{equation} \label{eq:bellman} Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)], \end{equation}\] <p>where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. The regularized optimal policy $\pi_\beta^<em>$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^</em>$ and $V_\beta^<em>$, respectively. By \eqref{eq:bellman}, it can be shown that # \label{eq:optimal:policy} \pi_\beta^</em>(a \mid s) = \exp{ (Q_\beta^<em>(s, a) - V_\beta^</em>(s))/\beta }. # Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap: # \label{eq:def:subopt} \SubOpt(\hat{\pi}) = \mathbb{E}<em>{s \sim \rho} [V</em>{\beta}^<em>(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^</em>(\rho) - V_\beta^{\hat{\pi}}(\rho), # where we use the shorthand $V_\beta^\pi(\rho) = \mathbb{E}<em>{s \sim \rho}[V</em>\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure $d^\pi(s) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.</p> <h3 id="advantages-of-token-wise-mdp-over-sentence-wise-bandit">Advantages of Token-Wise MDP over Sentence-Wise Bandit</h3> <p>\label{sec:token:reward}</p> <table> <tbody> <tr> <td>直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A =</td> <td>\mathcal{A}</td> <td>\(的确定性 MDP。我们使用自回归策略\)\pi^<em>\(来表示强大的 LLM 策略，例如 GPT-4。固定提示\)x\(，给定回答\)(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)\(，由\)\pi^</em>$$ 提供的评估为</td> </tr> </tbody> </table> \[\begin{equation} P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}. \end{equation}\] <p>通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 \(r_s\) 和 token 级奖励 \(r_t\) 可以分别由以下公式得到</p> \[\begin{equation} \label{eq:reward:example} r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}). \end{equation}\] <p>直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 \(r_s(x, y) = \sum_{h = 1}^H r_t( (x, y_{1:h-1}), y_h)\)。</p> <h3 id="method">Method</h3> <p>\label{sec:alg_theory}</p> <p>Motivated by Section~\ref{sec:formulation}, we tackle RLHF by treating it as an MDP problem. Under this MDP framework, we aim to develop an algorithmic framework that fully utilizes the token-level information. To this end, we develop the Reinforced Token Optimization (\texttt{RTO}) algorithm. At a high level, \texttt{RTO} consists of two main steps: {\color{bluee}(i) token-wise reward learning}, where \texttt{RTO} learns a token-wise reward based on the preference data; and {\color{bluee} (ii) optimizing token-wise reward} through RL training methods such as PPO. In Section~\ref{sec:theory:version}, we provide a theoretically grounded version of \texttt{RTO} with guaranteed sample complexity. To align more closely with practice, we present a practical implementation of \texttt{RTO} in Section~\ref{sec:practical:version}.</p> <p>\subsection{Theoretical Version with Sample Complexity Guarantee} \label{sec:theory:version}</p> <p>We focus on the offline setting and assume the access to an offline dataset $\cD = {(\tau^w, \tau^l)}$ that contains several trajectory pairs, where $\tau^w = {(s_h^w, a_h^w)}<em>{h=1}^H$ is preferred over $\tau^l = {(s_h^l, a_h^l)}</em>{h=1}^H$. Each pair of trajectories shares the same initial state/prompt (i.e., $s_1^w = s_1^l$), but differs in the subsequent tokens. We also assume that the reward function is linear, and our following results are ready to be extended to general function approximation \citep{chen2022human,wang2023rlhf,zhan2023provable}. \begin{assumption}[Linear Reward] \label{assumption:linear} We assume that the reward function $r$ is linear, i.e., $r(s, a) = \phi(s, a)^\top \theta^<em>$ for some known feature $\phi: \cS \times \cA \rightarrow \RR^d$ and unknown vector $\theta^</em> \in \RR^d$. We also assume that $|\phi(\cdot, \cdot)|<em>2 \le L$ and $| \theta^* |_2 \le B$. \end{assumption} Following the standard reward learning pipeline \citep{ouyang2022training}, we learn the reward function via maximum likelihood estimation (MLE). Specifically, if we parametrize the reward function by $\theta$, then the MLE is given by # \label{eq:mle} \theta</em>{\mle} = \argmax_{|\theta|<em>2 \le B} \cL</em>{\cD}(\theta), \quad \text{where } \cL_\cD(\theta) = \sum_{(\tau^w, \tau^l) \in \cD} &amp; \bigg[ \log \Big( \sigma \big(\sum_{h=1}^H r_\theta(s_h^w, a_h^w) - \sum_{h=1}^H r_\theta(s_h^l, a_h^l) \big) \Big) \bigg]. # Inspired by previous literature in offline RL \citep{jin2021pessimism,rashidinejad2021bridging,xiong2022nearly,zhu2023principled,zhan2023provable}, given the MLE $\theta_{\mle}$, we construct the pessimistic token-wise reward estimation as # \label{eq:pessimistic:reward} \hat{r}(s, a) = \phi(s, a)^\top \theta_{\mle} - \varrho \cdot |\phi(s, a)|<em>{\Sigma</em>\cD^{-1}}, # where $\Sigma_\cD = \sum_{(\tau^1, \tau^2) \in \cD} [\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ) (\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ))^\top] + \lambda I_d$, $\lambda &gt; 0$ is a tuning parameter, and $\varrho$ is a problem-dependent coefficient will be specified in Theorem~\ref{thm:offline} and \eqref{eq:varrho}. Finally, \texttt{RTO} outputs the optimal policy $\hat{\pi}$ with respect to $\hat{r}$, i.e., $\hat{\pi} = \argmax_\pi V_\beta^\pi(s; \hat{r})$ for any $s \in \cS$. The pseudocode of \texttt{RTO} is given in Algorithm~\ref{alg:offline}.</p> <p>\begin{algorithm}[t] \caption{Reinforced Token Optimization (Theoretical Version)} \label{alg:offline} \begin{algorithmic}[1] \STATE \textbf{Input:} Offline dataset $\cD$, $\lambda &gt; 0$, $\beta &gt; 0$, and problem dependent coefficient $\varrho$. \STATE Compute $\theta_{\mle}$ based on $\cD$ by maximizing the loglikelihood given in \eqref{eq:mle}. \STATE Calculate the pessimistic reward $\hat{r}$ via \eqref{eq:pessimistic:reward}. {\color{bluee}\COMMENT{token-wise reward learning}} \STATE Compute the corresponding optimal policy $\hat{\pi}$ with respect to $\hat{r}$. {\color{bluee}\COMMENT{optimizing token-wise reward}} \STATE \textbf{Output:} policy $\hat{\pi}$. \end{algorithmic} \end{algorithm}</p> <p>\begin{theorem} \label{thm:offline} Suppose Assumption~\ref{assumption:linear} holds. For $\beta &gt; 0$, $\lambda &gt; 0$, $\delta \in (0, 1)$, if we choose $\varrho = \tilde{\cO}(\sqrt{d})$ (see~\eqref{eq:varrho}), then the output policy $\hat{\pi}$ of Algorithm~\ref{alg:offline} satisfies $ \SubOpt(\hat{\pi}) \le 2 \varrho \cdot \mathbb{E}<em>{(s, a) \sim d^*} \big[|\phi(s, a) |</em>{\Sigma_\cD^{-1}} \big] - \beta \cdot \mathbb{E}<em>{s \sim d^*} \big[\mathrm{KL}\big(\pi</em>\beta^*(\cdot \mid s) | \hat{\pi}(\cdot \mid s) \big) \big]. $ \end{theorem}</p> <p>\begin{proof} See Appendix~\ref{appendix:pf:thm:offline} for a detailed proof. \end{proof}</p> <table> <tbody> <tr> <td>The first term in Theorem \ref{thm:offline} measures how well the offline dataset covers the trajectory generated by the policy $\pi_\beta^*$. Typically, this term decreases at a rate of $</td> <td>\cD</td> <td>^{-1/2}$ under the mild partial coverage assumption~\citep{jin2021pessimism,uehara2021pessimistic,xiong2022nearly,zhu2023principled,zhan2023provable}, where $</td> <td>\cD</td> <td>$ is the size of the offline dataset. The second KL term is always negative, and it arises from the goal of learning a regularized value. We also remark that our algorithm relies on the known transition kernel to compute the exact optimal policy with respect to $\hat{r}$. While this is natural in the context of large language models, we provide insights on how to extend our findings to stochastic regularized MDPs and the variant of our \texttt{RTO} algorithm in Appendix~\ref{appendix:variant}.</td> </tr> </tbody> </table> <p>There have also been previous works \citep{pacchiano2021dueling,chen2022human,wang2023rlhf,li2023reinforcement,zhan2023provable} studying RLHF under the MDP framework, also known as dueling RL and preference-based RL. However, these works do not consider the KL constraint, which is an essential component of RLHF. Furthermore, they do not explicitly emphasize the superiority of the MDP framework over the contextual dueling bandit problem in the context of LLMs, and their proposed algorithms lack practical implementation. In contrast, we will provide a practical implementation of our algorithm, demonstrating the practicality of our approach.</p> <p>\subsection{Practical Implementation} \label{sec:practical:version} In this subsection, we shift our focus to developing a practical version of \texttt{RTO}. The key challenge in implementing \texttt{RTO} in Algorithm~\ref{alg:offline} lies in learning the token-wise reward to be optimized from the offline data. In the most popular frameworks outlined in Instruct-GPT \citep{ouyang2022training}, Claude \citep{bai2022training}, and LLaMA2 \citep{touvron2023llama} projects replace the last layer of the LLM with a linear layer for a scalar output and maximize the log-likelihood as in \eqref{eqn:mle_old}. However, this approach gives only a sentence-level reward. To bridge the gap in the literature, we present our practical version of \texttt{RTO} in Algorithm~\ref{alg:offline:practical}, which features a novel calculation of token-wise reward. Our key observation is that, given a trajectory $\tau = {(s_h, a_h)}<em>{h=1}^H$, we have # \label{eq:prac:1} \sum</em>{h=1}^H \beta \log \frac{\pi_\beta^<em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} &amp;= \sum_{h = 1}^H \big(Q_\beta^</em>(s_h, a_h) - V_\beta^<em>(s_h) - \log \pi_{\mathrm{ref}}(a_h \mid s_h) \big) \notag <br/> &amp; = \sum_{h = 1}^H r(s_h, a_h) - V_\beta^</em>(s_1) + \underbrace{\sum_{h = 1}^{H-1} \big( \mathbb{E}<em>{s’ \sim \cP(\cdot \mid s_h, a_h)}[V</em>\beta^<em>(s’)] - V_\beta^</em>(s_{h+1}) \big)}<em>{(\star)}, # where the first equality uses the closed-form of optimal policy $\pi</em>\beta^<em>(a \mid s) = \exp{ (Q_\beta^</em>(s, a) - V_\beta^<em>(s))/\beta }$ in \eqref{eq:optimal:policy}, and the second equality follows from the fact that $Q_\beta^{\pi}(s, a) = r_\beta(s, a) + \mathbb{E}_{s’ \sim \cP(\cdot \mid s, a)}[V_\beta^\pi(s’)]$ in \eqref{eq:bellman} with $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. We focus on the typical LLM generation scenario where the transition kernel is deterministic. Then we have $(\star) = 0$ in \eqref{eq:prac:1}, yielding that $ \sum_{h = 1}^H r(s_h, a_h) = \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} + V_\beta^<em>(s_1) . $ Building upon this result and combining it with the definition of the BT model in \eqref{eq:BT:mdp}, for any trajectory pair ${\tau^j = {(s_{h}^j, a_{h}^j)}_{h=1}^H}_{j=1}^2$ satisfying $s_1^1 = s_1^2$, we have # \label{eq:prac:2} \PP(\tau^1 \succ \tau^2) = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg) = \sigma \bigg( \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h^1 \mid s_h^1)}{\pi_{\mathrm{ref}}(a_h^1 \mid s_h^1)} - \sum_{h=1}^H \beta \log \frac{\pi_\beta^<em>(a_h^2 \mid s_h^2)}{\pi_{\mathrm{ref}}(a_h^2 \mid s_h^2)} \bigg). # An interesting observation is that, based on the autoregressive nature of policies, \eqref{eq:prac:2} aligns with the learning objective of DPO proposed by \citet{rafailov2023direct}, but under the token-level MDP instead of the sentence-level bandit setup. Similar to the bandit setting where the learning objective is equivalent to a BT model with sentence-wise reward $r^</em>(x, y) = \beta \log \frac{\pi_{\beta}^<em>(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$ \citep{rafailov2023direct}, \eqref{eq:prac:2} shows that the learning objective in token-wise MDP equivalents to a BT model with a token-wise reward function # \label{eq:prac:3} r^</em>(s_h = (x, y_{1:h-1}), a_h = y_h) = \beta \log \frac{\pi_\beta^<em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} = \beta \log \frac{\pi_\beta^</em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})}, # where $x$ is the prompt, $y_{1:h-1}$ is the tokens generated so far, and $y_h$ is the token chosen at the current step. In contrast to the previous PPO implementation with sparse reward in \eqref{eq:ppo:reward}, we will assign the token-wise reward function defined in~\eqref{eq:prac:3} to each step. Formally, for any $h$, we define \begin{equation} \begin{aligned}\label{eq:prac:5} &amp;\beta_1 \log \frac{\pi_\beta^<em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} <br/> &amp; \qquad \approx \beta_1 \log \frac{\pi_{\mathrm{dpo}}(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} := r_{\mathrm{rto}}((x, y_{1:h-1}),y_h) \end{aligned} \end{equation} as the token-wise reward used by \texttt{RTO}, where $\beta_1$ and $\beta_2$ are tuning parameters, and $\pi$ is the current policy to be updated. In the last step of \eqref{eq:prac:5}, we use $\pi_{\mathrm{dpo}}$, the policy learned by DPO, as a proxy for the unknown $\pi_\beta^</em>$. Finally, we employ PPO to optimize the token-wise reward $r_{\mathrm{rto}}$ in \eqref{eq:prac:5}.</p>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF]]></summary></entry><entry><title type="html">RLHF - IPL</title><link href="https://rz-liu.github.io/blog/2024/IPL/" rel="alternate" type="text/html" title="RLHF - IPL"/><published>2024-06-08T00:00:00+00:00</published><updated>2024-06-08T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/IPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/IPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="IPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="rl">RL</h3> <p>Contractive Bellman operator</p> \[\begin{equation}\label{eq:bellman} (\mathcal{B}^\pi_r Q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot\mid s,a)}[V^\pi(s')], \end{equation}\] <h2 id="method">Method</h2> <h3 id="inverse-soft-bellman-operator">Inverse Soft-Bellman Operator</h3> \[\begin{equation}\label{eq:inverse_bellman} (\mathcal{T}^\pi Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'\sim p(\cdot\mid s,a)}[V^\pi(s')]. \end{equation}\] <p>记 \(r_{Q^\pi}\) 为使用 \(Q^\pi\) 导出的隐式奖励函数 (implicit reward function)，即 \(r_{Q^\pi}(s,a) = Q^\pi(s,a)\)，则我们可以得到使用 \(Q^\pi\) 导出的隐式奖励函数的偏好分布 (preference distribution) \(P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}]\):</p> \[\begin{equation}\label{eq:q_preference} P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) }{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) + \exp \sum_t (\mathcal{T}^\pi Q)(s_t^{2}, a_t^{2})}. \end{equation}\] <h3 id="optimal-inverse-bellman-operator">Optimal Inverse Bellman Operator</h3> \[\begin{equation} (\mathcal{T}^* Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'}[V^{\text{targ}}(s')], \ \text{ where } V^\text{targ}(s) \text{ is estimated as in } \mathcal{B}^*_r. \end{equation}\] <h3 id="regularization">Regularization</h3> \[\begin{equation} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{1}, \sigma^{2},y \sim \mathcal{D}_p} \left[y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}])\right]. \end{equation}\] <p>其中 \(P_{Q^*}\) 由将 \(\mathcal{T}^*Q\) 替换到 \eqref{eq:q_preference} 得到。</p> <p>但是仅仅使用此损失函数会导致较差的结果，因为此目标函数是不受约束的，由于 BT model 对于 shift 是不变的，即对于所有的奖励函数 \(r(s,a)\)，\(Q(s,a)\) 都是一样的。为了解决这个问题，IPL 引入一个凸正则项 \(\psi(\cdot)\)，对隐式奖励函数 \(r_{Q^\pi} = \mathcal{T^\pi}Q\) 进行正则化，得到正则化的偏好损失函数：</p> \[\begin{equation}\label{eq:ipl} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{(1)}, \sigma^{(2)},y \sim \mathcal{D}_p} \left[ y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}]) \right] + \lambda \psi(\mathcal{T}^*Q) \end{equation}\] \[\begin{equation}\label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\]]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Inverse Preference Learning: Preference-based RL without a Reward Function]]></summary></entry><entry><title type="html">RLHF - CPL</title><link href="https://rz-liu.github.io/blog/2024/CPL/" rel="alternate" type="text/html" title="RLHF - CPL"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/CPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/CPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="CPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h2 id="method">Method</h2> <p>###</p> \[\begin{equation}\label{eq:adv_to_pol1} A^*(s,a) = \alpha \log \pi^*(a|s). \end{equation}\] \[\begin{equation} \begin{aligned} A(s_t,a_t) &amp;= Q(s_t,a_t) - V(s_t) \\ &amp;= r_t + \gamma V(s_{t+1}) - V(s_t).\\ \sum_{h=t}^{H-1} A(s_h,a_h) &amp;= \sum_{h=t}^{H-1} \left[ r_h + \gamma V(s_{h+1}) - V(s_h) \right] \\ &amp;= \sum_{h=t}^{H-1} r_h + \gamma V(s_H) - V(s_t) \\ \end{aligned} \end{equation}\] \[\begin{equation} P_{A^*}\left[\sigma^+ \succ \sigma^- \right] = \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t)}{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi^*(a^-_t|s^-_t)}. \end{equation}\] \[\begin{equation}\label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <h3 id="practical-implementation">Practical Implementation</h3> <p>Regularization term</p> \[\begin{equation}\label{eq:reg_loss} \mathcal{L}_{\text{CPL}({\color{red}{\lambda}})}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{\mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp {\color{red}{\lambda}} \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <p>Python 实现如下，有点没搞懂为什么要用这个形式，是为了数值稳定性？</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Apply the log-sum-exp trick.
</span>    <span class="c1"># y = 1 if we prefer x2 to x1
</span>    <span class="c1"># We need to implement the numerical stability trick.
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">max21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">max12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max21</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span> <span class="o">-</span> <span class="n">max21</span><span class="p">))</span> <span class="o">+</span> <span class="n">max21</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max12</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span> <span class="o">-</span> <span class="n">max12</span><span class="p">))</span> <span class="o">+</span> <span class="n">max12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">nlp21</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">nlp12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>正常来说，不加这个 trick 的话，应该是这样的：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># y = 1 if we prefer x2 to x1
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">))</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning]]></summary></entry><entry><title type="html">RLHF - DPO</title><link href="https://rz-liu.github.io/blog/2024/DPO/" rel="alternate" type="text/html" title="RLHF - DPO"/><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/DPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/DPO/"><![CDATA[<h2 id="summary">Summary</h2> <p>这篇论文提出了 Direct Preference Optimization (DPO) <d-cite key="DPO"></d-cite>，使用偏好数据直接优化策略，而不需要先学奖励模型再学策略。</p> <h3 id="motivation">Motivation</h3> <ul> <li>之前的方法流程复杂，先学 reward model，再学 policy，效率低</li> <li>RL (PPO) 难以训练</li> </ul> <h3 id="key-idea">Key Idea</h3> <h3 id="contributions">Contributions</h3> <ol> <li>D</li> <li>D</li> <li>D</li> </ol> <h2 id="preliminaries">Preliminaries</h2> <h3 id="dataset">Dataset</h3> <h3 id="sft">SFT</h3> <p>Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt \(x\) 和高质量回答 \(y\) 的数据集，对模型进行微调，使得模型在 prompt \(x\) 上生成的回答接近 \(y\)，最终得到模型 \(\pi_{\text{SFT}}\)。</p> <h3 id="奖励建模">奖励建模</h3> <p>对于一个输入 \(x\)，经过 SFT 的模型可以产生成对的回答 \((y_1, y_2) \sim \pi_{\text{SFT}}(y \mid x)\)。我们使用 \(y_w\) 和 \(y_l\) 分别表示 \((y_1, y_2)\) 中更好的回答和更差的回答，则偏好关系可以定义为 \(y_w \succ y_l \mid x\)。假设 ground-truth reward function 为 \(r^*(x, y)\)，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution \(p^*\) 可以表示为</p> \[\begin{equation}\label{eq:bradley-terry} p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}. \end{equation}\] <p>假设我们有一个偏好数据集 \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\)，其中 \(y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}\)，我们可以将 reward learning 建模为一个二分类问题：</p> \[\begin{equation}\label{eq:reward_model} \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr] \end{equation}\] <h3 id="rl-fine-tuning">RL Fine-Tuning</h3> <p>使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。</p> \[\begin{equation}\label{eq:RL} \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi_{\text{ref}}(y \mid x)\bigr] \end{equation}\] <h2 id="method">Method</h2> <p>受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 <strong>key insight</strong> 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。</p> <h3 id="deriving-the-dpo-objective">Deriving the DPO objective</h3> <p>我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 \(r\) 是一个通用的函数。接下来推导KL 约束奖励最大化目标的最优解 (KL-constrained reward maximization objective) 的最优解。对于 \eqref{eq:RL}，我们可以将其重写为：</p> \[\begin{equation}\label{eq:RL_proof} \begin{aligned} &amp; \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(y \mid x)} \Big[ r(x, y) \Big] - \beta \mathbb{D}_{\textrm{KL}} \Big[ \pi(y \mid x) \mid\mid \pi_{\text{ref}}(y \mid x) \Big] \\ =&amp; \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ r(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} - \frac{1}{\beta} r(x, y) \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right)} - \log Z(x) \right], \end{aligned} \end{equation}\] <p>其中</p> \[\begin{equation} Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>可以看出 partition function \(Z(x)\) 只与 \(x\) 和 \(\pi_{\text{ref}}\) 有关，而与 \(\pi\) 无关。我们定义：</p> \[\begin{equation} \pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(\pi^*(y \mid x)\) 满足 \(\pi^*(y \mid x) \ge 0\) 且 \(\sum_y \pi^*(y \mid x) = 1\)。由于 \(Z(x)\) 不是 \(y\) 的函数，我们可以将 \eqref{eq:RL_proof} 重写为：</p> \[\begin{equation} \begin{aligned} &amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log\frac{\pi(y \mid x)}{\pi^*(y \mid x)} \right] - \log Z(x) \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \Big[ \mathbb{D}_{\text{KL}}(\pi(y \mid x) \mid\mid \pi^*(y \mid x)) - \log Z(x) \Big] \end{aligned} \end{equation}\] <p>由于 \(Z(x)\) 不依赖于 \(\pi\)，上式的最小值在 KL 散度最小时取得，根据 Gibbs’ inequality，当 \(\pi = \pi^*\) 时，KL 散度为 \(0\)。因此，KL 约束奖励最大化目标的最优解形式为：</p> \[\begin{equation}\label{eq:op_policy} \pi_r(y\mid x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(Z(x)\) 是 partition function。即使我们使用奖励函数 \(r_{\phi}\) 的 MLE 估计值，估计 partition function \(Z(x)\) 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 \(\pi_r\)、参考策略 \(\pi_{\text{ref}}\) 和未知的 partition function \(Z(\cdot)\)。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：</p> \[\begin{equation}\label{eq:main_eq} r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \beta \log Z(x). \end{equation}\] <p>我们可以将这种重新参数化应用于 ground-truth reward \(r^*\) 和相应的最优策略 \(\pi^*\)。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 \(p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))\)。将 \eqref{eq:main_eq} 中的重新参数化代入到 \(r^*(x,y)\) 的偏好模型 \eqref{eq:bradley-terry} 中：</p> \[\begin{equation} \begin{aligned} p^*(y_1 \succ y_2 \mid x) &amp;= \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right)}{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} + \beta \log Z(x)\right)} \\ &amp;= \frac{1}{1+\exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}-\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} \\ &amp;= \sigma \left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数。通过上式，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 \(\pi^*\) 和参考策略 \(\pi_{\text{ref}}\)。因此，Bradley-Terry 模型下的最优 RLHF 策略 \(\pi^*\) 满足偏好模型：</p> \[\begin{equation}\label{eq:objective} p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi_{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi_{\text{ref}}(y_1\mid x)}\right)} \end{equation}\] <p>虽然 \eqref{eq:objective} 使用了 BT 模型，我们可以类似地推导出 Plackett-Luce 模型 <d-cite key="plackett1975analysis, luce2005individual"></d-cite> 的表达式，详见附录。</p> <p>注意到我们现在可以将人类偏好数据的概率表示为最优策略而不是奖励模型，我们可以为参数化策略 \(\pi_{\theta}\) 制定一个最大似然目标。类似于奖励建模方法（即 \eqref{eq:reward_model}），我们的策略目标变为：</p> \[\begin{equation}\label{eq:optimum_model} \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}\right)\right]. \end{equation}\] <p>这样，我们既绕过了显式奖励建模步骤，又避免了执行强化学习优化。此外，由于我们的过程等价于拟合重新参数化的 Bradley-Terry 模型，因此它具有某些理论性质，例如在适当假设下，偏好数据分布的一致性~\citep{bong2022generalized}。</p> <h3 id="what-does-the-dpo-update-do">What does the DPO update do?</h3> <p>为了理解 DPO 的机制，我们可以分析损失函数 \(\mathcal{L}_\text{DPO}\) 的梯度。对于参数 \(\theta\) 的梯度可以写成：</p> \[\begin{equation}\label{eq:gradient} \begin{aligned} &amp; \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi_{\text{ref}}) = \\ &amp;-\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[ \underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong} \bigg[ \underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$} \bigg] \bigg], \end{aligned} \end{equation}\] <p>其中，\(\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\) 是由语言模型 \(\pi_\theta\) 和参考模型 \(\pi_{\text{ref}}\) 隐式定义的奖励。直观地说，损失函数 \(\mathcal{L}_\text{DPO}\) 的梯度增加了更好回答 \(y_w\) 的可能性，降低了更差回答 \(y_l\) 的可能性。重要的是，这些样本的权重取决于隐式奖励模型 \(\hat{r}_\theta\) 如何评价更差的回答，由 \(\beta\) 缩放，即隐式奖励模型对回答的排序有多么不正确，考虑到 KL 约束的强度。我们的实验表明这种加权的重要性，因为没有加权系数的这种方法可能会导致语言模型退化。</p> <h3 id="dpo-outline">DPO outline</h3> <p>DPO 的一般流程如下：1) 为每个 prompt \(x\) 采样完成 \(y_1, y_2 \sim \pi_{\text{ref}}(\cdot \mid x)\)，并使用人类偏好标记构建偏好离线数据集 \(\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N\)；2) 为给定的 \(\pi_{\text{ref}}\) 和 \(\mathcal{D}\) 以及所需的 \(\beta\)，优化语言模型 \(\pi_\theta\) 以最小化 \(\mathcal{L}_\text{DPO}\)。在实践中，我们希望重用公开可用的偏好数据集，而不是生成样本并收集人类偏好。由于偏好数据集是使用 \(\pi_{\text{ref}}\) 进行采样的，我们在可能的情况下初始化 \(\pi_{\text{ref}} = \pi_{\text{SFT}}\)。然而，当 \(\pi_{\text{SFT}}\) 不可用时，我们通过最大化更好回答的似然来初始化 \(\pi_{\text{ref}}\)，即 \(\pi_{\text{ref}} = \mathop{\mathrm{argmax}}_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]\)。这个过程有助于减轻真实参考分布和 DPO 使用的 \(\pi_{\text{ref}}\) 之间的分布偏移。</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">def</span> <span class="nf">dpo_loss</span><span class="p">(</span><span class="n">pi_logps</span><span class="p">,</span> <span class="n">ref_logps</span><span class="p">,</span> <span class="n">yw_idxs</span><span class="p">,</span> <span class="n">yl_idxs</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    pi_logps: policy logprobs, shape (B,)
    ref_logps: reference model logprobs, shape (B,)
    yw_idxs: preferred completion indices in [0, B-1], shape (T,)
    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
    beta: temperature controlling strength of KL penalty

    Each pair of (yw_idxs[i], yl_idxs[i]) represents the
      indices of a single preference pair.
    </span><span class="sh">"""</span>

    <span class="n">pi_yw_logps</span><span class="p">,</span> <span class="n">pi_yl_logps</span> <span class="o">=</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>
    <span class="n">ref_yw_logps</span><span class="p">,</span> <span class="n">ref_yl_logps</span> <span class="o">=</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>

    <span class="n">pi_logratios</span> <span class="o">=</span> <span class="n">pi_yw_logps</span> <span class="o">-</span> <span class="n">pi_yl_logps</span>
    <span class="n">ref_logratios</span> <span class="o">=</span> <span class="n">ref_yw_logps</span> <span class="o">-</span> <span class="n">ref_yl_logps</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logratios</span> <span class="o">-</span> <span class="n">ref_logratios</span><span class="p">))</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logps</span> <span class="o">-</span> <span class="n">ref_logps</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">rewards</span>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)]]></summary></entry><entry><title type="html">RLHF -</title><link href="https://rz-liu.github.io/blog/2024/template/" rel="alternate" type="text/html" title="RLHF -"/><published>2024-06-01T00:00:00+00:00</published><updated>2024-06-01T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/template</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/template/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://rz-liu.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://rz-liu.github.io/blog/2024/tabs</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="c76ac877-eb12-4ae1-8285-e10b22af6f98" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="c76ac877-eb12-4ae1-8285-e10b22af6f98" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="c51ccef6-dcf7-4cbd-9829-1fe42079c24e" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="c51ccef6-dcf7-4cbd-9829-1fe42079c24e" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="3533ad57-5a80-4494-84ac-226fbc82226a" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="3533ad57-5a80-4494-84ac-226fbc82226a" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://rz-liu.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://rz-liu.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>
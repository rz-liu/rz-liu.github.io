<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rz-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rz-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-14T03:35:31+00:00</updated><id>https://rz-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RLHF - RTO</title><link href="https://rz-liu.github.io/blog/2024/RTO/" rel="alternate" type="text/html" title="RLHF - RTO"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/RTO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/RTO/"><![CDATA[<h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>在本章，我们介绍标准的 RLHF 范式。设 \(x \in \mathcal{X}\) 表示从分布 \(\rho \in \Delta(\mathcal{X})\) 中采样的提示，\(y= (y_1,y_2,\ldots,y_h,\ldots)\) 表示由 LLM 生成的回答，其中 \(y_i\) 表示第 \(i\) 个 token。在实践中，通常假设 <d-cite key="christiano2017deep"></d-cite> 奖励信号根据 Bradley-Terry (BT) 模型 <d-cite key="BTModel"></d-cite> 生成：</p> \[\begin{equation} \label{eqn:bt} \begin{aligned} P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数, \(r\) 是定义在 <strong>sentence-level</strong> 的 ground-truth reward function，用于评价整个回复 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和基于奖励训练 RL。在第一步中，学习者被给定一个数据集 \(\mathcal{D} = \{(x, y^w, y^l)\}\)，其中 \(y^w\) 表示更好的回复，\(y^l\) 表示更差的回复。奖励函数通过最大似然估计（MLE）在数据集 \(\mathcal{D}\) 上学习：</p> \[\begin{equation} \label{eqn:mle_old} r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big]. \end{equation}\] <p>在第二步，优化学习到的奖励 \(r_{\mathrm{MLE}}\)，同时确保更新后的 LLM 不会与参考模型 \(\pi_{\mathrm{ref}}\) 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 \(r_{\mathrm{MLE}}\) 进行优化，带有 KL 正则化项：</p> \[\begin{equation} \hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg], \end{equation}\] <p>其中，\(\beta &gt; 0\) 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 \(x\) 和一个包含 \(H\) 个 token 的回复 \(y = y_{1:H}\)，其中 \(H\) 是 token 的数量，现有的 PPO 开源实现将句子级奖励 \(r_{\mathrm{MLE}}(x, y)\) 分配给最后一个 token，并优化以下奖励：</p> \[\begin{equation} \label{eq:ppo:reward} \begin{aligned} {r}_{\mathrm{ppo}}(x, y_{1:h}) = \begin{cases} {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h \le H-1, \\ {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h = H, \end{cases} \end{aligned} \end{equation}\] <p>其中，\(\pi\) 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。</p> <h2 id="formulation">Formulation</h2> <h3 id="mdp-formulation-for-rlhf">MDP Formulation for RLHF</h3> <p>我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)\)，其中 \(\mathcal{S}\) 是状态空间，\(\mathcal{A}\) 是动作空间，\(\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) 是环境动态，\(r\) 表示奖励函数，\(\rho\) 表示初始状态分布，\(H\) 是交互步数的最大值。MDP 中的策略 \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\) 是从状态到动作分布的映射。环境 \(\mathcal{M}\) 与代理之间的交互可以描述如下。首先，从初始分布 \(\rho\) 中抽取起始状态 \(s_1\)。在第 \(h\) 步，代理观察到状态 \(s_h\) 并根据其策略选择动作 \(a_h\)。然后，环境从分布 \(\mathcal{P}(\cdot \mid s_h, a_h)\) 中抽取下一个状态 \(s_{h+1}\)。这种交互会持续到满足某个结束条件，该条件将在 \(H\) 步内触发。</p> <p>在大型语言模型（LLM）的标准文本生成过程中，每个状态 \(s_h = (x, y_{1:h-1})\) 包括提示 \(x\) 和到目前为止生成的所有响应 token。每个动作 \(a_h = y_{h}\) 表示词汇表中的一个 token。环境动态 \(\mathcal{P}\) 通常是已知的且确定的，这意味着给定 tokens \(s_h = (x, y_{1:h-1})\) 和 \(a_h = y_{h}\)，环境将转移到 \(s_{h+1} = (x, y_{1:h})\)。策略 \(\pi\) 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 \(h\)，\(\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})\)。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，\(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 表示 token-wise 奖励。生成的文本以特殊的句子结束 token \(\texttt{EoS}\) 结束，该 token 终止生成过程。</p> <p>在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 \(\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H\) 和 \(\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H\)，偏好由以下公式计算：</p> \[\begin{equation} \label{eq:BT:mdp} P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg). \end{equation}\] <p>相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。</p> <h3 id="learning-objective">Learning Objective</h3> <p>与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 \(\pi\)，我们定义其对应的正则化价值函数为</p> \[\begin{equation} \label{eq:q:v} \begin{aligned} V_\beta^{\pi}(s; r) &amp;= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg], \end{aligned} \end{equation}\] <p>其中，期望 \(\mathbb{E}_{\pi}\) 是针对策略 \(\pi\) 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 \(H\)，因此 \eqref{eq:q:v} 中的求和最多在 \(H\) 步结束。在本文的其余部分，我们可能会将 \(\sum_{h=1}^\infty\) 和 \(\sum_{h=1}^H\) 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 \(Q_\beta^\pi\) 是策略 \(\pi\) 的正则化价值函数的关联函数，定义如下</p> \[\begin{equation} \label{eq:bellman} Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)], \end{equation}\]]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF]]></summary></entry><entry><title type="html">RLHF - IPL</title><link href="https://rz-liu.github.io/blog/2024/IPL/" rel="alternate" type="text/html" title="RLHF - IPL"/><published>2024-06-08T00:00:00+00:00</published><updated>2024-06-08T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/IPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/IPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="IPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="rl">RL</h3> <p>Contractive Bellman operator</p> \[\begin{equation} \label{eq:bellman} (\mathcal{B}^\pi_r Q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot\mid s,a)}[V^\pi(s')], \end{equation}\] <h2 id="method">Method</h2> <h3 id="inverse-soft-bellman-operator">Inverse Soft-Bellman Operator</h3> \[\begin{equation} \label{eq:inverse_bellman} (\mathcal{T}^\pi Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'\sim p(\cdot\mid s,a)}[V^\pi(s')]. \end{equation}\] <p>记 \(r_{Q^\pi}\) 为使用 \(Q^\pi\) 导出的隐式奖励函数 (implicit reward function)，即 \(r_{Q^\pi}(s,a) = Q^\pi(s,a)\)，则我们可以得到使用 \(Q^\pi\) 导出的隐式奖励函数的偏好分布 (preference distribution) \(P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}]\):</p> \[\begin{equation} \label{eq:q_preference} P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) }{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) + \exp \sum_t (\mathcal{T}^\pi Q)(s_t^{2}, a_t^{2})}. \end{equation}\] <h3 id="optimal-inverse-bellman-operator">Optimal Inverse Bellman Operator</h3> \[\begin{equation} (\mathcal{T}^* Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'}[V^{\text{targ}}(s')], \ \text{ where } V^\text{targ}(s) \text{ is estimated as in } \mathcal{B}^*_r. \end{equation}\] <h3 id="regularization">Regularization</h3> \[\begin{equation} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{1}, \sigma^{2},y \sim \mathcal{D}_p} \left[y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}])\right]. \end{equation}\] <p>其中 \(P_{Q^*}\) 由将 \(\mathcal{T}^*Q\) 替换到 \eqref{eq:q_preference} 得到。</p> <p>但是仅仅使用此损失函数会导致较差的结果，因为此目标函数是不受约束的，由于 BT model 对于 shift 是不变的，即对于所有的奖励函数 \(r(s,a)\)，\(Q(s,a)\) 都是一样的。为了解决这个问题，IPL 引入一个凸正则项 \(\psi(\cdot)\)，对隐式奖励函数 \(r_{Q^\pi} = \mathcal{T^\pi}Q\) 进行正则化，得到正则化的偏好损失函数：</p> \[\begin{equation} \label{eq:ipl} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{(1)}, \sigma^{(2)},y \sim \mathcal{D}_p} \left[ y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}]) \right] + \lambda \psi(\mathcal{T}^*Q) \end{equation}\] \[\begin{equation} \label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\]]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Inverse Preference Learning: Preference-based RL without a Reward Function]]></summary></entry><entry><title type="html">RLHF - CPL</title><link href="https://rz-liu.github.io/blog/2024/CPL/" rel="alternate" type="text/html" title="RLHF - CPL"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/CPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/CPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="CPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h2 id="method">Method</h2> <p>###</p> \[\begin{equation} \label{eq:adv_to_pol1} A^*(s,a) = \alpha \log \pi^*(a|s). \end{equation}\] \[\begin{equation} \begin{aligned} A(s_t,a_t) &amp;= Q(s_t,a_t) - V(s_t) \\ &amp;= r_t + \gamma V(s_{t+1}) - V(s_t).\\ \sum_{h=t}^{H-1} A(s_h,a_h) &amp;= \sum_{h=t}^{H-1} \left[ r_h + \gamma V(s_{h+1}) - V(s_h) \right] \\ &amp;= \sum_{h=t}^{H-1} r_h + \gamma V(s_H) - V(s_t) \\ \end{aligned} \end{equation}\] \[\begin{equation} P_{A^*}\left[\sigma^+ \succ \sigma^- \right] = \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t)}{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi^*(a^-_t|s^-_t)}. \end{equation}\] \[\begin{equation} \label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <h3 id="practical-implementation">Practical Implementation</h3> <p>Regularization term</p> \[\begin{equation} \label{eq:reg_loss} \mathcal{L}_{\text{CPL}({\color{red}{\lambda}})}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{\mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp {\color{red}{\lambda}} \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <p>Python 实现如下，有点没搞懂为什么要用这个形式，是为了数值稳定性？</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Apply the log-sum-exp trick.
</span>    <span class="c1"># y = 1 if we prefer x2 to x1
</span>    <span class="c1"># We need to implement the numerical stability trick.
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">max21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">max12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max21</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span> <span class="o">-</span> <span class="n">max21</span><span class="p">))</span> <span class="o">+</span> <span class="n">max21</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max12</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span> <span class="o">-</span> <span class="n">max12</span><span class="p">))</span> <span class="o">+</span> <span class="n">max12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">nlp21</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">nlp12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>正常来说，不加这个 trick 的话，应该是这样的：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># y = 1 if we prefer x2 to x1
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">))</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning]]></summary></entry><entry><title type="html">RLHF (1) - DPO</title><link href="https://rz-liu.github.io/blog/2024/DPO/" rel="alternate" type="text/html" title="RLHF (1) - DPO"/><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/DPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/DPO/"><![CDATA[<h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="dataset">Dataset</h3> <h3 id="sft">SFT</h3> <p>Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt \(x\) 和高质量回答 \(y\) 的数据集，对模型进行微调，使得模型在 prompt \(x\) 上生成的回答接近 \(y\)，最终得到模型 \(\pi^{\text{SFT}}\)。</p> <h3 id="奖励建模">奖励建模</h3> <p>对于一个输入 \(x\)，经过 SFT 的模型可以产生成对的回答 \((y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)\)。我们使用 \(y_w\) 和 \(y_l\) 分别表示 \((y_1, y_2)\) 中更好的回答和更差的回答，则偏好关系可以定义为 \(y_w \succ y_l \mid x\)。假设 ground-truth reward function 为 \(r^*(x, y)\)，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution \(p^*\) 可以表示为</p> \[\begin{equation} p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}. \end{equation}\] <p>假设我们有一个偏好数据集 \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\)，其中 \(y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}\)，我们可以将 reward learning 建模为一个二分类问题：</p> \[\begin{equation} \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr] \end{equation}\] <h3 id="rl-fine-tuning">RL Fine-Tuning</h3> <p>使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。</p> \[\begin{equation} \label{eq:RL} \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi^{\text{ref}}(y \mid x)\bigr] \end{equation}\] <h2 id="method">Method</h2> <p>受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 <strong>key insight</strong> 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。</p> <h3 id="deriving-the-dpo-objective">Deriving the DPO objective</h3> <p>我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 \(r\) 是一个通用的函数。根据以前的工作~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}，KL 约束奖励最大化目标的最优解形式为：</p> \[\begin{equation} \label{eq:op_policy} \pi_r(y\mid x) = \frac{1}{Z(x)}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(Z(x) =\sum_{y}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)\) 是 partition function。即使我们使用奖励函数 \(r_{\phi}\) 的 MLE 估计值，估计 partition function \(Z(x)\) 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 \(\pi_r\)、参考策略 \(\pi^{\text{ref}}\) 和未知的 partition function \(Z(\cdot)\)。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：</p> \[\begin{equation} \label{eq:main_eq} r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi^{\text{ref}}(y\mid x)} + \beta \log Z(x). \end{equation}\] <p>我们可以将这种重新参数化应用于 ground-truth reward \(r^*\) 和相应的最优策略 \(\pi^*\)。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 \(p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))\)。将 \eqref{eq:main_eq} 中的重新参数化代入到 \(r^*(x,y)\) 的偏好模型 \eqref{eq:bradley-terry} 中，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 \(\pi^*\) 和参考策略 \(\pi^{\text{ref}}\)。因此，Bradley-Terry 模型下的最优 RLHF 策略 \(\pi^*\) 满足偏好模型：</p> \[\begin{equation} \label{eq:objective} p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi^{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi^{\text{ref}}(y_1\mid x)}\right)} \end{equation}\] <p>The derivation is in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\ref{app:plackett_luce_models}.</p> <p>Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_\theta$. Analogous to the reward modeling approach (i.e. Eq.~\ref{eq:reward_model}), our policy objective becomes: \begin{equation}\label{eq:optimum_model} \mathcal{L}<em>\text{DPO}(\pi</em>{\theta}; \pi^{\text{ref}}) = -\mathbb{E}<em>{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi</em>{\theta}(y_w\mid x)}{\pi^{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi^{\text{ref}}(y_l\mid x)}\right)\right]. \end{equation} \rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.</p> <h3 id="what-does-the-dpo-update-do">What does the DPO update do?</h3> <p>For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}<em>\text{DPO}$. The gradient with respect to the parameters $\theta$ can be written as: \begin{multline*}\label{eq:gradient} \nabla</em>\theta \mathcal{L}<em>\text{DPO}(\pi</em>\theta;\pi^{\text{ref}}) = \ -\beta\mathbb{E}<em>{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}</em>\theta(x, y_l) - \hat{r}<em>\theta (x, y_w))}</em>\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}<em>\text{increase likelihood of $y_w$} - \underbrace{\nabla</em>\theta\log\pi(y_l \mid x)}<em>\text{decrease likelihood of $y_l$}\bigg]\bigg], \end{multline*} where $\hat{r}</em>\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi^{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi^{\text{ref}}$ (more in Section~\ref{sec:theory}). Intuitively, the gradient of the loss function $\mathcal{L}<em>\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}</em>\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\ref{tab:unlikelihood_generations}).</p> <h3 id="dpo-outline">DPO outline</h3> <p>The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \sim \pi^{\text{ref}}(\cdot \mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\mathcal{D} = {x^{(i)}, y_w^{(i)}, y_l)^{(i)}}<em>{i=1}^N$ and 2) optimize the language model $\pi</em>\theta$ to minimize $\mathcal{L}<em>\text{DPO}$ for the given $\pi^{\text{ref}}$ and $\mathcal{D}$ and desired $\beta$. In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pisft$, we initialize $\pi^{\text{ref}} = \pisft$ whenever available. However, when $\pisft$ is not available, we initialize $\pi^{\text{ref}}$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\pi^{\text{ref}} = \argmax</em>{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi^{\text{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\ref{app:implementation}.</p>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://rz-liu.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://rz-liu.github.io/blog/2024/tabs</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="3f2c5c1f-be8f-48c2-8060-3cec71621e7b" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="3f2c5c1f-be8f-48c2-8060-3cec71621e7b" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="f269c5ea-e858-4a94-9133-57cb0be85a82" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="f269c5ea-e858-4a94-9133-57cb0be85a82" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="a2050b01-7ab2-4ac6-b53e-5457dc617d5b" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="a2050b01-7ab2-4ac6-b53e-5457dc617d5b" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://rz-liu.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://rz-liu.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://rz-liu.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with bibliography</title><link href="https://rz-liu.github.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="a post with bibliography"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://rz-liu.github.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://rz-liu.github.io/blog/2023/post-bibliography/"><![CDATA[<p>This post shows how to add bibliography to simple blog posts. We support every citation style that <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a> does. That means simple citation like (missing reference), multiple citations like (missing reference), long references like (missing reference) or also quotes:</p> <blockquote><p>Lorem ipsum dolor sit amet, consectetur adipisicing elit,<br/>sed do eiusmod tempor.</p><p>Lorem ipsum dolor sit amet, consectetur adipisicing.</p><cite>(missing reference)</cite></blockquote> <p>If you would like something more academic, check the <a href="/blog/2021/distill/">distill style post</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[an example of a blog post with bibliography]]></summary></entry></feed>
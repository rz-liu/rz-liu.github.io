<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rz-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rz-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-23T05:57:49+00:00</updated><id>https://rz-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ICLR</title><link href="https://rz-liu.github.io/blog/Scaling-LLM-Test-Time/" rel="alternate" type="text/html" title="ICLR"/><published>2024-09-23T00:00:00+00:00</published><updated>2024-09-23T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/Scaling-LLM-Test-Time</id><content type="html" xml:base="https://rz-liu.github.io/blog/Scaling-LLM-Test-Time/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li>让LLM使用更多test-time computation，而不是增加模型参数，可以更有效地提升模型性能。</li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>构造一个推理 (rationale) 数据集，构造方法：对于模型能回答对的问题，提示模型进行推理，最终得到正确答案；对于回答错误的问题，将正确答案提供给模型，让模型根据答案逆向推理，得到推理过程。将模型输出的推理过程加入数据集，如此迭代，不断提升模型的推理能力。</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RL,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters (arXiv 2408)]]></summary></entry><entry><title type="html">ICLR</title><link href="https://rz-liu.github.io/blog/STaR/" rel="alternate" type="text/html" title="ICLR"/><published>2024-09-22T00:00:00+00:00</published><updated>2024-09-22T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/STaR</id><content type="html" xml:base="https://rz-liu.github.io/blog/STaR/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>构造一个推理 (rationale) 数据集，构造方法：对于模型能回答对的问题，提示模型进行推理，最终得到正确答案；对于回答错误的问题，将正确答案提供给模型，让模型根据答案逆向推理，得到推理过程。将模型输出的推理过程加入数据集，如此迭代，不断提升模型的推理能力。</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RL,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning (NeurIPS 22)]]></summary></entry><entry><title type="html">RLHF - SELM</title><link href="https://rz-liu.github.io/blog/SELM/" rel="alternate" type="text/html" title="RLHF - SELM"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/SELM</id><content type="html" xml:base="https://rz-liu.github.io/blog/SELM/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="SELM"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li>Online RLHF 可以主动迭代式探索 out-of-distribution (OOD) 区域，但是随机采样的方式可能会导致采样效率低下，高奖励的区域可能会被忽略，因此需要一种更加高效的采样方式。</li> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>加一个 optimism bonus，鼓励 agent 探索未知区域，同时也能够保证 agent 在已知区域的收敛性。</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <h3 id="large-language-models">Large Language Models</h3> <p>A language model \(\pi\in\Delta_\mathcal{Y}^\mathcal{X}\) typically takes the prompt \(x\in\mathcal{X}\) as input and outputs the response \(y\in\mathcal{Y}\). Here, \(\mathcal{X}\) and \(\mathcal{Y}\) are finite spaces of prompts and responses, respectively. Given the prompt \(x\in\mathcal{X}\), a discrete probability distribution \(\pi(\cdot \mid x)\in\Delta_\mathcal{Y}\) is generated, where \(\Delta_\mathcal{Y}\) is the set of discrete distributions over \(\mathcal{Y}\). Modern recipes for training LLMs consist of pre-training and post-training procedures, where during pre-training, LLMs learn to predict the next word on a huge and diverse dataset of text sequences in order to understand the underlying patterns and structures of natural language in an unsupervised manner. The post-training procedure aims to align better to end tasks and human preferences with two phases happening in order: Supervised Fine-Tuning (SFT) and human preference alignment. Here, SFT fine-tunes the pre-trained LLM with supervised learning on high-quality data to follow instructions on downstream tasks and obtain a model \(\pi^{\text{SFT}}\). In the following of this paper, we focus mainly on preference alignment.</p> <p>\subsection{Reward Modeling and Preference Optimization} \paragraph{Reinforcement Learning from Human Feedback (RLHF).} Standard RLHF frameworks consist of learning a reward model and then optimizing the LLM policy using the learned reward.</p> <p>Specifically, a point-wise reward \(r(x, y): \mathcal{X}\times\mathcal{Y}\rightarrow \mathcal{R}\) represents the Elo score \citep{elo1978rating} of the response \(y\) given the prompt \(x\). Then the preference distribution can be expressed by the Bradley-Terry model that distinguishes between the preferred response \(y_w\) and the dispreferred response \(y_l\) given prompt \(x\), denoted as \(y_w\succ y_l \mid x\), using the logistic function \(\sigma\):</p> \[\begin{equation}\label{bt_model} \begin{aligned} p(y_w\succ y_l \mid x) &amp;:= \mathbb{E}_{h}\bigl[\mathbb{1}(h \text{ prefers } y_w \text{ over } y_l \text{ given } x)\bigr] \notag\\ &amp;\,= \sigma\bigl(r(x, y_w) - r(x, y_l)\bigr) = \frac{\exp\bigl(r(x, y_w)\bigr)}{\exp\bigl(r(x, y_w)\bigr) + \exp\bigl(r(x, y_l)\bigr)}, \end{aligned} \end{equation}\] <p>where \(h\) denotes the human rater and the expectation is over \(h\) to account for the randomness of the choices of human raters we ask for their preference. When provided a static dataset of \(N\) comparisons \(\mathcal{D}=\{x_i, y_{w,i}, y_{l,i}\}_{i=1}^N\), the parameterized reward model can be learned by minimizing the following logistic regression loss:</p> \[\begin{equation}\label{lr_loss} \mathcal{L}_{\text{lr}}(r; \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\bigl[\log\sigma\bigl(r(x, y_w) - r(x, y_l)\bigr)\bigr]. \end{equation}\] <p>Using the learned reward, the LLM policy \(\pi\in\Delta_\mathcal{Y}^\mathcal{X}\) is optimized with reinforcement learning (RL) to maximize the expected reward while maintaining a small deviation from some base reference policy \(\pi_{\text{ref}}\), i.e., maximizing the following objective</p> \[\begin{equation}\label{rlhf_kl} \mathcal{J}(\pi; \mathcal{D}) = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi(\cdot \mid x)}\bigl[r(x, y)\bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}), \end{equation}\] <p>where \(\beta\) is a hyperparameter and \(\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}) := \mathbb{E}_{x\sim\mathcal{D}} [\text{KL}(\pi(\cdot \mid x) \, \mid \mid \pi_{\text{ref}}(\cdot \mid x))]\) is the expected Kullback-Leibler (KL) divergence. An ideal \(\pi_{\text{ref}}\) is the policy that helps mitigate the distribution shift issue \citep{rafailov2024direct,guo2024direct} between the true preference distribution and the policy \(\pi\) during the off-policy RL training. Since we only have access to the dataset \(\mathcal{D}\) sampled from the unavailable true preference distribution, \(\pi_{\text{ref}}\) can be obtained by fine-tuning on the preferred responses in \(\mathcal{D}\) or simply setting \(\pi_{\text{ref}}=\pi^{\text{SFT}}\) and performing RLHF based on the SFT model.</p> <h3 id="direct-alignment-from-preference">Direct Alignment from Preference</h3> <p>With the motivation to get rid of a separate reward model, which is computationally costly to train, recent works \citep{rafailov2024direct,azar2023general,zhao2023slic,tunstall2023zephyr,ethayarajh2024kto} derived the preference loss as a function of the policy by changing of variables. Among them, DPO \citep{rafailov2024direct} shows that when the BT model in \eqref{bt_model} can perfectly fit the preference, the global optimizers of the RLHF objective in \eqref{rlhf_kl} and the following loss are equivalent:</p> \[\begin{equation} \mathcal{L}_{\text{DPO}}(\pi;\mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\biggl[\log\sigma\biggl(\beta\log\frac{\pi(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta\log\frac{\pi(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\biggr)\biggr]. \end{equation}\] <h2 id="method">Method</h2> <h3 id="rm-free-objective-for-active-exploration">RM-Free Objective for Active Exploration</h3> <p>\label{sec_der}</p> <p>In this section, we present several modifications to the optimistically biased objective \eqref{eq_intro} motivated in the introduction. Then we derive an RM-free objective for the LLM policy and analyze how active exploration works by examining its gradient.</p> <p>First, we consider the equivalence of \eqref{eq_intro}: \(\max_r-\mathcal{L}_{\text{lr}}(r; \mathcal{D}) + \alpha\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]\), where the inner \(\pi\) is deterministic when optimal. To account for the change of \(\pi\) relative to the reference policy \(\pi_{\text{ref}}\), we introduce two modifications: (1) replacing the optimistic bias term \(\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]\) with \(\max_\pi\mathbb{E}_{y\sim\pi, y'\sim\pi_{\text{ref}}}[r(x, y)-r(x, y')]\), and (2) incorporating a KL-divergence loss term between \(\pi\) and \(\pi_{\text{ref}}\). These changes ensure that the resulting optimistic RM elicits responses with high potential unknown to the reference policy \(\pi_{\text{ref}}\) while minimizing the deviation between \(\pi\) and \(\pi_{\text{ref}}\).</p> <p>Formally, for the reward function \(r\), the bilevel optimization problem with optimism is formulated as:</p> \[\begin{equation}\label{our_obj} \max_r -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) +\alpha\max_\pi\Biggl(\underbrace{\mathbb{E}_{\substack{x\sim\mathcal{D}_t, y\sim\pi(\cdot \mid x)\\ y'\sim\pi_{\text{ref}}(\cdot \mid x)}}\Bigl[r(x, y) - r(x, y')\Bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}})}_{\mathcal{F}(\pi; r)}\Biggr), \end{equation}\] <p>where \(\mathcal{D}_t=\{x_i, y_{w,i}^t, y_{l,i}^t\}_{i=1}^N\) is the associated dataset at iteration \(t\) and \(\mathcal{L}_{\text{lr}}\) is the logistic regression loss defined in \eqref{lr_loss}. The nested optimization in \eqref{our_obj} can be handled by first solving the inner optimization \(\mathcal{F}(\pi; r)\) to obtain \(\pi_r\) that is optimal under \(r\). The solution is as follows and we defer all the derivations in this section to Appendix \ref{derivation_1}.</p> \[\begin{equation} \pi_r(y \mid x) := \mathop{\mathrm{argmax}}_\pi\mathcal{F}(\pi; r) = \frac{1}{Z(x)}\pi_{\text{ref}}(y \mid x)\exp\bigl(r(x, y) / \beta\bigr), \end{equation}\] <p>where the partition function \(Z(x) = \sum_y\pi_{\text{ref}}(y \mid x)\exp(r(x, y)/\beta)\). By substituting \(\pi=\pi_r\) into \(\mathcal{F}(\pi; r)\), we can rewrite the bilevel objective in \eqref{our_obj} as a single-level one:</p> \[\begin{equation} \max_r -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) + \alpha\mathcal{F}(\pi_r; r). \end{equation}\] <p>Following the implicit reward formulation in DPO, we reparameterize the reward function with \(\theta\in\Theta\) as \(\hat{r}_\theta(x, y)=\beta(\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x))\), which is the optimal solution of \eqref{rlhf_kl} and can express \textit{all} reward classes consistent with the BT model as proved in \citep{rafailov2024direct}. With this change of variable, we obtain the RM-free objective for direct preference alignment with optimism:</p> \[\begin{equation}\label{final_obj} \max_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr]. \end{equation}\] <p>We now analyze how this new objective encourages active exploration. Specifically, we derive the gradient of \eqref{final_obj} with respect to \(\theta\) as</p> \[\begin{equation}\label{eq_grad} \begin{aligned} &amp;\underbrace{-\beta\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}_t}\Bigl[\sigma\bigl(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\bigr)\bigl(\nabla_\theta\log\pi_\theta(y_w \mid x) - \nabla_\theta\log\pi_\theta(y_l \mid x)\bigr)\Bigr]}_{\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)} \notag\\ &amp;\qquad\qquad\qquad\qquad\qquad\qquad - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_\theta(\cdot \mid x)}\bigl[\exp\bigl(-\hat{r}_\theta(x, y)/\beta\bigr)\nabla_\theta\log\pi_\theta(y \mid x)\bigr]. \end{aligned} \end{equation}\] <p>We note that the second line, corresponding to the gradient of the optimism term, decreases the log-likelihood of response \(y\) generated by \(\pi_\theta\) that has a low value of \(\exp(-\hat{r}_\theta(x, y)/\beta)\). Therefore, the added optimism term biases the gradient toward parameter regions that can elicit responses \(y\) with high implicit reward \(\hat{r}_\theta\), consistent with our intuition outlined in Figure \ref{urm_illu}.</p> <p>This also explains why \(\mathbb{E}_{\pi_{\text{ref}}}[\log\pi_\theta]\) is minimized in our objective \eqref{final_obj}, which is equivalent to maximizing the KL divergence between \(\pi_{\text{ref}}\) and \(\pi_\theta\), while the reverse KL in the policy optimization objective \eqref{rlhf_kl} is minimized. For the DPO gradient \(\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)\), the degree of deviation of policy \(\pi_\theta\) from \(\pi_{\text{ref}}\) only affects the preference estimated with \(\hat{r}_\theta\). In other words, \(\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))\) is a scalar value and the policy deviation only determines the \textit{step size} of the policy gradient, instead of its \textit{direction}. On the other hand, our added exploration term directly controls the direction of the gradient toward potentially more rewarding areas while still fitting the preference data in \(\mathcal{D}_t\). As more feedback data is collected iteratively, deviating from the unbiasedly fitted model incurs a higher DPO loss, which ultimately dominates our objective at convergence. This mechanism ensures that the resulting LLM effectively balances between exploring novel responses and exploiting previously observed ones, leading to a more accurate and aligned model.</p> <h3 id="algorithm">Algorithm</h3> <p>\label{sec_algo}</p> <p>With the optimistically biased objective derived above, the language model can actively generate OOD responses worth exploring. Human or AI feedback follows to reduce the uncertainty in these regions. These two steps are executed iteratively to get a more and more aligned model.</p> <p>In practice, we split the offline preference dataset into three portions with equal sizes, one for each iteration. Besides, we use AI rankers, such as external RMs, to provide feedback on the model-generated response and the original chosen, rejected responses. The complete pseudocode of our algorithm, named \textit{Self-Exploring Language Models} (SELM), is outlined in Algorithm \ref{alg_se}. \begin{algorithm}[H] \caption{Self-Exploring Language Models (SELM)} \begin{algorithmic}[1]\label{alg_se} \REQUIRE Reference model \(\pi_{\text{ref}}\), preference dataset \(\mathcal{D}\), online iterations \(T\), optimism coefficient \(\alpha\). \FOR{iteration \(t = 1, 2, \ldots, T\)} \STATE Set \(\mathcal{D}_{t}\) as the \(t\)-th portion of \(\mathcal{D}\) and generate \(y\sim\pi_{\text{ref}}(\cdot \mid x)\) for each prompt \(x\) in \(\mathcal{D}_t\). \STATE Rank \(\{y, y_w, y_l\}\) and update \(\mathcal{D}_t\) to contain the best (chosen) and worst (rejected) responses. \STATE Train the LLM \(\pi_{\theta_t} = \mathop{\mathrm{argmax}}_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha \mathbb{E}_{x\sim\mathcal{D}_t}[\log\pi_{\theta}(y \mid x)]\) and let \(\pi_{\text{ref}}=\pi_{\theta_t}\). \ENDFOR \end{algorithmic} \end{algorithm}</p> <p>\subsection{Self-Exploration Reduces Indiscriminate Favor of Unseen Extrapolations} It has been observed recently \citep{rafailov2024r,pal2024smaug,xu2024dpo} that DPO decreases the likelihood of responses generated by the reference policy. It is because for any prompt \(x\), at convergence when \(\pi_\theta \neq \pi_{\text{ref}}\), it holds that</p> \[\begin{equation} \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\hat{r}_\theta(x, y)/\beta\bigr] = \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x)\bigr] = -\text{KL}\bigl(\pi_{\text{ref}}(\cdot \mid x) \mid \mid \pi_\theta(\cdot \mid x)\bigr) &lt; 0, \end{equation}\] <p>while at the beginning of training when \(\pi_\theta = \pi_{\text{ref}}\), the above terms are zero. Thus, the expected implicit reward \(\hat{r}_\theta\) as well as the likelihood of \(\pi_\theta\) will decrease on the reference model’s responses. This indicates that DPO stimulates a biased distribution favoring unseen extrapolated responses. In the online iterative setting that we consider, the LLM policy generates responses and receives preference feedback alternately, where biasing towards OOD regions may sometimes help discover outstanding novel responses. However, DPO \textit{indiscriminately} favors unseen extrapolations and \textit{passively} explores based purely on the randomness inherent in sampling from the LLM. As a consequence, the vast space of natural language makes it almost impossible to exhaustively explore all the possible responses and identify those that most effectively benefit alignment.</p> <p>Next, we demonstrate that SELM mitigates this issue by performing guided exploration. Specifically, consider the proposed self-exploration objective in \eqref{final_obj}, which, in addition to the standard DPO loss, also minimizes \(\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]\). We now investigate how the probability distribution changes with this term incorporated. \begin{theorem} \label{thm} For any \(\rho\in\Theta\) in the policy parameter space, let \(\hat{r}_\rho(x, y) = \beta(\log\pi_\rho(y \mid x) - \log\pi_{\text{ref}}(y \mid x))\) be the reparameterized implicit reward. Denote \(\pi^{\min}_\rho\) as the policy that minimizes the expected implicit reward under the KL constraint, i.e.,</p> \[\begin{equation}\label{eq_pi_rho} \pi^{\min}_\rho(\cdot \mid x) := \mathop{\mathrm{argmin}}_\pi\mathbb{E}_{x, y\sim\pi(\cdot \mid x)}\bigl[\hat{r}_\rho(x, y)\bigr] + \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_\rho). \end{equation}\] <p>Then minimizing \(\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]\) decreases the likelihood of responses sampled from \(\pi^{\min}_\rho\):</p> \[\begin{equation} \min_{\pi_\theta}\mathbb{E}_{x, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr] = \min_{\pi_\theta}\mathbb{E}_{x,y\sim\pi^{\min}_\rho(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr]. \end{equation}\] <p>\end{theorem}</p> <p>The above theorem states that maximizing the divergence between \(\pi_\theta\) and \(\pi_{\text{ref}}\) is essentially reducing the probability of generating responses with low implicit rewards reparameterized by any policy parameter \(\rho\) during training. In other words, the policy not only exploits the existing preference data but also learns to avoid generating the text \(y\) that is assigned a low reward value. This process occurs in every iteration with updated reference models. Consequently, responses with high potential rewards are selectively preferred and many commonplace responses receive a small probability mass, thus mitigating the indiscriminate favoring of unseen responses and improving exploration efficiency.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)]]></summary></entry><entry><title type="html">RLHF - VPO</title><link href="https://rz-liu.github.io/blog/VPO/" rel="alternate" type="text/html" title="RLHF - VPO"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/VPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/VPO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF (ArXiv 2405.19320)]]></summary></entry><entry><title type="html">RLHF - XPO</title><link href="https://rz-liu.github.io/blog/XPO/" rel="alternate" type="text/html" title="RLHF - XPO"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/XPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/XPO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF (ArXiv 2405.21046)]]></summary></entry><entry><title type="html">RLHF - TDPO</title><link href="https://rz-liu.github.io/blog/TDPO/" rel="alternate" type="text/html" title="RLHF - TDPO"/><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/TDPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/TDPO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>###</p> \[\begin{equation} 1 \end{equation}\] <h2 id="method">Method</h2> <p>###</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Token-level Direct Preference Optimization (ICML 24)]]></summary></entry><entry><title type="html">RLHF - DPO-Q*</title><link href="https://rz-liu.github.io/blog/DPO-Q/" rel="alternate" type="text/html" title="RLHF - DPO-Q*"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/DPO-Q*</id><content type="html" xml:base="https://rz-liu.github.io/blog/DPO-Q/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO-Q*"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li></li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li></li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>在 In this section we first define the per-token MDP for large language models, and then describe how it relates to classic RLHF approaches and direct alignment algorithms, specifically DPO. We operate in the typical RLHF setting where we have a dataset \(\mathcal{D}=\{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N\) of language prompts \(\mathbf{x}\) and target answers \(\mathbf{y}\), which can each individually be broken down into a sequence of tokens, for example \(\mathbf{x}=(x_0, \ldots, x_m)\), from a fixed discrete vocabulary \(\mathcal{A}\). Throughout this section we will use the \(\mathbf{x}\), \(\mathbf{y}\) notation for the contextual bandit framing where the entire response \(\mathbf{y}\) is the action, but will use state \(\mathbf{s}\) and action \(\mathbf{a}\) notation from RL literature for describing sequences at the token-level.</p> <h3 id="the-token-level-mdp-for-large-language-models">The Token-level MDP for Large Language Models</h3> <p>We define the token level MDP as a tuple \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, f, r, \rho_0)\), where the state space \(\mathcal{S}\) consists of all tokens generated so far (i.e. \(\mathbf{s}_t=\{x_0, \ldots, x_m, y_0, \ldots, y_t\}\)) and the action space is the vocabulary of tokens \(\mathcal{A}\). The dynamics \(f\) are the deterministic transition model between tokens \(f(\mathbf{s},\mathbf{a}) = \mathbf{s} \mid \mathbf{a}\), where \(\mid\) is concatenation. The initial state distribution \(\rho_0\) is a distribution over prompts \(\mathbf{x}\), where an initial state \(\mathbf{s}_0\) is comprised of the tokens from \(\mathbf{x}\). In RLHF, the reward function is learned from human feedback over preferences between responses which we will denote using trajectories \(\tau\) at the token level. As is typically done \citep{ziegler2020finetuning, stiennon2022learning}, we assume that preference trajectories start at the same state (initial propmpt) and end in a terminal state (\textbf{EOS} token), from which future rewards are zero. % While the reward function \(r\) will ultimately be learned from feedback, we assume that within the token-MDP the reward \(r\) at terminal states must be zero. Note that this is not a restrictive assumption, as \(r(\mathbf{s}_t, \mathbf{a}_t)\) can fully represent any reward dependent on the next state \(\mathbf{s}_{t+1}\) per the deterministic dynamics. In this token level MDP, the corresponding Bradley-Terry preference model \cite{bradley1952rankanalysis, christiano2017deep} is</p> \[\begin{equation}\label{eq:dense-bradley-terry} p^*(\tau^w \succeq \tau^l)=\frac{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)}{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)+ \exp\left(\sum_{i=1}^M r(\mathbf{s}_i^l, \mathbf{a}_i^l)\right)}. \end{equation}\] <p>which gives the probability that the <code class="language-plaintext highlighter-rouge">win'' trajectory $$\tau^w$$ of length $$N$$ is preferred to the</code>loss’’ trajectory \(\tau^l\) of length \(M\). Now that we have defined the token level MDP, we can show how it relates to both classic and direct alignment RLHF methods.</p> <h3 id="the-classical-rlhf-methods">The Classical RLHF Methods</h3> <p>Most classical RLHF approaches \citep{ziegler2020finetuning, bai2022constitutional, ouyang2022training} first learn a reward function from human feedback on prompt and response pairs \((\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\), then optimize it with a policy gradient-based method like PPO \citep{schulman2017proximal} with an entropy-bonus using the following KL-constrained RL objective</p> \[\begin{equation}\label{eq:multi_step_RL} \max_{\pi_{\theta}} \mathbb{E}_{a_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_t)}\left[\sum_{t=0}^T (r(\mathbf{s}_t, \mathbf{a}_t) + \underbrace{\beta\log\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}_{\text{KL penalty}}) + \beta\mathcal{H}(\pi_{\theta}) \mid \mathbf{s}_0\sim\rho(\mathbf{s}_0)\right] \end{equation}\] <p>where \(\pi_{\mathrm{ref}}\) is a reference policy, often resulting from supervised finetuning, from which the learned policy should not significantly deviate. However, in classic RLHF methods the reward function is learned as a contextual bandit with the preference model</p> \[\begin{equation}\label{eq:bandit_pref} p^*(\mathbf{y}^w \succeq \mathbf{y}^l) = \frac{\exp r(\mathbf{x},\mathbf{y}^w)} {\exp r(\mathbf{x},\mathbf{y}^w) + \exp r(\mathbf{x},\mathbf{y}^l)} \end{equation}\] <p>and is thus only applied at the final timestep for the last action where \(\mathbf{a}\) is \textbf{EOS}. In practice the actual reward used in the token-level PPO is</p> \[\begin{equation}\label{eq:token_reward} r(\mathbf{s}_t, \mathbf{a}_t) = \begin{cases} \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\ r(\mathbf{x}, \mathbf{y}) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1}=\mathbf{y} \text{ is terminal} \end{cases} \end{equation}\] <p>in a maximum entropy formulation. This leads to an interesting contradiction where the reward function \(r\) is treated like a bandit, but the actual RL value function and optimization is done per-token in practice.</p> <h3 id="direct-preference-optimization">Direct Preference Optimization</h3> <p>Unlike classical RLHF, DPO, as derived in \citet{rafailov2023direct}, stays entirely within the contextual bandits setting entirely and also uses the bandit-based preference model in \cref{eq:bandit_pref}. To circumvent the need for an RL algorithm, DPO uses the well-known closed form solution to the KL-contextual bandit version of the RL problem posed in \cref{eq:multi_step_RL} \citep{ziebart2008maximum, levine2018reinforcement}:</p> \[\begin{equation} \pi^*(\mathbf{y} \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})}\pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})e^{r(\mathbf{x}, \mathbf{y})} \end{equation}\] <p>where \(\pi^*\) is the optimal policy and \(Z(\mathbf{x})\) is the partition function that normalizes it. DPO re-arranges this equation to solve for reward as</p> \[\begin{equation} r(\mathbf{x},\mathbf{y}) = \beta \log \pi^*(\mathbf{y} \mid \mathbf{x}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x}) - Z(\mathbf{x}). \end{equation}\] <p>Substituting this relationship into the standard binary cross-entropy loss function used for reward modeling yields the DPO loss equation as the partition function \(Z(\mathbf{x})\) cancels from the Bradley Terry model.</p> \[\begin{equation}\label{eq:optimum_model} \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\mathrm{ref}}) = -\mathbb{E}_{(\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(\mathbf{y}^w \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^w \mid \mathbf{x})} - \beta \log \frac{\pi_{\theta}(\mathbf{y}^l \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^l \mid \mathbf{x})}\right)\right] \end{equation}\] <p>For brevity we use \(\sigma\) to denote the logistic function. In the next section, we show how an alternative derivation of DPO can also cast its optimization within the token-level MDP.</p> <h2 id="method">Method</h2> <p>In this section we explore how DPO can theoretically be cast into the token-level MDP, and explore the consequences of doing so. First, we provide a token level derivation of DPO under the assumptions in \cref{section:tokenMDP}. Next, we show that even in the token MDP, DPO is able to fit any reward function in the multi-step Bradley Terry preference model \cref{eq:dense-bradley-terry}. Ultimately, this shows that DPO can potentially be used for more sequential optimization tasks, like multi-turn interactions or even multi-modal generation.</p> <h3 id="dpo-as-a-q-function-in-the-token-level-mdp">DPO as a \(Q\)-function in the Token Level MDP</h3> <p>\noindent \textbf{RL in the Token-level MDP.} While the original derivation of DPO relies on the fact that \(Q^*(\mathbf{x}, \mathbf{y}) = r(\mathbf{x}, \mathbf{y})\), this relationship does not hold in the token-level MDP. To resolve this, we need to develop new mathematical results that will allow us to relate the reward function in the Token-level Bradley Terry model \cref{eq:dense-bradley-terry} to the corresponding optimal policy \(\pi^*\). In the general maximum entropy RL setting, the fixed point solution of \cref{eq:multi_step_RL} is given by \citep{ziebart2010modeling} as</p> \[\begin{equation}\label{eq:policy} \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = e^{(Q^*(\mathbf{s}_t, \mathbf{a}_t)-V^*(\mathbf{s}_t))/\beta} \end{equation}\] <p>where \(\pi^*(\mathbf{a} \mid \mathbf{s})\) is the optimal policy and \(Q^*(\mathbf{s}, \mathbf{a})\) is the optimal Q-function which models the total future reward from \((\mathbf{s}, \mathbf{a})\) under \(\pi^*\). The optimal value function \(V^*\) is a function of \(Q^*\),</p> \[\begin{equation}\label{eq:value} V^*(\mathbf{s}_t) = \beta\log\int_{\mathcal{A}} e^{Q^*(\mathbf{s}_t, \mathbf{a})/\beta}d\mathbf{a} \end{equation}\] <p>such that the policy \(\pi^*\) integrates to one. Unfortunately unlike in the bandits setting this relationship gives us no specific information about the reward function \(r\) at a single state action pair since the optimal policy optimizes for total future returns as estimated by \(Q\). To do so, we will need to consider the relationship between \(Q^*\) and \(r\).</p> <p>\noindent \textbf{From \(r\) to \(Q^*\).} The relationship between future returns and the current timestep is captured by the belmman equaitons which are satisifed by any valid Q-function. We write this below for the optimal policy \(\pi^*\) under the reward \(r\) with a KL divergence penalty:</p> \[\begin{equation}\label{eq:critic} Q^*(\mathbf{s}_t, \mathbf{a}_t) = \begin{cases} r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) + V^*(\mathbf{s}_{t+1}), &amp; \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\ r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1} \text{ is terminal} \end{cases} \end{equation}\] <p>We can then rearrange the bellman equation for the optimal \(Q\)-function in terms of the reward. This style of relationship was first explored by \citet{garg2022iqlearn} in imitation learning and later in \citet{hejna2024inverse} for preference-based RL. However, these works \emph{require} the use of a discount factor \(\gamma &lt; 1\) which is typically not used in RLHF. In the appendix we prove the following Lemma which shows that this relationship is indeed one-to-one in the token MDP as well.</p> <p>\begin{lemma} \label{lemma:r_to_q} Under mild assumptions, there is a bijection between reward functions \(r(\mathbf{s}_t, \mathbf{a}_t)\) and corresponding optimal Q-functions \(Q^*(\mathbf{s}_t, \mathbf{a}_t)\) in the token MDP. \end{lemma}</p> <p>This leads us to a rather interesting conclusion – that an LLM is \emph{always} the optimal soft Q-functions for \emph{some} reward function in the token MDP. Consider any LLM which outputs logits \(l_\theta\) and temperature parameter \(\beta\). As is common practice, we take the sampling policy \(\pi\) to be the softmax over tokens modulated by temperature parameter \(\beta\) – which is precisely \cref{eq:policy} where \(Q^* = l_\theta\) because the value optimal function \(V^*\) is precisely \(\beta \log Z(\mathbf{s}_t)\), normalizing the distribution. The corresponding reward function may not be smooth or well-behaved. Notably, the logits have a free parameter due to the softmax. While this free-parameter results in the same optimal policy per later arguments, it means the sequence of values may not be smooth. The question then becomes how to finetune the LLM such that it is the optimal Q-function for a reward function \(r\) that aligns with human preferences. To do so, we will complete our derivation of DPO in the token MDP.</p> <p>\textbf{DPO learns our best estimate of \(Q^*\).} Now that we have established a bijection between \(r\) and \(Q^*\), we can derive a token-level version of DPO to align the implicit reward, induced by the \(Q\) function represented by the language model, with that of the best estimate of reward, according to Bradley-Terry model in \cref{eq:dense-bradley-terry}. To do so, we need to represent the sum of rewards first in terms of the \(Q\)-function \(Q^*\), and then in terms of the policy \(\pi^*\). We complete the first step by inverting the Bellman equation in \cref{eq:critic} and substituting it into the sum of rewards over a trajectory \(\tau = \{\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{a}_{T-1}, \mathbf{s}_T\}\).</p> \[\begin{equation} \begin{aligned} \sum_{t=0}^{T-1}r(\mathbf{s}_t, \mathbf{a}_t) &amp;= \sum_{t=0}^{T-1}\left(Q^*(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) - V^*(\mathbf{s}_{t+1})\right) = \\ &amp;= Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1}Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_{t}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) \end{aligned} \end{equation}\] <p>The equality follows from \(V^*(\mathbf{s}_T)=0\) and re-arranging the sum to isolate \(t=0\). As \(V^*\) is written entirely in terms of \(Q^*\) and \(\beta\) per \cref{eq:value}, we have expressed the sum of return over the sequence just in terms of \(Q^*\). Next, we exchange \(Q^*\) for \(\pi^*\). We can log-linearize \cref{eq:policy} as \(\beta \log \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_t)\). This is equivalent to stating that the language model probabilities are just the softmax over \(l_\theta = Q^*\) with temperature \(\beta\). Continuing from the above, with this substitution we get</p> \[\begin{equation} = Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid\mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} =V^*(\mathbf{s}_0) + \sum_{t=0}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} \end{equation}\] <p>where the final step results from adding and subtracting \(V^*(\mathbf{s}_0)\) and applying the substitution again. Now, this representation for the sum of rewards in terms of the optimal policy can be directly substituted into the preference model in \cref{eq:dense-bradley-terry}, where the \(V^*(\mathbf{s}_0)\) term will cancel just as \(Z(\mathbf{x})\) did in the original DPO derivation assuming \(\tau^w\) and \(\tau^l\) start at the same state \(\mathbf{s}_0\), giving us the policy-induced preference model</p> \[\begin{equation}\label{eq:policy_pref} p_{\pi^*}(\tau^w \succeq \tau^l) = \sigma \left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)} - \sum_{t=0}^{M-1} \beta \log \frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right). \end{equation}\] <p>To derive the final DPO loss function, we can take the KL-divergence between the empirical preference model of our dataset \(p_\mathcal{D}\) and the preference model implied by a learned policy \(p_{\pi_\theta}\), \(\mathbb{D}_{\mathrm{KL}} (p_\mathcal{D} \mid\mid p_{\pi_\theta})\). This results in</p> \[\begin{equation}\label{eq:DPO} \mathcal{L}(\pi_{\theta}, \mathcal{D}) = -\mathbb{E}_{(\tau_w, \tau_l)\sim \mathcal{D}}\left[\log \sigma\left(\left( \sum_{t=0}^{N-1}\beta \log\frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}\right)- \left( \sum_{t=0}^{M-1}\beta \log\frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right)\right)\right] \end{equation}\] <p>In the next section we demonstrate that DPO can learn any dense reward function in the token-level MDP.</p> <h3 id="token-level-dpo-can-parameterize-any-dense-reward-function">Token-Level DPO Can Parameterize Any Dense Reward Function.</h3> <p>In the previous section we derived DPO using the bijection between reward functions and optimal \(Q\)-functions uniquely available in the token-level MDP. An alternative view of DPO casts it as restricting the learned reward function such that it belongs to the class optimal advantage functions \(A^*(\mathbf{s},\mathbf{a}) = Q^*(\mathbf{s},\mathbf{a}) - V^*(\mathbf{s})\) from which an optimal policy is readily obtained per \cref{eq:policy}. Here we show that this restriction does not limit the class of reward functions we can represent. We begin by expanding the definition of equivalency used in \citet{rafailov2023direct} to the broader class of potential-based reward shaping functions:</p> <p>\begin{definition}\label{def:equivalence} Two reward functions \(r(\mathbf{s}_t, \mathbf{a}_t)\) and \(r'(\mathbf{s}_t, \mathbf{a}_t)\) are equivalent if there exists a potential function \(\Phi(\mathbf{s})\), such that \(r'(\mathbf{s}_t, \mathbf{a}_t) =r(\mathbf{s}_t, \mathbf{a}_t) + \Phi(\mathbf{s}_{t+1}) - \Phi(\mathbf{s}_{t})\). \end{definition}</p> <p>In \citet{ng1999policy}’s seminal work, the authors proved that two equivalent reward functions defined per \cref{def:equivalence} have the same optimal policy. By log-linearizing the optimal policy fixed point in \cref{eq:policy} and substituting in the Bellman equation from \cref{eq:critic}, we have</p> \[\begin{equation}\label{eq:advantage} \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{t+1}) - V^*(\mathbf{s}_{t}). \end{equation}\] <p>This is precisely the optimal advantage function, where \(V^*\) directly follows the form of a potential shaping function. As also noted by contemporary works, using the advantage as reward preserves the optimal policy \citep{knox2024learning, hejna2024contrastive}. Unlike prior works, however, we demonstrate that this re-parameterization also leads to the same exact preference distribution as \(r\).</p> <p>\begin{theorem}\label{theorem:equiv} Given a reference policy \(\pi_{\mathrm{ref}}\) and a parameter \(\beta&gt;0\) all reward classes consistent with the Plackett-Luce (and Bradley-Terry) models in \cref{eq:dense-bradley-terry} can be represented with the a re-parameterization of the form</p> \[\begin{equation}\label{eq:reward_param} r(\mathbf{s}, \mathbf{a}) = \beta \log \pi(\mathbf{s} \mid \mathbf{a}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{s} \mid \mathbf{a}) \end{equation}\] <p>within the token MDP where \(V^*(\mathbf{s}_t) = 0\) for all terminal states. \end{theorem}</p> <p>\begin{proof} Above we derived the invariance of the optimal policy under the re-parameterization. The preference model can be shown to be invariant by following the same steps used to arrive at \cref{eq:policy_pref} in the last section. % as \(\sum_{t=0}^{N-1} \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = \sum_{t=0}^{N-1} r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{N}) - V^*(\mathbf{s}_{0}).\) Assuming we reach a terminal state, \(V^*(\mathbf{s}_{N}) = 0\) and assuming all responses begin at the same state \(V^*(\mathbf{s}_{0})\) is cancelled from the preference model. \looseness=-1 \end{proof} Interestingly, in practice, the potential function \(\Phi(\mathbf{s}_t)\) represents the free parameter in the logits of the language model. An equal shift along all logits yields the same policy, but different Q-functions and corresponding rewards. The above Theorem proves that all of these are in the same equivalence class and induce the same set of preferences.</p> <p>Moreover, this Theorem implies that we can use DPO to learn the optimal policy for any per-token reward function, provided preference queries start at the same state and end at a terminal state. In addition, DPO \emph{always} fits an optimal advantage function for \emph{some} reward which is responsible for credit assignment. Thus, the training data determines how close the learned advantage corresponds to that of the true reward. This is in contrast to methods that estimate the reward function and then additionally employ some policy improvement mechanism. Which algorithm performs better remains largely an open or empirical question.</p> <p>The above derivations cast a language model as a Q function in the discrete token-level MDP. While this interpretation does not generally hold in continuous spaces, we can extend many of our results to other specially structured MDPs, like those present in diffusion. See Appendix \ref{appendix:diffusion} for more thorough treatment.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)]]></summary></entry><entry><title type="html">RLHF - RTO</title><link href="https://rz-liu.github.io/blog/RTO/" rel="alternate" type="text/html" title="RLHF - RTO"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/RTO</id><content type="html" xml:base="https://rz-liu.github.io/blog/RTO/"><![CDATA[<h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 Reinforced Token Optimization (RTO)<d-cite key="RTO"></d-cite>，使用 MDP 框架解决 RLHF 问题。RTO 算法包括两个主要步骤：(i) token-wise reward learning，其中 RTO 基于偏好数据学习 token-wise 奖励；(ii) optimizing token-wise reward，通过 RL 训练方法（如 PPO）优化 token-wise 奖励。</p> <p>这篇工作和 DPO-Q*<d-cite key="DPO-Q*"></d-cite>是同期工作，区别在于这篇工作</p> <h3 id="motivation">Motivation</h3> <ul> <li>之前的方法使用的是 sentence-level 奖励，这种奖励 sparse，不利于学习。</li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>利用 fine-grained token-wise information 学习奖励函数</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li></li> <li></li> <li></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>设 \(x \in \mathcal{X}\) 表示从分布 \(\rho \in \Delta(\mathcal{X})\) 中采样的提示 (prompt)，\(y= (y_1,y_2,\ldots,y_h,\ldots)\) 表示由 LLM 生成的回答，其中 \(y_i\) 表示第 \(i\) 个 token。在 RLHF 中<d-cite key="christiano2017deep"></d-cite>，通常使用 Bradley-Terry (BT) 模型<d-cite key="BTModel"></d-cite>建模偏好：</p> \[\begin{equation}\label{eqn:bt} \begin{aligned} P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数, \(r\) 是定义在 <strong>sentence-level</strong> 的真实奖励函数 (ground-truth reward function)，用于评价整条回答 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和根据奖励训练 RL。在第一步，需要给定一个数据集 \(\mathcal{D} = \{(x, y^w, y^l)\}\)，其中 \(y^w\) 表示更好的回答，\(y^l\) 表示更差的回答。奖励函数通过最大似然估计（MLE）在数据集 \(\mathcal{D}\) 上学习：</p> \[\begin{equation}\label{eqn:mle_old} r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[ \log\big(\sigma(r(x, y^w) - r(x, y^l))\big) \big]. \end{equation}\] <p>在第二步，优化学习到的奖励 \(r_{\mathrm{MLE}}\)，同时确保更新后的 LLM 不会与参考模型 \(\pi_{\mathrm{ref}}\) 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 \(r_{\mathrm{MLE}}\) 进行优化，带有 KL 正则化项：</p> \[\begin{equation} \hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg], \end{equation}\] <p>其中，\(\beta &gt; 0\) 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 \(x\) 和一个包含 \(H\) 个 token 的回答 \(y = y_{1:H}\)，其中 \(H\) 是 token 的数量，现有的 PPO 开源实现将句子级奖励 \(r_{\mathrm{MLE}}(x, y)\) 分配给最后一个 token，并优化以下奖励：</p> \[\begin{equation}\label{eq:ppo:reward} \begin{aligned} {r}_{\mathrm{ppo}}(x, y_{1:h}) = \begin{cases} {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h \le H-1, \\ {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h = H, \end{cases} \end{aligned} \end{equation}\] <p>其中，\(\pi\) 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。</p> <h2 id="formulation">Formulation</h2> <h3 id="mdp-formulation-for-rlhf">MDP Formulation for RLHF</h3> <p>我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)\)，其中 \(\mathcal{S}\) 是状态空间，\(\mathcal{A}\) 是动作空间，\(\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) 是环境动态，\(r\) 表示奖励函数，\(\rho\) 表示初始状态分布，\(H\) 是交互步数的最大值。MDP 中的策略 \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\) 是从状态到动作分布的映射。环境 \(\mathcal{M}\) 与代理之间的交互可以描述如下。首先，从初始分布 \(\rho\) 中抽取起始状态 \(s_1\)。在第 \(h\) 步，代理观察到状态 \(s_h\) 并根据其策略选择动作 \(a_h\)。然后，环境从分布 \(\mathcal{P}(\cdot \mid s_h, a_h)\) 中抽取下一个状态 \(s_{h+1}\)。这种交互会持续到满足某个结束条件，该条件将在 \(H\) 步内触发。</p> <p>在大型语言模型（LLM）的标准文本生成过程中，每个状态 \(s_h = (x, y_{1:h-1})\) 包括提示 \(x\) 和到目前为止生成的所有响应 token。每个动作 \(a_h = y_{h}\) 表示词汇表中的一个 token。环境动态 \(\mathcal{P}\) 通常是已知的且确定的，这意味着给定 tokens \(s_h = (x, y_{1:h-1})\) 和 \(a_h = y_{h}\)，环境将转移到 \(s_{h+1} = (x, y_{1:h})\)。策略 \(\pi\) 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 \(h\)，\(\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})\)。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，\(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 表示 token-wise 奖励。生成的文本以特殊的句子结束 token \(\texttt{EoS}\) 结束，该 token 终止生成过程。</p> <p>在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 \(\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H\) 和 \(\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H\)，偏好由以下公式计算：</p> \[\begin{equation}\label{eq:BT:mdp} P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg). \end{equation}\] <p>相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。</p> <h3 id="learning-objective">Learning Objective</h3> <p>与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 \(\pi\)，我们定义其对应的正则化价值函数为</p> \[\begin{equation}\label{eq:q:v} \begin{aligned} V_\beta^{\pi}(s; r) &amp;= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg], \end{aligned} \end{equation}\] <p>其中，期望 \(\mathbb{E}_{\pi}\) 是针对策略 \(\pi\) 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 \(H\)，因此 \eqref{eq:q:v} 中的求和最多在 \(H\) 步结束。在本文的其余部分，我们可能会将 \(\sum_{h=1}^\infty\) 和 \(\sum_{h=1}^H\) 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 \(Q_\beta^\pi\) 是策略 \(\pi\) 的正则化价值函数的关联函数，定义如下</p> \[\begin{equation}\label{eq:bellman} Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)], \end{equation}\] <p>where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. The regularized optimal policy $\pi_\beta^<em>$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^</em>$ and $V_\beta^*$, respectively. By \eqref{eq:bellman}, it can be shown that</p> \[\begin{equation}\label{eq:optimal:policy} \pi_\beta^*(a \mid s) = \exp\{ (Q_\beta^*(s, a) - V_\beta^*(s))/\beta \}. \end{equation}\] <p>Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap:</p> \[\begin{equation}\label{eq:def:subopt} \SubOpt(\hat{\pi}) = \mathbb{E}_{s \sim \rho} [V_{\beta}^*(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^*(\rho) - V_\beta^{\hat{\pi}}(\rho), \end{equation}\] <p>where we use the shorthand $V_\beta^\pi(\rho) = \mathbb{E}<em>{s \sim \rho}[V</em>\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure $d^\pi(s) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.</p> <h3 id="advantages-of-token-wise-mdp-over-sentence-wise-bandit">Advantages of Token-Wise MDP over Sentence-Wise Bandit</h3> <p>\label{sec:token:reward}</p> <table> <tbody> <tr> <td>直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A =</td> <td>\mathcal{A}</td> <td>\(的确定性 MDP。我们使用自回归策略\)\pi^<em>\(来表示强大的 LLM 策略，例如 GPT-4。固定提示\)x\(，给定回答\)(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)\(，由\)\pi^</em>$$ 提供的评估为</td> </tr> </tbody> </table> \[\begin{equation} P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}. \end{equation}\] <p>通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 \(r_s\) 和 token 级奖励 \(r_t\) 可以分别由以下公式得到</p> \[\begin{equation}\label{eq:reward:example} r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}). \end{equation}\] <p>直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 \(r_s(x, y) = \sum_{h = 1}^H r_t((x, y_{1:h-1}), y_h)\)。</p> <h3 id="method">Method</h3> <p>\label{sec:alg_theory}</p> <p>Motivated by Section~\ref{sec:formulation}, we tackle RLHF by treating it as an MDP problem. Under this MDP framework, we aim to develop an algorithmic framework that fully utilizes the token-level information. To this end, we develop the Reinforced Token Optimization (\texttt{RTO}) algorithm. At a high level, \texttt{RTO} consists of two main steps: {\color{bluee}(i) token-wise reward learning}, where \texttt{RTO} learns a token-wise reward based on the preference data; and {\color{bluee} (ii) optimizing token-wise reward} through RL training methods such as PPO. In Section~\ref{sec:theory:version}, we provide a theoretically grounded version of \texttt{RTO} with guaranteed sample complexity. To align more closely with practice, we present a practical implementation of \texttt{RTO} in Section~\ref{sec:practical:version}.</p> <p>\subsection{Theoretical Version with Sample Complexity Guarantee} \label{sec:theory:version}</p> <p>We focus on the offline setting and assume the access to an offline dataset $\cD = {(\tau^w, \tau^l)}$ that contains several trajectory pairs, where $\tau^w = {(s_h^w, a_h^w)}<em>{h=1}^H$ is preferred over $\tau^l = {(s_h^l, a_h^l)}</em>{h=1}^H$. Each pair of trajectories shares the same initial state/prompt (i.e., $s_1^w = s_1^l$), but differs in the subsequent tokens. We also assume that the reward function is linear, and our following results are ready to be extended to general function approximation \citep{chen2022human,wang2023rlhf,zhan2023provable}. \begin{assumption}[Linear Reward] \label{assumption:linear} We assume that the reward function $r$ is linear, i.e., $r(s, a) = \phi(s, a)^\top \theta^<em>$ for some known feature $\phi: \cS \times \cA \rightarrow \RR^d$ and unknown vector $\theta^</em> \in \RR^d$. We also assume that $|\phi(\cdot, \cdot)|_2 \le L$ and $| \theta^* |_2 \le B$. \end{assumption} Following the standard reward learning pipeline \citep{ouyang2022training}, we learn the reward function via maximum likelihood estimation (MLE). Specifically, if we parametrize the reward function by $\theta$, then the MLE is given by</p> \[\begin{equation}\label{eq:mle} \theta_{\mathrm{MLE}} = \argmax_{\|\theta\|_2 \le B} \mathcal{L}_{\mathcal{D}}(\theta), \quad \text{where } \mathcal{L}_{\mathcal{D}}(\theta) = \sum_{(\tau^w, \tau^l) \in \mathcal{D}} \bigg[ \log \Big( \sigma \big(\sum_{h=1}^H r_\theta(s_h^w, a_h^w) - \sum_{h=1}^H r_\theta(s_h^l, a_h^l) \big) \Big) \bigg]. \end{equation}\] <p>Inspired by previous literature in offline RL \citep{jin2021pessimism,rashidinejad2021bridging,xiong2022nearly,zhu2023principled,zhan2023provable}, given the MLE $\theta_{\mle}$, we construct the pessimistic token-wise reward estimation as # \label{eq:pessimistic:reward} \hat{r}(s, a) = \phi(s, a)^\top \theta_{\mle} - \varrho \cdot |\phi(s, a)|<em>{\Sigma</em>\cD^{-1}}, # where $\Sigma_\cD = \sum_{(\tau^1, \tau^2) \in \cD} [\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ) (\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ))^\top] + \lambda I_d$, $\lambda &gt; 0$ is a tuning parameter, and $\varrho$ is a problem-dependent coefficient will be specified in Theorem~\ref{thm:offline} and \eqref{eq:varrho}. Finally, \texttt{RTO} outputs the optimal policy $\hat{\pi}$ with respect to $\hat{r}$, i.e., $\hat{\pi} = \argmax_\pi V_\beta^\pi(s; \hat{r})$ for any $s \in \cS$. The pseudocode of \texttt{RTO} is given in Algorithm~\ref{alg:offline}.</p> <p>\begin{algorithm}[t] \caption{Reinforced Token Optimization (Theoretical Version)} \label{alg:offline} \begin{algorithmic}[1] \STATE \textbf{Input:} Offline dataset $\cD$, $\lambda &gt; 0$, $\beta &gt; 0$, and problem dependent coefficient $\varrho$. \STATE Compute $\theta_{\mle}$ based on $\cD$ by maximizing the loglikelihood given in \eqref{eq:mle}. \STATE Calculate the pessimistic reward $\hat{r}$ via \eqref{eq:pessimistic:reward}. {\color{bluee}\COMMENT{token-wise reward learning}} \STATE Compute the corresponding optimal policy $\hat{\pi}$ with respect to $\hat{r}$. {\color{bluee}\COMMENT{optimizing token-wise reward}} \STATE \textbf{Output:} policy $\hat{\pi}$. \end{algorithmic} \end{algorithm}</p> <p>\begin{theorem} \label{thm:offline} Suppose Assumption~\ref{assumption:linear} holds. For $\beta &gt; 0$, $\lambda &gt; 0$, $\delta \in (0, 1)$, if we choose $\varrho = \tilde{\cO}(\sqrt{d})$ (see~\eqref{eq:varrho}), then the output policy $\hat{\pi}$ of Algorithm~\ref{alg:offline} satisfies $ \SubOpt(\hat{\pi}) \le 2 \varrho \cdot \mathbb{E}<em>{(s, a) \sim d^*} \big[|\phi(s, a) |</em>{\Sigma_\cD^{-1}} \big] - \beta \cdot \mathbb{E}<em>{s \sim d^*} \big[\mathrm{KL}\big(\pi</em>\beta^*(\cdot \mid s) | \hat{\pi}(\cdot \mid s) \big) \big]. $ \end{theorem}</p> <p>\begin{proof} See Appendix~\ref{appendix:pf:thm:offline} for a detailed proof. \end{proof}</p> <table> <tbody> <tr> <td>The first term in Theorem \ref{thm:offline} measures how well the offline dataset covers the trajectory generated by the policy $\pi_\beta^*$. Typically, this term decreases at a rate of $</td> <td>\cD</td> <td>^{-1/2}$ under the mild partial coverage assumption~\citep{jin2021pessimism,uehara2021pessimistic,xiong2022nearly,zhu2023principled,zhan2023provable}, where $</td> <td>\cD</td> <td>$ is the size of the offline dataset. The second KL term is always negative, and it arises from the goal of learning a regularized value. We also remark that our algorithm relies on the known transition kernel to compute the exact optimal policy with respect to $\hat{r}$. While this is natural in the context of large language models, we provide insights on how to extend our findings to stochastic regularized MDPs and the variant of our \texttt{RTO} algorithm in Appendix~\ref{appendix:variant}.</td> </tr> </tbody> </table> <p>There have also been previous works \citep{pacchiano2021dueling,chen2022human,wang2023rlhf,li2023reinforcement,zhan2023provable} studying RLHF under the MDP framework, also known as dueling RL and preference-based RL. However, these works do not consider the KL constraint, which is an essential component of RLHF. Furthermore, they do not explicitly emphasize the superiority of the MDP framework over the contextual dueling bandit problem in the context of LLMs, and their proposed algorithms lack practical implementation. In contrast, we will provide a practical implementation of our algorithm, demonstrating the practicality of our approach.</p> <p>\subsection{Practical Implementation} \label{sec:practical:version} In this subsection, we shift our focus to developing a practical version of \texttt{RTO}. The key challenge in implementing \texttt{RTO} in Algorithm~\ref{alg:offline} lies in learning the token-wise reward to be optimized from the offline data. In the most popular frameworks outlined in Instruct-GPT \citep{ouyang2022training}, Claude \citep{bai2022training}, and LLaMA2 \citep{touvron2023llama} projects replace the last layer of the LLM with a linear layer for a scalar output and maximize the log-likelihood as in \eqref{eqn:mle_old}. However, this approach gives only a sentence-level reward. To bridge the gap in the literature, we present our practical version of \texttt{RTO} in Algorithm~\ref{alg:offline:practical}, which features a novel calculation of token-wise reward. Our key observation is that, given a trajectory $\tau = {(s_h, a_h)}_{h=1}^H$, we have</p> \[\begin{equation}\label{eq:prac:1} \begin{aligned} \sum_{h=1}^H \beta \log \frac{\pi_\beta^*(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} &amp;= \sum_{h=1}^H \big(Q_\beta^*(s_h, a_h) - V_\beta^*(s_h) - \log \pi_{\mathrm{ref}}(a_h \mid s_h) \big) \\ &amp;= \sum_{h=1}^H r(s_h, a_h) - V_\beta^*(s_1) + \underbrace{\sum_{h=1}^{H-1} \big( \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s_h, a_h)}[V_\beta^*(s')] - V_\beta^*(s_{h+1}) \big)}_{(\star)}, \end{aligned} \end{equation}\] <p>where the first equality uses the closed-form of optimal policy $\pi_\beta^<em>(a \mid s) = \exp{ (Q_\beta^</em>(s, a) - V_\beta^<em>(s))/\beta }$ in \eqref{eq:optimal:policy}, and the second equality follows from the fact that $Q_\beta^{\pi}(s, a) = r_\beta(s, a) + \mathbb{E}_{s’ \sim \cP(\cdot \mid s, a)}[V_\beta^\pi(s’)]$ in \eqref{eq:bellman} with $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. We focus on the typical LLM generation scenario where the transition kernel is deterministic. Then we have $(\star) = 0$ in \eqref{eq:prac:1}, yielding that $ \sum_{h = 1}^H r(s_h, a_h) = \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} + V_\beta^<em>(s_1) . $ Building upon this result and combining it with the definition of the BT model in \eqref{eq:BT:mdp}, for any trajectory pair ${\tau^j = {(s_{h}^j, a_{h}^j)}_{h=1}^H}_{j=1}^2$ satisfying $s_1^1 = s_1^2$, we have # \label{eq:prac:2} \PP(\tau^1 \succ \tau^2) = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg) = \sigma \bigg( \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h^1 \mid s_h^1)}{\pi_{\mathrm{ref}}(a_h^1 \mid s_h^1)} - \sum_{h=1}^H \beta \log \frac{\pi_\beta^<em>(a_h^2 \mid s_h^2)}{\pi_{\mathrm{ref}}(a_h^2 \mid s_h^2)} \bigg). # An interesting observation is that, based on the autoregressive nature of policies, \eqref{eq:prac:2} aligns with the learning objective of DPO proposed by \citet{rafailov2023direct}, but under the token-level MDP instead of the sentence-level bandit setup. Similar to the bandit setting where the learning objective is equivalent to a BT model with sentence-wise reward $r^</em>(x, y) = \beta \log \frac{\pi_{\beta}^<em>(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$ \citep{rafailov2023direct}, \eqref{eq:prac:2} shows that the learning objective in token-wise MDP equivalents to a BT model with a token-wise reward function # \label{eq:prac:3} r^</em>(s_h = (x, y_{1:h-1}), a_h = y_h) = \beta \log \frac{\pi_\beta^<em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} = \beta \log \frac{\pi_\beta^</em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})}, # where $x$ is the prompt, $y_{1:h-1}$ is the tokens generated so far, and $y_h$ is the token chosen at the current step. In contrast to the previous PPO implementation with sparse reward in \eqref{eq:ppo:reward}, we will assign the token-wise reward function defined in~\eqref{eq:prac:3} to each step. Formally, for any $h$, we define \begin{equation} \begin{aligned}\label{eq:prac:5} &amp;\beta_1 \log \frac{\pi_\beta^<em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} <br/> &amp; \qquad \approx \beta_1 \log \frac{\pi_{\mathrm{dpo}}(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} := r_{\mathrm{rto}}((x, y_{1:h-1}),y_h) \end{aligned} \end{equation} as the token-wise reward used by \texttt{RTO}, where $\beta_1$ and $\beta_2$ are tuning parameters, and $\pi$ is the current policy to be updated. In the last step of \eqref{eq:prac:5}, we use $\pi_{\mathrm{dpo}}$, the policy learned by DPO, as a proxy for the unknown $\pi_\beta^</em>$. Finally, we employ PPO to optimize the token-wise reward $r_{\mathrm{rto}}$ in \eqref{eq:prac:5}.</p>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF]]></summary></entry><entry><title type="html">RLHF - IPL</title><link href="https://rz-liu.github.io/blog/IPL/" rel="alternate" type="text/html" title="RLHF - IPL"/><published>2024-06-08T00:00:00+00:00</published><updated>2024-06-08T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/IPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/IPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="IPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="rl">RL</h3> <p>Contractive Bellman operator</p> \[\begin{equation}\label{eq:bellman} (\mathcal{B}^\pi_r Q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot\mid s,a)}[V^\pi(s')], \end{equation}\] <h2 id="method">Method</h2> <h3 id="inverse-soft-bellman-operator">Inverse Soft-Bellman Operator</h3> \[\begin{equation}\label{eq:inverse_bellman} (\mathcal{T}^\pi Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'\sim p(\cdot\mid s,a)}[V^\pi(s')]. \end{equation}\] <p>记 \(r_{Q^\pi}\) 为使用 \(Q^\pi\) 导出的隐式奖励函数 (implicit reward function)，即 \(r_{Q^\pi}(s,a) = Q^\pi(s,a)\)，则我们可以得到使用 \(Q^\pi\) 导出的隐式奖励函数的偏好分布 (preference distribution) \(P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}]\):</p> \[\begin{equation}\label{eq:q_preference} P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) }{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) + \exp \sum_t (\mathcal{T}^\pi Q)(s_t^{2}, a_t^{2})}. \end{equation}\] <h3 id="optimal-inverse-bellman-operator">Optimal Inverse Bellman Operator</h3> \[\begin{equation} (\mathcal{T}^* Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'}[V^{\text{targ}}(s')], \ \text{ where } V^\text{targ}(s) \text{ is estimated as in } \mathcal{B}^*_r. \end{equation}\] <h3 id="regularization">Regularization</h3> \[\begin{equation} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{1}, \sigma^{2},y \sim \mathcal{D}_p} \left[y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}])\right]. \end{equation}\] <p>其中 \(P_{Q^*}\) 由将 \(\mathcal{T}^*Q\) 替换到 \eqref{eq:q_preference} 得到。</p> <p>但是仅仅使用此损失函数会导致较差的结果，因为此目标函数是不受约束的，由于 BT model 对于 shift 是不变的，即对于所有的奖励函数 \(r(s,a)\)，\(Q(s,a)\) 都是一样的。为了解决这个问题，IPL 引入一个凸正则项 \(\psi(\cdot)\)，对隐式奖励函数 \(r_{Q^\pi} = \mathcal{T^\pi}Q\) 进行正则化，得到正则化的偏好损失函数：</p> \[\begin{equation}\label{eq:ipl} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{(1)}, \sigma^{(2)},y \sim \mathcal{D}_p} \left[ y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}]) \right] + \lambda \psi(\mathcal{T}^*Q) \end{equation}\] \[\begin{equation}\label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\]]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Inverse Preference Learning: Preference-based RL without a Reward Function]]></summary></entry><entry><title type="html">RLHF - CPL</title><link href="https://rz-liu.github.io/blog/CPL/" rel="alternate" type="text/html" title="RLHF - CPL"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/CPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/CPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="CPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h2 id="method">Method</h2> <p>###</p> \[\begin{equation}\label{eq:adv_to_pol1} A^*(s,a) = \alpha \log \pi^*(a|s). \end{equation}\] \[\begin{equation} \begin{aligned} A(s_t,a_t) &amp;= Q(s_t,a_t) - V(s_t) \\ &amp;= r_t + \gamma V(s_{t+1}) - V(s_t).\\ \sum_{h=t}^{H-1} A(s_h,a_h) &amp;= \sum_{h=t}^{H-1} \left[ r_h + \gamma V(s_{h+1}) - V(s_h) \right] \\ &amp;= \sum_{h=t}^{H-1} r_h + \gamma V(s_H) - V(s_t) \\ \end{aligned} \end{equation}\] \[\begin{equation} P_{A^*}\left[\sigma^+ \succ \sigma^- \right] = \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t)}{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi^*(a^-_t|s^-_t)}. \end{equation}\] \[\begin{equation}\label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <h3 id="practical-implementation">Practical Implementation</h3> <p>Regularization term</p> \[\begin{equation}\label{eq:reg_loss} \mathcal{L}_{\text{CPL}({\color{red}{\lambda}})}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{\mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp {\color{red}{\lambda}} \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <p>Python 实现如下，有点没搞懂为什么要用这个形式，是为了数值稳定性？</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Apply the log-sum-exp trick.
</span>    <span class="c1"># y = 1 if we prefer x2 to x1
</span>    <span class="c1"># We need to implement the numerical stability trick.
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">max21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">max12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max21</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span> <span class="o">-</span> <span class="n">max21</span><span class="p">))</span> <span class="o">+</span> <span class="n">max21</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max12</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span> <span class="o">-</span> <span class="n">max12</span><span class="p">))</span> <span class="o">+</span> <span class="n">max12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">nlp21</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">nlp12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>正常来说，不加这个 trick 的话，应该是这样的：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># y = 1 if we prefer x2 to x1
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">))</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning]]></summary></entry></feed>
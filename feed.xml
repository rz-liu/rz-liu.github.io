<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rz-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rz-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-13T15:01:12+00:00</updated><id>https://rz-liu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RLHF - RTO</title><link href="https://rz-liu.github.io/blog/2024/RTO/" rel="alternate" type="text/html" title="RLHF - RTO"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/RTO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/RTO/"><![CDATA[<h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>在本章，我们介绍标准的 RLHF 范式。设 \(x \in \mathcal{X}\) 表示从分布 \(\rho \in \Delta(\mathcal{X})\) 中采样的提示，\(y= (y_1,y_2,\ldots,y_h,\ldots)\) 表示由 LLM 生成的回答，其中 \(y_i\) 表示第 \(i\) 个 token。在实践中，通常假设 <d-cite key="christiano2017deep"></d-cite> 奖励信号根据 Bradley-Terry (BT) 模型 <d-cite key="BTModel"></d-cite> 生成：</p> \[\begin{equation} \label{eqn:bt} \begin{aligned} P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数, \(r\) 是定义在 <strong>sentence-level</strong> 的 ground-truth reward function，用于评价整个回复 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和基于奖励训练 RL。在第一步中，学习者被给定一个数据集 \(\mathcal{D} = \{(x, y^w, y^l)\}\)，其中 \(y^w\) 表示更好的回复，\(y^l\) 表示更差的回复。奖励函数通过最大似然估计（MLE）在数据集 \(\mathcal{D}\) 上学习：</p> \[\begin{equation} \label{eqn:mle_old} r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big]. \end{equation}\] <p>在第二步，优化学习到的奖励 \(r_{\mathrm{MLE}}\)，同时确保更新后的 LLM 不会与参考模型 \(\pi_{\mathrm{ref}}\) 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 \(r_{\mathrm{MLE}}\) 进行优化，带有 KL 正则化项：</p> \[\begin{equation} \hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg], \end{equation}\] <p>其中，\(\beta &gt; 0\) 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 \(x\) 和一个包含 \(H\) 个 token 的回复 \(y = y_{1:H}\)，其中 \(H\) 是 token 的数量，现有的 PPO 开源实现将句子级奖励 \(r_{\mathrm{MLE}}(x, y)\) 分配给最后一个 token，并优化以下奖励：</p> \[\begin{equation} \label{eq:ppo:reward} \begin{aligned} {r}_{\mathrm{ppo}}(x, y_{1:h}) = \begin{cases} {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h \le H-1, \\ {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h = H, \end{cases} \end{aligned} \end{equation}\] <p>其中，\(\pi\) 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。</p> <h2 id="formulation">Formulation</h2> <h3 id="mdp-formulation-for-rlhf">MDP Formulation for RLHF</h3> <p>我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)\)，其中 \(\mathcal{S}\) 是状态空间，\(\mathcal{A}\) 是动作空间，\(\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) 是环境动态，\(r\) 表示奖励函数，\(\rho\) 表示初始状态分布，\(H\) 是交互步数的最大值。MDP 中的策略 \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\) 是从状态到动作分布的映射。环境 \(\mathcal{M}\) 与代理之间的交互可以描述如下。首先，从初始分布 \(\rho\) 中抽取起始状态 \(s_1\)。在第 \(h\) 步，代理观察到状态 \(s_h\) 并根据其策略选择动作 \(a_h\)。然后，环境从分布 \(\mathcal{P}(\cdot \mid s_h, a_h)\) 中抽取下一个状态 \(s_{h+1}\)。这种交互会持续到满足某个结束条件，该条件将在 \(H\) 步内触发。</p> <p>在大型语言模型（LLM）的标准文本生成过程中，每个状态 \(s_h = (x, y_{1:h-1})\) 包括提示 \(x\) 和到目前为止生成的所有响应 token。每个动作 \(a_h = y_{h}\) 表示词汇表中的一个 token。环境动态 \(\mathcal{P}\) 通常是已知的且确定的，这意味着给定 tokens \(s_h = (x, y_{1:h-1})\) 和 \(a_h = y_{h}\)，环境将转移到 \(s_{h+1} = (x, y_{1:h})\)。策略 \(\pi\) 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 \(h\)，\(\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})\)。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，\(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 表示 token-wise 奖励。生成的文本以特殊的句子结束 token \(\texttt{EoS}\) 结束，该 token 终止生成过程。</p> <p>在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 \(\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H\) 和 \(\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H\)，偏好由以下公式计算：</p> \[\begin{equation} \label{eq:BT:mdp} P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg). \end{equation}\] <p>相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。</p> <h3 id="learning-objective">Learning Objective</h3> <p>与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 \(\pi\)，我们定义其对应的正则化价值函数为</p> \[\begin{equation} \label{eq:q:v} \begin{aligned} V_\beta^{\pi}(s; r) &amp;= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg], \end{aligned} \end{equation}\] <p>其中，期望 \(\mathbb{E}_{\pi}\) 是针对策略 \(\pi\) 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 \(H\)，因此 \eqref{eq:q:v} 中的求和最多在 \(H\) 步结束。在本文的其余部分，我们可能会将 \(\sum_{h=1}^\infty\) 和 \(\sum_{h=1}^H\) 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 \(Q_\beta^\pi\) 是策略 \(\pi\) 的正则化价值函数的关联函数，定义如下</p> \[\begin{equation} \label{eq:bellman} Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)], \end{equation}\] <p>where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. The regularized optimal policy $\pi_\beta^<em>$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^</em>$ and $V_\beta^<em>$, respectively. By \eqref{eq:bellman}, it can be shown that # \label{eq:optimal:policy} \pi_\beta^</em>(a \mid s) = \exp{ (Q_\beta^<em>(s, a) - V_\beta^</em>(s))/\beta }. # Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap: # \label{eq:def:subopt} \SubOpt(\hat{\pi}) = \mathbb{E}<em>{s \sim \rho} [V</em>{\beta}^<em>(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^</em>(\rho) - V_\beta^{\hat{\pi}}(\rho), # where we use the shorthand $V_\beta^\pi(\rho) = \mathbb{E}<em>{s \sim \rho}[V</em>\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure $d^\pi(s) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \mathbb{E}<em>{s_1 \sim \rho} [ \sum</em>{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.</p> <h3 id="advantages-of-token-wise-mdp-over-sentence-wise-bandit">Advantages of Token-Wise MDP over Sentence-Wise Bandit</h3> <p>\label{sec:token:reward}</p> <table> <tbody> <tr> <td>直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A =</td> <td>\mathcal{A}</td> <td>\(的确定性 MDP。我们使用自回归策略\)\pi^<em>\(来表示强大的 LLM 策略，例如 GPT-4。固定提示\)x\(，给定回复\)(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)\(，由\)\pi^</em>$$ 提供的评估为</td> </tr> </tbody> </table> \[\begin{equation} P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}. \end{equation}\] <p>通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 \(r_s\) 和 token 级奖励 \(r_t\) 可以分别由以下公式得到</p> \[\begin{equation} \label{eq:reward:example} r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}). \end{equation}\] <p>直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 \(r_s(x, y) = \sum_{h = 1}^H r_t( (x, y_{1:h-1}), y_h)\)。</p> <h3 id="method">Method</h3> <p>\label{sec:alg_theory}</p> <p>Motivated by Section~\ref{sec:formulation}, we tackle RLHF by treating it as an MDP problem. Under this MDP framework, we aim to develop an algorithmic framework that fully utilizes the token-level information. To this end, we develop the Reinforced Token Optimization (\texttt{RTO}) algorithm. At a high level, \texttt{RTO} consists of two main steps: {\color{bluee}(i) token-wise reward learning}, where \texttt{RTO} learns a token-wise reward based on the preference data; and {\color{bluee} (ii) optimizing token-wise reward} through RL training methods such as PPO. In Section~\ref{sec:theory:version}, we provide a theoretically grounded version of \texttt{RTO} with guaranteed sample complexity. To align more closely with practice, we present a practical implementation of \texttt{RTO} in Section~\ref{sec:practical:version}.</p> <p>\subsection{Theoretical Version with Sample Complexity Guarantee} \label{sec:theory:version}</p> <p>We focus on the offline setting and assume the access to an offline dataset $\cD = {(\tau^w, \tau^l)}$ that contains several trajectory pairs, where $\tau^w = {(s_h^w, a_h^w)}<em>{h=1}^H$ is preferred over $\tau^l = {(s_h^l, a_h^l)}</em>{h=1}^H$. Each pair of trajectories shares the same initial state/prompt (i.e., $s_1^w = s_1^l$), but differs in the subsequent tokens. We also assume that the reward function is linear, and our following results are ready to be extended to general function approximation \citep{chen2022human,wang2023rlhf,zhan2023provable}. \begin{assumption}[Linear Reward] \label{assumption:linear} We assume that the reward function $r$ is linear, i.e., $r(s, a) = \phi(s, a)^\top \theta^<em>$ for some known feature $\phi: \cS \times \cA \rightarrow \RR^d$ and unknown vector $\theta^</em> \in \RR^d$. We also assume that $|\phi(\cdot, \cdot)|<em>2 \le L$ and $| \theta^* |_2 \le B$. \end{assumption} Following the standard reward learning pipeline \citep{ouyang2022training}, we learn the reward function via maximum likelihood estimation (MLE). Specifically, if we parametrize the reward function by $\theta$, then the MLE is given by # \label{eq:mle} \theta</em>{\mle} = \argmax_{|\theta|<em>2 \le B} \cL</em>{\cD}(\theta), \quad \text{where } \cL_\cD(\theta) = \sum_{(\tau^w, \tau^l) \in \cD} &amp; \bigg[ \log \Big( \sigma \big(\sum_{h=1}^H r_\theta(s_h^w, a_h^w) - \sum_{h=1}^H r_\theta(s_h^l, a_h^l) \big) \Big) \bigg]. # Inspired by previous literature in offline RL \citep{jin2021pessimism,rashidinejad2021bridging,xiong2022nearly,zhu2023principled,zhan2023provable}, given the MLE $\theta_{\mle}$, we construct the pessimistic token-wise reward estimation as # \label{eq:pessimistic:reward} \hat{r}(s, a) = \phi(s, a)^\top \theta_{\mle} - \varrho \cdot |\phi(s, a)|<em>{\Sigma</em>\cD^{-1}}, # where $\Sigma_\cD = \sum_{(\tau^1, \tau^2) \in \cD} [\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ) (\sum_{h=1}^H (\phi(s_h^1, a_h^1) - \phi(s_h^2, a_h^2) ))^\top] + \lambda I_d$, $\lambda &gt; 0$ is a tuning parameter, and $\varrho$ is a problem-dependent coefficient will be specified in Theorem~\ref{thm:offline} and \eqref{eq:varrho}. Finally, \texttt{RTO} outputs the optimal policy $\hat{\pi}$ with respect to $\hat{r}$, i.e., $\hat{\pi} = \argmax_\pi V_\beta^\pi(s; \hat{r})$ for any $s \in \cS$. The pseudocode of \texttt{RTO} is given in Algorithm~\ref{alg:offline}.</p> <p>\begin{algorithm}[t] \caption{Reinforced Token Optimization (Theoretical Version)} \label{alg:offline} \begin{algorithmic}[1] \STATE \textbf{Input:} Offline dataset $\cD$, $\lambda &gt; 0$, $\beta &gt; 0$, and problem dependent coefficient $\varrho$. \STATE Compute $\theta_{\mle}$ based on $\cD$ by maximizing the loglikelihood given in \eqref{eq:mle}. \STATE Calculate the pessimistic reward $\hat{r}$ via \eqref{eq:pessimistic:reward}. {\color{bluee}\COMMENT{token-wise reward learning}} \STATE Compute the corresponding optimal policy $\hat{\pi}$ with respect to $\hat{r}$. {\color{bluee}\COMMENT{optimizing token-wise reward}} \STATE \textbf{Output:} policy $\hat{\pi}$. \end{algorithmic} \end{algorithm}</p> <p>\begin{theorem} \label{thm:offline} Suppose Assumption~\ref{assumption:linear} holds. For $\beta &gt; 0$, $\lambda &gt; 0$, $\delta \in (0, 1)$, if we choose $\varrho = \tilde{\cO}(\sqrt{d})$ (see~\eqref{eq:varrho}), then the output policy $\hat{\pi}$ of Algorithm~\ref{alg:offline} satisfies $ \SubOpt(\hat{\pi}) \le 2 \varrho \cdot \mathbb{E}<em>{(s, a) \sim d^*} \big[|\phi(s, a) |</em>{\Sigma_\cD^{-1}} \big] - \beta \cdot \mathbb{E}<em>{s \sim d^*} \big[\mathrm{KL}\big(\pi</em>\beta^*(\cdot \mid s) | \hat{\pi}(\cdot \mid s) \big) \big]. $ \end{theorem}</p> <p>\begin{proof} See Appendix~\ref{appendix:pf:thm:offline} for a detailed proof. \end{proof}</p> <table> <tbody> <tr> <td>The first term in Theorem \ref{thm:offline} measures how well the offline dataset covers the trajectory generated by the policy $\pi_\beta^*$. Typically, this term decreases at a rate of $</td> <td>\cD</td> <td>^{-1/2}$ under the mild partial coverage assumption~\citep{jin2021pessimism,uehara2021pessimistic,xiong2022nearly,zhu2023principled,zhan2023provable}, where $</td> <td>\cD</td> <td>$ is the size of the offline dataset. The second KL term is always negative, and it arises from the goal of learning a regularized value. We also remark that our algorithm relies on the known transition kernel to compute the exact optimal policy with respect to $\hat{r}$. While this is natural in the context of large language models, we provide insights on how to extend our findings to stochastic regularized MDPs and the variant of our \texttt{RTO} algorithm in Appendix~\ref{appendix:variant}.</td> </tr> </tbody> </table> <p>There have also been previous works \citep{pacchiano2021dueling,chen2022human,wang2023rlhf,li2023reinforcement,zhan2023provable} studying RLHF under the MDP framework, also known as dueling RL and preference-based RL. However, these works do not consider the KL constraint, which is an essential component of RLHF. Furthermore, they do not explicitly emphasize the superiority of the MDP framework over the contextual dueling bandit problem in the context of LLMs, and their proposed algorithms lack practical implementation. In contrast, we will provide a practical implementation of our algorithm, demonstrating the practicality of our approach.</p> <p>\subsection{Practical Implementation} \label{sec:practical:version} In this subsection, we shift our focus to developing a practical version of \texttt{RTO}. The key challenge in implementing \texttt{RTO} in Algorithm~\ref{alg:offline} lies in learning the token-wise reward to be optimized from the offline data. In the most popular frameworks outlined in Instruct-GPT \citep{ouyang2022training}, Claude \citep{bai2022training}, and LLaMA2 \citep{touvron2023llama} projects replace the last layer of the LLM with a linear layer for a scalar output and maximize the log-likelihood as in \eqref{eqn:mle_old}. However, this approach gives only a sentence-level reward. To bridge the gap in the literature, we present our practical version of \texttt{RTO} in Algorithm~\ref{alg:offline:practical}, which features a novel calculation of token-wise reward. Our key observation is that, given a trajectory $\tau = {(s_h, a_h)}<em>{h=1}^H$, we have # \label{eq:prac:1} \sum</em>{h=1}^H \beta \log \frac{\pi_\beta^<em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} &amp;= \sum_{h = 1}^H \big(Q_\beta^</em>(s_h, a_h) - V_\beta^<em>(s_h) - \log \pi_{\mathrm{ref}}(a_h \mid s_h) \big) \notag <br/> &amp; = \sum_{h = 1}^H r(s_h, a_h) - V_\beta^</em>(s_1) + \underbrace{\sum_{h = 1}^{H-1} \big( \mathbb{E}<em>{s’ \sim \cP(\cdot \mid s_h, a_h)}[V</em>\beta^<em>(s’)] - V_\beta^</em>(s_{h+1}) \big)}<em>{(\star)}, # where the first equality uses the closed-form of optimal policy $\pi</em>\beta^<em>(a \mid s) = \exp{ (Q_\beta^</em>(s, a) - V_\beta^<em>(s))/\beta }$ in \eqref{eq:optimal:policy}, and the second equality follows from the fact that $Q_\beta^{\pi}(s, a) = r_\beta(s, a) + \mathbb{E}_{s’ \sim \cP(\cdot \mid s, a)}[V_\beta^\pi(s’)]$ in \eqref{eq:bellman} with $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. We focus on the typical LLM generation scenario where the transition kernel is deterministic. Then we have $(\star) = 0$ in \eqref{eq:prac:1}, yielding that $ \sum_{h = 1}^H r(s_h, a_h) = \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} + V_\beta^<em>(s_1) . $ Building upon this result and combining it with the definition of the BT model in \eqref{eq:BT:mdp}, for any trajectory pair ${\tau^j = {(s_{h}^j, a_{h}^j)}_{h=1}^H}_{j=1}^2$ satisfying $s_1^1 = s_1^2$, we have # \label{eq:prac:2} \PP(\tau^1 \succ \tau^2) = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg) = \sigma \bigg( \sum_{h=1}^H \beta \log \frac{\pi_\beta^</em>(a_h^1 \mid s_h^1)}{\pi_{\mathrm{ref}}(a_h^1 \mid s_h^1)} - \sum_{h=1}^H \beta \log \frac{\pi_\beta^<em>(a_h^2 \mid s_h^2)}{\pi_{\mathrm{ref}}(a_h^2 \mid s_h^2)} \bigg). # An interesting observation is that, based on the autoregressive nature of policies, \eqref{eq:prac:2} aligns with the learning objective of DPO proposed by \citet{rafailov2023direct}, but under the token-level MDP instead of the sentence-level bandit setup. Similar to the bandit setting where the learning objective is equivalent to a BT model with sentence-wise reward $r^</em>(x, y) = \beta \log \frac{\pi_{\beta}^<em>(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$ \citep{rafailov2023direct}, \eqref{eq:prac:2} shows that the learning objective in token-wise MDP equivalents to a BT model with a token-wise reward function # \label{eq:prac:3} r^</em>(s_h = (x, y_{1:h-1}), a_h = y_h) = \beta \log \frac{\pi_\beta^<em>(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} = \beta \log \frac{\pi_\beta^</em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})}, # where $x$ is the prompt, $y_{1:h-1}$ is the tokens generated so far, and $y_h$ is the token chosen at the current step. In contrast to the previous PPO implementation with sparse reward in \eqref{eq:ppo:reward}, we will assign the token-wise reward function defined in~\eqref{eq:prac:3} to each step. Formally, for any $h$, we define \begin{equation} \begin{aligned}\label{eq:prac:5} &amp;\beta_1 \log \frac{\pi_\beta^<em>(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} <br/> &amp; \qquad \approx \beta_1 \log \frac{\pi_{\mathrm{dpo}}(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_h \mid x, y_{1:h-1})} - \beta_2 \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} := r_{\mathrm{rto}}((x, y_{1:h-1}),y_h) \end{aligned} \end{equation} as the token-wise reward used by \texttt{RTO}, where $\beta_1$ and $\beta_2$ are tuning parameters, and $\pi$ is the current policy to be updated. In the last step of \eqref{eq:prac:5}, we use $\pi_{\mathrm{dpo}}$, the policy learned by DPO, as a proxy for the unknown $\pi_\beta^</em>$. Finally, we employ PPO to optimize the token-wise reward $r_{\mathrm{rto}}$ in \eqref{eq:prac:5}.</p>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF]]></summary></entry><entry><title type="html">RLHF - IPL</title><link href="https://rz-liu.github.io/blog/2024/IPL/" rel="alternate" type="text/html" title="RLHF - IPL"/><published>2024-06-08T00:00:00+00:00</published><updated>2024-06-08T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/IPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/IPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="IPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="rl">RL</h3> <p>Contractive Bellman operator</p> \[\begin{equation} \label{eq:bellman} (\mathcal{B}^\pi_r Q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot\mid s,a)}[V^\pi(s')], \end{equation}\] <h2 id="method">Method</h2> <h3 id="inverse-soft-bellman-operator">Inverse Soft-Bellman Operator</h3> \[\begin{equation} \label{eq:inverse_bellman} (\mathcal{T}^\pi Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'\sim p(\cdot\mid s,a)}[V^\pi(s')]. \end{equation}\] <p>记 \(r_{Q^\pi}\) 为使用 \(Q^\pi\) 导出的隐式奖励函数 (implicit reward function)，即 \(r_{Q^\pi}(s,a) = Q^\pi(s,a)\)，则我们可以得到使用 \(Q^\pi\) 导出的隐式奖励函数的偏好分布 (preference distribution) \(P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}]\):</p> \[\begin{equation} \label{eq:q_preference} P_{Q^\pi}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) }{\exp \sum_t (\mathcal{T}^\pi Q)(s_t^{1}, a_t^{1}) + \exp \sum_t (\mathcal{T}^\pi Q)(s_t^{2}, a_t^{2})}. \end{equation}\] <h3 id="optimal-inverse-bellman-operator">Optimal Inverse Bellman Operator</h3> \[\begin{equation} (\mathcal{T}^* Q)(s,a) = Q(s,a) - \gamma \mathbb{E}_{s'}[V^{\text{targ}}(s')], \ \text{ where } V^\text{targ}(s) \text{ is estimated as in } \mathcal{B}^*_r. \end{equation}\] <h3 id="regularization">Regularization</h3> \[\begin{equation} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{1}, \sigma^{2},y \sim \mathcal{D}_p} \left[y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}])\right]. \end{equation}\] <p>其中 \(P_{Q^*}\) 由将 \(\mathcal{T}^*Q\) 替换到 \eqref{eq:q_preference} 得到。</p> <p>但是仅仅使用此损失函数会导致较差的结果，因为此目标函数是不受约束的，由于 BT model 对于 shift 是不变的，即对于所有的奖励函数 \(r(s,a)\)，\(Q(s,a)\) 都是一样的。为了解决这个问题，IPL 引入一个凸正则项 \(\psi(\cdot)\)，对隐式奖励函数 \(r_{Q^\pi} = \mathcal{T^\pi}Q\) 进行正则化，得到正则化的偏好损失函数：</p> \[\begin{equation} \label{eq:ipl} \mathcal{L}_p(Q) = -\mathbb{E}_{\sigma^{(1)}, \sigma^{(2)},y \sim \mathcal{D}_p} \left[ y \log P_{Q^*}[\sigma^{1} \succ \sigma^{2}] + (1-y) \log (1-P_{Q^*}[\sigma^{1} \succ \sigma^{2}]) \right] + \lambda \psi(\mathcal{T}^*Q) \end{equation}\] \[\begin{equation} \label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\]]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Inverse Preference Learning: Preference-based RL without a Reward Function]]></summary></entry><entry><title type="html">RLHF - CPL</title><link href="https://rz-liu.github.io/blog/2024/CPL/" rel="alternate" type="text/html" title="RLHF - CPL"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/CPL</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/CPL/"><![CDATA[<h2 id="summary">Summary</h2> <d-cite key="CPL"></d-cite> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h2 id="method">Method</h2> <p>###</p> \[\begin{equation} \label{eq:adv_to_pol1} A^*(s,a) = \alpha \log \pi^*(a|s). \end{equation}\] \[\begin{equation} \begin{aligned} A(s_t,a_t) &amp;= Q(s_t,a_t) - V(s_t) \\ &amp;= r_t + \gamma V(s_{t+1}) - V(s_t).\\ \sum_{h=t}^{H-1} A(s_h,a_h) &amp;= \sum_{h=t}^{H-1} \left[ r_h + \gamma V(s_{h+1}) - V(s_h) \right] \\ &amp;= \sum_{h=t}^{H-1} r_h + \gamma V(s_H) - V(s_t) \\ \end{aligned} \end{equation}\] \[\begin{equation} P_{A^*}\left[\sigma^+ \succ \sigma^- \right] = \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t)}{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi^*(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi^*(a^-_t|s^-_t)}. \end{equation}\] \[\begin{equation} \label{eq:loss} \mathcal{L}_\text{CPL}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{(\sigma^+\hspace{-0.8mm},\sigma^-) \sim \mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <h3 id="practical-implementation">Practical Implementation</h3> <p>Regularization term</p> \[\begin{equation} \label{eq:reg_loss} \mathcal{L}_{\text{CPL}({\color{red}{\lambda}})}(\pi_\theta, \mathcal{D}_\text{pref}) = \mathbb{E}_{\mathcal{D}_{\text{pref}}}\left[ -\log \frac{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) }{\exp \sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a^+_t|s^+_t) + \exp {\color{red}{\lambda}} \sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a^-_t|s^-_t)} \right]. \end{equation}\] <p>Python 实现如下，有点没搞懂为什么要用这个形式，是为了数值稳定性？</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Apply the log-sum-exp trick.
</span>    <span class="c1"># y = 1 if we prefer x2 to x1
</span>    <span class="c1"># We need to implement the numerical stability trick.
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">max21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">max12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp21</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max21</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span> <span class="o">-</span> <span class="n">max21</span><span class="p">))</span> <span class="o">+</span> <span class="n">max21</span>  <span class="c1"># (B,)
</span>    <span class="n">nlp12</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">max12</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span> <span class="o">-</span> <span class="n">max12</span><span class="p">))</span> <span class="o">+</span> <span class="n">max12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">nlp21</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">nlp12</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>正常来说，不加这个 trick 的话，应该是这样的：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">biased_bce_with_logits</span><span class="p">(</span><span class="n">adv1</span><span class="p">,</span> <span class="n">adv2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># y = 1 if we prefer x2 to x1
</span>
    <span class="n">logit21</span> <span class="o">=</span> <span class="n">adv2</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv1</span>  <span class="c1"># (B,)
</span>    <span class="n">logit12</span> <span class="o">=</span> <span class="n">adv1</span> <span class="o">-</span> <span class="n">bias</span> <span class="o">*</span> <span class="n">adv2</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit21</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logit12</span><span class="p">))</span>  <span class="c1"># (B,)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>

    <span class="c1"># Now compute the accuracy
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">adv2</span> <span class="o">&gt;</span> <span class="n">adv1</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="Robotics"/><summary type="html"><![CDATA[A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning]]></summary></entry><entry><title type="html">RLHF (1) - DPO</title><link href="https://rz-liu.github.io/blog/2024/DPO/" rel="alternate" type="text/html" title="RLHF (1) - DPO"/><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/DPO</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/DPO/"><![CDATA[<h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="dataset">Dataset</h3> <h3 id="sft">SFT</h3> <p>Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt \(x\) 和高质量回答 \(y\) 的数据集，对模型进行微调，使得模型在 prompt \(x\) 上生成的回答接近 \(y\)，最终得到模型 \(\pi^{\text{SFT}}\)。</p> <h3 id="奖励建模">奖励建模</h3> <p>对于一个输入 \(x\)，经过 SFT 的模型可以产生成对的回答 \((y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)\)。我们使用 \(y_w\) 和 \(y_l\) 分别表示 \((y_1, y_2)\) 中更好的回答和更差的回答，则偏好关系可以定义为 \(y_w \succ y_l \mid x\)。假设 ground-truth reward function 为 \(r^*(x, y)\)，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution \(p^*\) 可以表示为</p> \[\begin{equation} p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}. \end{equation}\] <p>假设我们有一个偏好数据集 \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\)，其中 \(y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}\)，我们可以将 reward learning 建模为一个二分类问题：</p> \[\begin{equation} \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr] \end{equation}\] <h3 id="rl-fine-tuning">RL Fine-Tuning</h3> <p>使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。</p> \[\begin{equation} \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi^{\text{ref}}(y \mid x)\bigr] \end{equation}\] <h2 id="method">Method</h2> <p>受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 <strong>key insight</strong> 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。</p> <h3 id="deriving-the-dpo-objective">Deriving the DPO objective</h3> <p>我们从与以前的工作相同的 RL 目标开始，公式如下： We start with the same RL objective as prior work, Eq.~\ref{eq:RL}, under a general reward function $r$. Following prior work~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\ref{eq:RL} takes the form:</p> \[\begin{equation} \label{eq:op_policy} \pi_r(y\mid x) = \frac{1}{Z(x)}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>where $Z(x) =\sum_{y}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ is the partition function. See Appendix \ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\phi}$ of the ground-truth reward function $r^<em>$, it is still expensive to estimate the partition function $Z(x)$ \citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\pi_r$, the reference policy $\pi^{\text{ref}}$, and the unknown partition function $Z(\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\ref{eq:op_policy} and then with some algebra we obtain: \begin{equation}\label{eq:main_eq} r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi^{\text{ref}}(y\mid x)} + \beta \log Z(x). \end{equation} We can apply this reparameterization to the ground-truth reward $r^</em>$ and corresponding optimal model $\pi^<em>$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^</em>(y_1 \succ y_2 \mid x) = \sigma(r^<em>(x, y_1) - r^</em>(x, y_2))}$. Substituting the reparameterization in Eq.~\ref{eq:main_eq} for $r^<em>(x,y)$ into the preference model Eq.~\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\pi^</em>$ and reference policy $\pi^{\text{ref}}$. Thus, the optimal RLHF policy $\pi^<em>$ under the Bradley-Terry model satisfies the preference model: \begin{equation}\label{eq:objective} p^</em>(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^<em>(y_2\mid x)}{\pi^{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^</em>(y_1\mid x)}{\pi^{\text{ref}}(y_1\mid x)}\right)} \end{equation} The derivation is in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\ref{app:plackett_luce_models}.</p> <p>Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_\theta$. Analogous to the reward modeling approach (i.e. Eq.~\ref{eq:reward_model}), our policy objective becomes: \begin{equation}\label{eq:optimum_model} \mathcal{L}<em>\text{DPO}(\pi</em>{\theta}; \pi^{\text{ref}}) = -\mathbb{E}<em>{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi</em>{\theta}(y_w\mid x)}{\pi^{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi^{\text{ref}}(y_l\mid x)}\right)\right]. \end{equation} \rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.</p> <p>\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}<em>\text{DPO}$. The gradient with respect to the parameters $\theta$ can be written as: \begin{multline*}\label{eq:gradient} \nabla</em>\theta \mathcal{L}<em>\text{DPO}(\pi</em>\theta;\pi^{\text{ref}}) = \ -\beta\mathbb{E}<em>{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}</em>\theta(x, y_l) - \hat{r}<em>\theta (x, y_w))}</em>\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}<em>\text{increase likelihood of $y_w$} - \underbrace{\nabla</em>\theta\log\pi(y_l \mid x)}<em>\text{decrease likelihood of $y_l$}\bigg]\bigg], \end{multline*} where $\hat{r}</em>\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi^{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi^{\text{ref}}$ (more in Section~\ref{sec:theory}). Intuitively, the gradient of the loss function $\mathcal{L}<em>\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}</em>\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\ref{tab:unlikelihood_generations}).</p> <p>\textbf{DPO outline.} The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \sim \pi^{\text{ref}}(\cdot \mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\mathcal{D} = {x^{(i)}, y_w^{(i)}, y_l)^{(i)}}<em>{i=1}^N$ and 2) optimize the language model $\pi</em>\theta$ to minimize $\mathcal{L}<em>\text{DPO}$ for the given $\pi^{\text{ref}}$ and $\mathcal{D}$ and desired $\beta$. In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pisft$, we initialize $\pi^{\text{ref}} = \pisft$ whenever available. However, when $\pisft$ is not available, we initialize $\pi^{\text{ref}}$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\pi^{\text{ref}} = \argmax</em>{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi^{\text{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\ref{app:implementation}.</p>]]></content><author><name>Runze Liu</name></author><category term="RLHF,"/><category term="LLM"/><summary type="html"><![CDATA[A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://rz-liu.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://rz-liu.github.io/blog/2024/tabs</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="41955b05-598e-4ea7-a105-a6cc3c01856d" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="41955b05-598e-4ea7-a105-a6cc3c01856d" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="d22e548a-60e8-436a-aae1-1521979cd0fa" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="d22e548a-60e8-436a-aae1-1521979cd0fa" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="36802834-b62b-4264-a3a8-3f36db9f7684" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="36802834-b62b-4264-a3a8-3f36db9f7684" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://rz-liu.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://rz-liu.github.io/blog/2024/typograms</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://rz-liu.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://rz-liu.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://rz-liu.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://rz-liu.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://rz-liu.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry></feed>
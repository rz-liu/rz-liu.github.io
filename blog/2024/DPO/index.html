<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RLHF (1) - DPO | Runze Liu </title> <meta name="author" content="Runze Liu"> <meta name="description" content="A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%91%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rz-liu.github.io/blog/2024/DPO/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">mjx-container[jax="CHTML"][display="true"]{margin-top:0!important;margin-bottom:1em!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RLHF (1) - DPO",
            "description": "A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "published": "June 04, 2024",
            "authors": [
              
              {
                "author": "Runze Liu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Runze</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RLHF (1) - DPO</h1> <p>A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#summary">Summary</a> </div> <div> <a href="#preliminaries">Preliminaries</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="dataset">Dataset</h3> <h3 id="sft">SFT</h3> <p>Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt \(x\) 和高质量回答 \(y\) 的数据集，对模型进行微调，使得模型在 prompt \(x\) 上生成的回答接近 \(y\)，最终得到模型 \(\pi^{\text{SFT}}\)。</p> <h3 id="奖励建模">奖励建模</h3> <p>对于一个输入 \(x\)，经过 SFT 的模型可以产生成对的回答 \((y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)\)。我们使用 \(y_w\) 和 \(y_l\) 分别表示 \((y_1, y_2)\) 中更好的回答和更差的回答，则偏好关系可以定义为 \(y_w \succ y_l \mid x\)。假设 ground-truth reward function 为 \(r^*(x, y)\)，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution \(p^*\) 可以表示为</p> \[\begin{equation} p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}. \end{equation}\] <p>假设我们有一个偏好数据集 \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\)，其中 \(y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}\)，我们可以将 reward learning 建模为一个二分类问题：</p> \[\begin{equation} \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr] \end{equation}\] <h3 id="rl-fine-tuning">RL Fine-Tuning</h3> <p>使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。</p> \[\begin{equation} \label{eq:RL} \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi^{\text{ref}}(y \mid x)\bigr] \end{equation}\] <h2 id="method">Method</h2> <p>受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 <strong>key insight</strong> 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。</p> <h3 id="deriving-the-dpo-objective">Deriving the DPO objective</h3> <p>我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 \(r\) 是一个通用的函数。根据以前的工作~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}，KL 约束奖励最大化目标的最优解形式为：</p> \[\begin{equation} \label{eq:op_policy} \pi_r(y\mid x) = \frac{1}{Z(x)}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(Z(x) =\sum_{y}\pi^{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)\) 是 partition function。即使我们使用奖励函数 \(r_{\phi}\) 的 MLE 估计值，估计 partition function \(Z(x)\) 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 \(\pi_r\)、参考策略 \(\pi^{\text{ref}}\) 和未知的 partition function \(Z(\cdot)\)。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：</p> \[\begin{equation} \label{eq:main_eq} r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi^{\text{ref}}(y\mid x)} + \beta \log Z(x). \end{equation}\] <p>我们可以将这种重新参数化应用于 ground-truth reward \(r^*\) 和相应的最优策略 \(\pi^*\)。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 \(p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))\)。将 \eqref{eq:main_eq} 中的重新参数化代入到 \(r^*(x,y)\) 的偏好模型 \eqref{eq:bradley-terry} 中，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 \(\pi^*\) 和参考策略 \(\pi^{\text{ref}}\)。因此，Bradley-Terry 模型下的最优 RLHF 策略 \(\pi^*\) 满足偏好模型：</p> \[\begin{equation} \label{eq:objective} p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi^{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi^{\text{ref}}(y_1\mid x)}\right)} \end{equation}\] <p>The derivation is in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\ref{app:plackett_luce_models}.</p> <p>Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\pi_\theta$. Analogous to the reward modeling approach (i.e. Eq.~\ref{eq:reward_model}), our policy objective becomes: \begin{equation}\label{eq:optimum_model} \mathcal{L}<em>\text{DPO}(\pi</em>{\theta}; \pi^{\text{ref}}) = -\mathbb{E}<em>{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi</em>{\theta}(y_w\mid x)}{\pi^{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi^{\text{ref}}(y_l\mid x)}\right)\right]. \end{equation} \rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\pi_\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.</p> <h3 id="what-does-the-dpo-update-do">What does the DPO update do?</h3> <p>For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\mathcal{L}<em>\text{DPO}$. The gradient with respect to the parameters $\theta$ can be written as: \begin{multline*}\label{eq:gradient} \nabla</em>\theta \mathcal{L}<em>\text{DPO}(\pi</em>\theta;\pi^{\text{ref}}) = \ -\beta\mathbb{E}<em>{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}</em>\theta(x, y_l) - \hat{r}<em>\theta (x, y_w))}</em>\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}<em>\text{increase likelihood of $y_w$} - \underbrace{\nabla</em>\theta\log\pi(y_l \mid x)}<em>\text{decrease likelihood of $y_l$}\bigg]\bigg], \end{multline*} where $\hat{r}</em>\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi^{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi^{\text{ref}}$ (more in Section~\ref{sec:theory}). Intuitively, the gradient of the loss function $\mathcal{L}<em>\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\hat{r}</em>\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\ref{tab:unlikelihood_generations}).</p> <h3 id="dpo-outline">DPO outline</h3> <p>The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \sim \pi^{\text{ref}}(\cdot \mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\mathcal{D} = {x^{(i)}, y_w^{(i)}, y_l)^{(i)}}<em>{i=1}^N$ and 2) optimize the language model $\pi</em>\theta$ to minimize $\mathcal{L}<em>\text{DPO}$ for the given $\pi^{\text{ref}}$ and $\mathcal{D}$ and desired $\beta$. In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\pisft$, we initialize $\pi^{\text{ref}} = \pisft$ whenever available. However, when $\pisft$ is not available, we initialize $\pi^{\text{ref}}$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\pi^{\text{ref}} = \argmax</em>{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\pi^{\text{ref}}$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\ref{app:implementation}.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Runze Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 13, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> </body> </html>
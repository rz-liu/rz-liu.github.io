<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RLHF - RTO | Runze Liu </title> <meta name="author" content="Runze Liu"> <meta name="description" content="A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%91%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rz-liu.github.io/blog/2024/RTO/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">mjx-container[jax="CHTML"][display="true"]{margin-top:0!important;margin-bottom:1em!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RLHF - RTO",
            "description": "A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF",
            "published": "June 13, 2024",
            "authors": [
              
              {
                "author": "Runze Liu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Runze</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RLHF - RTO</h1> <p>A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#summary">Summary</a> </div> <div> <a href="#preliminaries">Preliminaries</a> </div> <div> <a href="#formulation">Formulation</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="summary">Summary</h2> <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>In this section, we introduce the standard RLHF paradigm. Let $x \in \cX$ denote the prompt sampled from a distribution $\rho \in \Delta(\cX)$, and $y= (y_1,y_2,\dots,y_h,\dots)$ be the corresponding response, which is a sequence of tokens generated by LLMs, where $y_i$ represents the $i$-th token. In practice, it is widely assumed \citep{christiano2017deep, ziegler2019fine, bai2022training, ouyang2022training, touvron2023llama} that the preference signal is generated according to the Bradley-Terry (BT) model \citep{bradley1952rank}:</p> \[\begin{equation} \label{eqn:bt} \begin{aligned} P(y^1 \succ y^2|x,y^1,y^2) = \frac{\exp(r(x,y^1))}{\exp(r(x,y^1)) + \exp(r(x,y^2))} = \sigma\big( r(x, y^1) - r(x, y^2) \big), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数, \(r\) 是定义在 <strong>sentence-level</strong> 的 ground-truth reward function，用于评价整个回复 (response) 的性能。经典的 RLHF 算法通常包括两个步骤：根据人类反馈训练奖励函数和基于奖励训练 RL。在第一步中，学习者被给定一个数据集 \(\mathcal{D} = \{(x, y^w, y^l)\}\)，其中 \(y^w\) 表示更好的回复，\(y^l\) 表示更差的回复。奖励函数通过最大似然估计（MLE）在数据集 \(\mathcal{D}\) 上学习：</p> \[\begin{equation} \label{eqn:mle_old} r_{\mathrm{MLE}} = \mathop{\mathrm{argmax}}_{r} \mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} \big[\log\big(\sigma(r(x, y^w) - r(x, y^l))\big)\big]. \end{equation}\] <p>在第二步，优化学习到的奖励 \(r_{\mathrm{MLE}}\)，同时确保更新后的 LLM 不会与参考模型 \(\pi_{\mathrm{ref}}\) 显著偏离。通常使用经过 SFT 的 LLM 作为参考模型，这是因为优化奖励通常会导致奖励欺骗 (reward hacking)，意味着 LLM 将利用奖励模型的缺陷，追求高奖励，但同时表现较差。形式上，LLM 相对于学习到的奖励 \(r_{\mathrm{MLE}}\) 进行优化，带有 KL 正则化项：</p> \[\begin{equation} \hat{\pi} = \mathop{\mathrm{argmax}}_{\pi} \mathbb{E}_{x \sim \rho, y \sim \pi(\cdot \mid x)} \bigg[ r_{\mathrm{MLE}}(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \bigg], \end{equation}\] <p>其中，\(\beta &gt; 0\) 为 KL 惩罚系数。这种 KL 正则化目标在实践中被广泛采用，以平衡奖励优化和保持接近参考策略的目标。另一个主要的技术原因是，这种正则化确保了框架接受随机最优策略，与确定性贪婪奖励最大化器相比。策略优化步骤通常通过 PPO <d-cite key="PPO"></d-cite> 实现，这是一个解决多步决策问题的深度 RL 算法，其实现需要每步的奖励信号（对应于 LLM 的每个 token）。为此，给定一个提示 \(x\) 和一个包含 \(H\) 个 token 的回复 \(y = y_{1:H}\)，其中 \(H\) 是 token 的数量，现有的 PPO 开源实现将句子级奖励 \(r_{\mathrm{MLE}}(x, y)\) 分配给最后一个 token，并优化以下奖励：</p> \[\begin{equation} \label{eq:ppo:reward} \begin{aligned} {r}_{\mathrm{ppo}}(x, y_{1:h}) = \begin{cases} {\color{red}{0}} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h \le H-1, \\ {\color{red}r_{\mathrm{MLE}}(x, y)} - \beta \log \frac{\pi(y_h \mid x, y_{1:h-1})}{\pi_{\mathrm{ref}}(y_{h} \mid x, y_{1:h-1})} &amp; \text{ if } h = H, \end{cases} \end{aligned} \end{equation}\] <p>其中，\(\pi\) 是当前要改进的策略。然而，众所周知，稀疏奖励可能会使学习比密集奖励更困难 <d-cite key="HER"></d-cite>。一种自然的解决方案是设计用于 PPO 训练的密集 token-wise 奖励，但这超出了当前 RLHF 的 bandit 形式，并激励我们提供一个具有更精细 token-wise 特征的框架，以便使用 token-wise 奖励。</p> <h2 id="formulation">Formulation</h2> <h3 id="mdp-formulation-for-rlhf">MDP Formulation for RLHF</h3> <p>我们将 RLHF 问题建模为马尔可夫决策过程（MDP），记为 \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, H)\)，其中 \(\mathcal{S}\) 是状态空间，\(\mathcal{A}\) 是动作空间，\(\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) 是环境动态，\(r\) 表示奖励函数，\(\rho\) 表示初始状态分布，\(H\) 是交互步数的最大值。MDP 中的策略 \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\) 是从状态到动作分布的映射。环境 \(\mathcal{M}\) 与代理之间的交互可以描述如下。首先，从初始分布 \(\rho\) 中抽取起始状态 \(s_1\)。在第 \(h\) 步，代理观察到状态 \(s_h\) 并根据其策略选择动作 \(a_h\)。然后，环境从分布 \(\mathcal{P}(\cdot \mid s_h, a_h)\) 中抽取下一个状态 \(s_{h+1}\)。这种交互会持续到满足某个结束条件，该条件将在 \(H\) 步内触发。</p> <p>在大型语言模型（LLM）的标准文本生成过程中，每个状态 \(s_h = (x, y_{1:h-1})\) 包括提示 \(x\) 和到目前为止生成的所有响应 token。每个动作 \(a_h = y_{h}\) 表示词汇表中的一个 token。环境动态 \(\mathcal{P}\) 通常是已知的且确定的，这意味着给定 tokens \(s_h = (x, y_{1:h-1})\) 和 \(a_h = y_{h}\)，环境将转移到 \(s_{h+1} = (x, y_{1:h})\)。策略 \(\pi\) 将到目前为止观察到的所有 token 映射到词汇表上的分布。重要的是注意，策略捕捉了 LLM 的自回归特性，即对于任何 \(h\)，\(\pi(y_{1:h} \mid x) = \prod_{i = 1}^h \pi(y_i \mid x, y_{1:h-1})\)。由于这一点，我们可以将其称为自回归策略，以区分其他方式定义的策略。此外，\(r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) 表示 token-wise 奖励。生成的文本以特殊的句子结束 token \(\texttt{EoS}\) 结束，该 token 终止生成过程。</p> <p>在我们的 RLHF 的 MDP formulation 中，我们还使用 BT 模型 <d-cite key="BTModel"></d-cite> 建模偏好信号，但将 \eqref{eqn:bt} 中的句子级奖励函数替换为 token-wise 奖励函数。具体来说，对于任何轨迹对 \(\tau^1 = \{(s_h^1, a_h^1)\}_{h=1}^H\) 和 \(\tau^2 = \{(s_h^2, a_h^2)\}_{h=1}^H\)，偏好由以下公式计算：</p> \[\begin{equation} \label{eq:BT:mdp} P(\tau^1 \succ \tau^2) = \frac{\exp(\sum_{h=1}^H r(s_h^1, a_h^1))}{\exp(\sum_{h=1}^H r(s_h^1, a_h^1)) + \exp(\sum_{h=1}^H r(s_h^2, a_h^2))} = \sigma\bigg( \sum_{h=1}^H r(s_h^1, a_h^1) - \sum_{h=1}^H r(s_h^2, a_h^2) \bigg). \end{equation}\] <p>相比于将 RLHF 问题建模为 contextual dueling bandit 问题，我们的 MDP formulation 有一个微妙的区别，即 contextual dueling bandit 的策略将提示映射到句子上的分布，而这种分布并不能捕捉 LLM 的自回归特性。相反，我们的 MDP formulation 精确捕捉了这种特性。更重要的是，MDP formulation 中的奖励函数是在 token 级别上定义的，这与 contextual dueling bandit 中的句子级奖励有着显著的不同。</p> <h3 id="learning-objective">Learning Objective</h3> <p>与经典 RL 方法不同，经典 RL 的唯一目标是最大化奖励函数，RLHF 的目标是最大化奖励函数的同时确保学到的策略不会与参考模型（例如 SFT 模型）相差太远。受此启发以及熵正则化 MDP 的公式化，对于任何策略 \(\pi\)，我们定义其对应的正则化价值函数为</p> \[\begin{equation} \label{eq:q:v} \begin{aligned} V_\beta^{\pi}(s; r) &amp;= \mathbb{E}_{\pi} \bigg[ \sum_{h = 1}^\infty \bigg( r(s_h, a_h) - \beta \cdot \log \frac{\pi(a_h \mid s_h)}{\pi_{\mathrm{ref}}(a_h \mid s_h)} \bigg) \bigg| s_1 = s\bigg], \end{aligned} \end{equation}\] <p>其中，期望 \(\mathbb{E}_{\pi}\) 是针对策略 \(\pi\) 的随机性。在这里，求和在满足某个条件时结束。特别地，由于我们假设 LLM 生成的响应的最大长度最多为 \(H\)，因此 \eqref{eq:q:v} 中的求和最多在 \(H\) 步结束。在本文的其余部分，我们可能会将 \(\sum_{h=1}^\infty\) 和 \(\sum_{h=1}^H\) 互换使用，因为它们大多具有相同的含义。正则化 Q 函数 \(Q_\beta^\pi\) 是策略 \(\pi\) 的正则化价值函数的关联函数，定义如下</p> \[\begin{equation} \label{eq:bellman} Q_\beta^{\pi}(s, a; r) = r_\beta(s, a) + \mathbb{E}_{s' \sim \mathcal{P}(\cdot \mid s, a)}[V_\beta^\pi(s'; r)], \qquad V_\beta^\pi(s; r) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} [ - \beta \log \pi(a \mid s) + Q_\beta^\pi(s, a; r)], \end{equation}\] <p>where we denote $r_\beta(s, a) = r(s, a) + \beta \log \pi_{\mathrm{ref}}(a \mid s)$. Moreover, when it is clear from the context, we may omit the dependency of the ground-truth reward function $r$ in $Q_\beta^\pi(s, a; r), V_\beta^\pi(s; r)$ and use the shorthand $Q_\beta^\pi(s, a), V_\beta^\pi(s)$. The regularized optimal policy $\pi_\beta^<em>$ is the policy that maximizes the regularized value function defined in \eqref{eq:q:v}, and its corresponding optimal Q-function and value function are denoted as $Q_\beta^</em>$ and $V_\beta^<em>$, respectively. By \eqref{eq:bellman}, it can be shown that # \label{eq:optimal:policy} \pi_\beta^</em>(a \mid s) = \exp{ (Q_\beta^<em>(s, a) - V_\beta^</em>(s))/\beta }. # Our learning objective is to find a near-optimal policy $\hat{\pi}$, and its optimality gap is measured by the following suboptimality gap: # \label{eq:def:subopt} \SubOpt(\hat{\pi}) = \EE_{s \sim \rho} [V_{\beta}^<em>(s) - V_{\beta}^{\hat{\pi}}(s)] = V_\beta^</em>(\rho) - V_\beta^{\hat{\pi}}(\rho), # where we use the shorthand $V_\beta^\pi(\rho) = \EE_{s \sim \rho}[V_\beta^\pi(s)]$ for any policy $\pi$. For ease of presentation, we define the state visitation measure $d^\pi(s) = \EE_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_t = s \mid s_1 )]$ and the state-action visitation measure $d^\pi(s, a) = \EE_{s_1 \sim \rho} [ \sum_{h = 1}^\infty \PP(s_h = s, a_h = a \mid s_1 ) ]$. We also use the shorthand $d^* = d^{\pi_\beta^*}$ to further simplify the notation.</p> <h3 id="advantages-of-token-wise-mdp-over-sentence-wise-bandit">Advantages of Token-Wise MDP over Sentence-Wise Bandit</h3> <p>\label{sec:token:reward}</p> <table> <tbody> <tr> <td>直觉上，基于 token 的奖励和基于句子的奖励之间的区别反映了 sparse reward 和 dense reward 之间的差异。为了说明这一点，我们专注于具有动作集大小 $$A =</td> <td>\mathcal{A}</td> <td>\(的确定性 MDP。我们使用自回归策略\)\pi^<em>\(来表示强大的 LLM 策略，例如 GPT-4。固定提示\)x\(，给定回复\)(y^1 = y_{1:H}^1, y^2 = y_{1:H}^2)\(，由\)\pi^</em>$$ 提供的评估为</td> </tr> </tbody> </table> \[\begin{equation} P(y^1 \succ y^2 \mid x, y_1, y_2) = \frac{\pi^*(y^1 \mid x)}{\pi^*(y^1 \mid x) + \pi^*(y^2 \mid x)}. \end{equation}\] <p>通过比较 \eqref{eqn:bt} 中的 BT 模型和我们的 MDP 公式 \eqref{eq:BT:mdp} 中的 bandit 模型，我们观察到句子级奖励 \(r_s\) 和 token 级奖励 \(r_t\) 可以分别由以下公式得到</p> \[\begin{equation} \label{eq:reward:example} r_{s}(x, y) = \log \pi^*(y \mid x), \qquad r_{t}((x, y_{1:h-1}), y_h) = \log \pi^*(y_h \mid x, y_{1:h-1}). \end{equation}\] <p>直观地，强大的 LLM 倾向于选择具有更高奖励的响应。此外，很容易证明 \(r_s(x, y) = \sum_{h = 1}^H r_t( (x, y_{1:h-1}), y_h)\)。</p> <h3 id="method">Method</h3> <p>\label{sec:alg_theory}</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Runze Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 13, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> </body> </html>
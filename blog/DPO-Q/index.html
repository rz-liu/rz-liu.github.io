<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RLHF - DPO-Q* | Runze Liu </title> <meta name="author" content="Runze Liu"> <meta name="description" content="A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%91%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rz-liu.github.io/blog/DPO-Q/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">mjx-container[jax="CHTML"][display="true"]{margin-top:0!important;margin-bottom:1em!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RLHF - DPO-Q*",
            "description": "A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)",
            "paper_url": "",
            "published": "June 13, 2024",
            "authors": [
              
              {
                "author": "Runze Liu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Runze</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RLHF - DPO-Q*</h1> <p>A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv) </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#summary">Summary</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#key-idea">Key Idea</a> </li> <li> <a href="#contributions">Contributions</a> </li> </ul> <div> <a href="#preliminaries">Preliminaries</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#implementation">Implementation</a> </div> <div> <a href="#experiments">Experiments</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="DPO-Q*"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li> </li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li> </li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li> </li> <li> </li> <li> </li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>在 In this section we first define the per-token MDP for large language models, and then describe how it relates to classic RLHF approaches and direct alignment algorithms, specifically DPO. We operate in the typical RLHF setting where we have a dataset \(\mathcal{D}=\{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^N\) of language prompts \(\mathbf{x}\) and target answers \(\mathbf{y}\), which can each individually be broken down into a sequence of tokens, for example \(\mathbf{x}=(x_0, \ldots, x_m)\), from a fixed discrete vocabulary \(\mathcal{A}\). Throughout this section we will use the \(\mathbf{x}\), \(\mathbf{y}\) notation for the contextual bandit framing where the entire response \(\mathbf{y}\) is the action, but will use state \(\mathbf{s}\) and action \(\mathbf{a}\) notation from RL literature for describing sequences at the token-level.</p> <h3 id="the-token-level-mdp-for-large-language-models">The Token-level MDP for Large Language Models</h3> <p>We define the token level MDP as a tuple \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, f, r, \rho_0)\), where the state space \(\mathcal{S}\) consists of all tokens generated so far (i.e. \(\mathbf{s}_t=\{x_0, \ldots, x_m, y_0, \ldots, y_t\}\)) and the action space is the vocabulary of tokens \(\mathcal{A}\). The dynamics \(f\) are the deterministic transition model between tokens \(f(\mathbf{s},\mathbf{a}) = \mathbf{s} \mid \mathbf{a}\), where \(\mid\) is concatenation. The initial state distribution \(\rho_0\) is a distribution over prompts \(\mathbf{x}\), where an initial state \(\mathbf{s}_0\) is comprised of the tokens from \(\mathbf{x}\). In RLHF, the reward function is learned from human feedback over preferences between responses which we will denote using trajectories \(\tau\) at the token level. As is typically done \citep{ziegler2020finetuning, stiennon2022learning}, we assume that preference trajectories start at the same state (initial propmpt) and end in a terminal state (\textbf{EOS} token), from which future rewards are zero. % While the reward function \(r\) will ultimately be learned from feedback, we assume that within the token-MDP the reward \(r\) at terminal states must be zero. Note that this is not a restrictive assumption, as \(r(\mathbf{s}_t, \mathbf{a}_t)\) can fully represent any reward dependent on the next state \(\mathbf{s}_{t+1}\) per the deterministic dynamics. In this token level MDP, the corresponding Bradley-Terry preference model \cite{bradley1952rankanalysis, christiano2017deep} is</p> \[\begin{equation}\label{eq:dense-bradley-terry} p^*(\tau^w \succeq \tau^l)=\frac{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)}{\exp\left(\sum_{i=1}^N r(\mathbf{s}_i^w, \mathbf{a}_i^w)\right)+ \exp\left(\sum_{i=1}^M r(\mathbf{s}_i^l, \mathbf{a}_i^l)\right)}. \end{equation}\] <p>which gives the probability that the <code class="language-plaintext highlighter-rouge">win'' trajectory $$\tau^w$$ of length $$N$$ is preferred to the</code>loss’’ trajectory \(\tau^l\) of length \(M\). Now that we have defined the token level MDP, we can show how it relates to both classic and direct alignment RLHF methods.</p> <h3 id="the-classical-rlhf-methods">The Classical RLHF Methods</h3> <p>Most classical RLHF approaches \citep{ziegler2020finetuning, bai2022constitutional, ouyang2022training} first learn a reward function from human feedback on prompt and response pairs \((\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\), then optimize it with a policy gradient-based method like PPO \citep{schulman2017proximal} with an entropy-bonus using the following KL-constrained RL objective</p> \[\begin{equation}\label{eq:multi_step_RL} \max_{\pi_{\theta}} \mathbb{E}_{a_t \sim \pi_{\theta}(\cdot \mid \mathbf{s}_t)}\left[\sum_{t=0}^T (r(\mathbf{s}_t, \mathbf{a}_t) + \underbrace{\beta\log\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}_{\text{KL penalty}}) + \beta\mathcal{H}(\pi_{\theta}) \mid \mathbf{s}_0\sim\rho(\mathbf{s}_0)\right] \end{equation}\] <p>where \(\pi_{\mathrm{ref}}\) is a reference policy, often resulting from supervised finetuning, from which the learned policy should not significantly deviate. However, in classic RLHF methods the reward function is learned as a contextual bandit with the preference model</p> \[\begin{equation}\label{eq:bandit_pref} p^*(\mathbf{y}^w \succeq \mathbf{y}^l) = \frac{\exp r(\mathbf{x},\mathbf{y}^w)} {\exp r(\mathbf{x},\mathbf{y}^w) + \exp r(\mathbf{x},\mathbf{y}^l)} \end{equation}\] <p>and is thus only applied at the final timestep for the last action where \(\mathbf{a}\) is \textbf{EOS}. In practice the actual reward used in the token-level PPO is</p> \[\begin{equation}\label{eq:token_reward} r(\mathbf{s}_t, \mathbf{a}_t) = \begin{cases} \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\ r(\mathbf{x}, \mathbf{y}) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1}=\mathbf{y} \text{ is terminal} \end{cases} \end{equation}\] <p>in a maximum entropy formulation. This leads to an interesting contradiction where the reward function \(r\) is treated like a bandit, but the actual RL value function and optimization is done per-token in practice.</p> <h3 id="direct-preference-optimization">Direct Preference Optimization</h3> <p>Unlike classical RLHF, DPO, as derived in \citet{rafailov2023direct}, stays entirely within the contextual bandits setting entirely and also uses the bandit-based preference model in \cref{eq:bandit_pref}. To circumvent the need for an RL algorithm, DPO uses the well-known closed form solution to the KL-contextual bandit version of the RL problem posed in \cref{eq:multi_step_RL} \citep{ziebart2008maximum, levine2018reinforcement}:</p> \[\begin{equation} \pi^*(\mathbf{y} \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})}\pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})e^{r(\mathbf{x}, \mathbf{y})} \end{equation}\] <p>where \(\pi^*\) is the optimal policy and \(Z(\mathbf{x})\) is the partition function that normalizes it. DPO re-arranges this equation to solve for reward as</p> \[\begin{equation} r(\mathbf{x},\mathbf{y}) = \beta \log \pi^*(\mathbf{y} \mid \mathbf{x}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x}) - Z(\mathbf{x}). \end{equation}\] <p>Substituting this relationship into the standard binary cross-entropy loss function used for reward modeling yields the DPO loss equation as the partition function \(Z(\mathbf{x})\) cancels from the Bradley Terry model.</p> \[\begin{equation}\label{eq:optimum_model} \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\mathrm{ref}}) = -\mathbb{E}_{(\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(\mathbf{y}^w \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^w \mid \mathbf{x})} - \beta \log \frac{\pi_{\theta}(\mathbf{y}^l \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}^l \mid \mathbf{x})}\right)\right] \end{equation}\] <p>For brevity we use \(\sigma\) to denote the logistic function. In the next section, we show how an alternative derivation of DPO can also cast its optimization within the token-level MDP.</p> <h2 id="method">Method</h2> <p>In this section we explore how DPO can theoretically be cast into the token-level MDP, and explore the consequences of doing so. First, we provide a token level derivation of DPO under the assumptions in \cref{section:tokenMDP}. Next, we show that even in the token MDP, DPO is able to fit any reward function in the multi-step Bradley Terry preference model \cref{eq:dense-bradley-terry}. Ultimately, this shows that DPO can potentially be used for more sequential optimization tasks, like multi-turn interactions or even multi-modal generation.</p> <h3 id="dpo-as-a-q-function-in-the-token-level-mdp">DPO as a \(Q\)-function in the Token Level MDP</h3> <p>\noindent \textbf{RL in the Token-level MDP.} While the original derivation of DPO relies on the fact that \(Q^*(\mathbf{x}, \mathbf{y}) = r(\mathbf{x}, \mathbf{y})\), this relationship does not hold in the token-level MDP. To resolve this, we need to develop new mathematical results that will allow us to relate the reward function in the Token-level Bradley Terry model \cref{eq:dense-bradley-terry} to the corresponding optimal policy \(\pi^*\). In the general maximum entropy RL setting, the fixed point solution of \cref{eq:multi_step_RL} is given by \citep{ziebart2010modeling} as</p> \[\begin{equation}\label{eq:policy} \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = e^{(Q^*(\mathbf{s}_t, \mathbf{a}_t)-V^*(\mathbf{s}_t))/\beta} \end{equation}\] <p>where \(\pi^*(\mathbf{a} \mid \mathbf{s})\) is the optimal policy and \(Q^*(\mathbf{s}, \mathbf{a})\) is the optimal Q-function which models the total future reward from \((\mathbf{s}, \mathbf{a})\) under \(\pi^*\). The optimal value function \(V^*\) is a function of \(Q^*\),</p> \[\begin{equation}\label{eq:value} V^*(\mathbf{s}_t) = \beta\log\int_{\mathcal{A}} e^{Q^*(\mathbf{s}_t, \mathbf{a})/\beta}d\mathbf{a} \end{equation}\] <p>such that the policy \(\pi^*\) integrates to one. Unfortunately unlike in the bandits setting this relationship gives us no specific information about the reward function \(r\) at a single state action pair since the optimal policy optimizes for total future returns as estimated by \(Q\). To do so, we will need to consider the relationship between \(Q^*\) and \(r\).</p> <p>\noindent \textbf{From \(r\) to \(Q^*\).} The relationship between future returns and the current timestep is captured by the belmman equaitons which are satisifed by any valid Q-function. We write this below for the optimal policy \(\pi^*\) under the reward \(r\) with a KL divergence penalty:</p> \[\begin{equation}\label{eq:critic} Q^*(\mathbf{s}_t, \mathbf{a}_t) = \begin{cases} r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) + V^*(\mathbf{s}_{t+1}), &amp; \text{if } \mathbf{s}_{t+1} \text{ is not terminal} \\ r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t), &amp; \text{if } \mathbf{s}_{t+1} \text{ is terminal} \end{cases} \end{equation}\] <p>We can then rearrange the bellman equation for the optimal \(Q\)-function in terms of the reward. This style of relationship was first explored by \citet{garg2022iqlearn} in imitation learning and later in \citet{hejna2024inverse} for preference-based RL. However, these works \emph{require} the use of a discount factor \(\gamma &lt; 1\) which is typically not used in RLHF. In the appendix we prove the following Lemma which shows that this relationship is indeed one-to-one in the token MDP as well.</p> <p>\begin{lemma} \label{lemma:r_to_q} Under mild assumptions, there is a bijection between reward functions \(r(\mathbf{s}_t, \mathbf{a}_t)\) and corresponding optimal Q-functions \(Q^*(\mathbf{s}_t, \mathbf{a}_t)\) in the token MDP. \end{lemma}</p> <p>This leads us to a rather interesting conclusion – that an LLM is \emph{always} the optimal soft Q-functions for \emph{some} reward function in the token MDP. Consider any LLM which outputs logits \(l_\theta\) and temperature parameter \(\beta\). As is common practice, we take the sampling policy \(\pi\) to be the softmax over tokens modulated by temperature parameter \(\beta\) – which is precisely \cref{eq:policy} where \(Q^* = l_\theta\) because the value optimal function \(V^*\) is precisely \(\beta \log Z(\mathbf{s}_t)\), normalizing the distribution. The corresponding reward function may not be smooth or well-behaved. Notably, the logits have a free parameter due to the softmax. While this free-parameter results in the same optimal policy per later arguments, it means the sequence of values may not be smooth. The question then becomes how to finetune the LLM such that it is the optimal Q-function for a reward function \(r\) that aligns with human preferences. To do so, we will complete our derivation of DPO in the token MDP.</p> <p>\textbf{DPO learns our best estimate of \(Q^*\).} Now that we have established a bijection between \(r\) and \(Q^*\), we can derive a token-level version of DPO to align the implicit reward, induced by the \(Q\) function represented by the language model, with that of the best estimate of reward, according to Bradley-Terry model in \cref{eq:dense-bradley-terry}. To do so, we need to represent the sum of rewards first in terms of the \(Q\)-function \(Q^*\), and then in terms of the policy \(\pi^*\). We complete the first step by inverting the Bellman equation in \cref{eq:critic} and substituting it into the sum of rewards over a trajectory \(\tau = \{\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{a}_{T-1}, \mathbf{s}_T\}\).</p> \[\begin{equation} \begin{aligned} \sum_{t=0}^{T-1}r(\mathbf{s}_t, \mathbf{a}_t) &amp;= \sum_{t=0}^{T-1}\left(Q^*(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) - V^*(\mathbf{s}_{t+1})\right) = \\ &amp;= Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1}Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_{t}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t) \end{aligned} \end{equation}\] <p>The equality follows from \(V^*(\mathbf{s}_T)=0\) and re-arranging the sum to isolate \(t=0\). As \(V^*\) is written entirely in terms of \(Q^*\) and \(\beta\) per \cref{eq:value}, we have expressed the sum of return over the sequence just in terms of \(Q^*\). Next, we exchange \(Q^*\) for \(\pi^*\). We can log-linearize \cref{eq:policy} as \(\beta \log \pi^*(\mathbf{a}_t \mid \mathbf{s}_t) = Q^*(\mathbf{s}_t, \mathbf{a}_t) - V^*(\mathbf{s}_t)\). This is equivalent to stating that the language model probabilities are just the softmax over \(l_\theta = Q^*\) with temperature \(\beta\). Continuing from the above, with this substitution we get</p> \[\begin{equation} = Q^*(\mathbf{s}_0, \mathbf{a}_0) - \beta \log \pi_{\mathrm{ref}}(\mathbf{a}_0 \mid \mathbf{s}_0) + \sum_{t=1}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid\mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} =V^*(\mathbf{s}_0) + \sum_{t=0}^{T-1} \beta \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} \end{equation}\] <p>where the final step results from adding and subtracting \(V^*(\mathbf{s}_0)\) and applying the substitution again. Now, this representation for the sum of rewards in terms of the optimal policy can be directly substituted into the preference model in \cref{eq:dense-bradley-terry}, where the \(V^*(\mathbf{s}_0)\) term will cancel just as \(Z(\mathbf{x})\) did in the original DPO derivation assuming \(\tau^w\) and \(\tau^l\) start at the same state \(\mathbf{s}_0\), giving us the policy-induced preference model</p> \[\begin{equation}\label{eq:policy_pref} p_{\pi^*}(\tau^w \succeq \tau^l) = \sigma \left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)} - \sum_{t=0}^{M-1} \beta \log \frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right). \end{equation}\] <p>To derive the final DPO loss function, we can take the KL-divergence between the empirical preference model of our dataset \(p_\mathcal{D}\) and the preference model implied by a learned policy \(p_{\pi_\theta}\), \(\mathbb{D}_{\mathrm{KL}} (p_\mathcal{D} \mid\mid p_{\pi_\theta})\). This results in</p> \[\begin{equation}\label{eq:DPO} \mathcal{L}(\pi_{\theta}, \mathcal{D}) = -\mathbb{E}_{(\tau_w, \tau_l)\sim \mathcal{D}}\left[\log \sigma\left(\left( \sum_{t=0}^{N-1}\beta \log\frac{\pi^*(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^w \mid \mathbf{s}_t^w)}\right)- \left( \sum_{t=0}^{M-1}\beta \log\frac{\pi^*(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}{\pi_{\mathrm{ref}}(\mathbf{a}_t^l \mid \mathbf{s}_t^l)}\right)\right)\right] \end{equation}\] <p>In the next section we demonstrate that DPO can learn any dense reward function in the token-level MDP.</p> <h3 id="token-level-dpo-can-parameterize-any-dense-reward-function">Token-Level DPO Can Parameterize Any Dense Reward Function.</h3> <p>In the previous section we derived DPO using the bijection between reward functions and optimal \(Q\)-functions uniquely available in the token-level MDP. An alternative view of DPO casts it as restricting the learned reward function such that it belongs to the class optimal advantage functions \(A^*(\mathbf{s},\mathbf{a}) = Q^*(\mathbf{s},\mathbf{a}) - V^*(\mathbf{s})\) from which an optimal policy is readily obtained per \cref{eq:policy}. Here we show that this restriction does not limit the class of reward functions we can represent. We begin by expanding the definition of equivalency used in \citet{rafailov2023direct} to the broader class of potential-based reward shaping functions:</p> <p>\begin{definition}\label{def:equivalence} Two reward functions \(r(\mathbf{s}_t, \mathbf{a}_t)\) and \(r'(\mathbf{s}_t, \mathbf{a}_t)\) are equivalent if there exists a potential function \(\Phi(\mathbf{s})\), such that \(r'(\mathbf{s}_t, \mathbf{a}_t) =r(\mathbf{s}_t, \mathbf{a}_t) + \Phi(\mathbf{s}_{t+1}) - \Phi(\mathbf{s}_{t})\). \end{definition}</p> <p>In \citet{ng1999policy}’s seminal work, the authors proved that two equivalent reward functions defined per \cref{def:equivalence} have the same optimal policy. By log-linearizing the optimal policy fixed point in \cref{eq:policy} and substituting in the Bellman equation from \cref{eq:critic}, we have</p> \[\begin{equation}\label{eq:advantage} \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{t+1}) - V^*(\mathbf{s}_{t}). \end{equation}\] <p>This is precisely the optimal advantage function, where \(V^*\) directly follows the form of a potential shaping function. As also noted by contemporary works, using the advantage as reward preserves the optimal policy \citep{knox2024learning, hejna2024contrastive}. Unlike prior works, however, we demonstrate that this re-parameterization also leads to the same exact preference distribution as \(r\).</p> <p>\begin{theorem}\label{theorem:equiv} Given a reference policy \(\pi_{\mathrm{ref}}\) and a parameter \(\beta&gt;0\) all reward classes consistent with the Plackett-Luce (and Bradley-Terry) models in \cref{eq:dense-bradley-terry} can be represented with the a re-parameterization of the form</p> \[\begin{equation}\label{eq:reward_param} r(\mathbf{s}, \mathbf{a}) = \beta \log \pi(\mathbf{s} \mid \mathbf{a}) - \beta \log \pi_{\mathrm{ref}}(\mathbf{s} \mid \mathbf{a}) \end{equation}\] <p>within the token MDP where \(V^*(\mathbf{s}_t) = 0\) for all terminal states. \end{theorem}</p> <p>\begin{proof} Above we derived the invariance of the optimal policy under the re-parameterization. The preference model can be shown to be invariant by following the same steps used to arrive at \cref{eq:policy_pref} in the last section. % as \(\sum_{t=0}^{N-1} \beta \log\frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\mathrm{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = \sum_{t=0}^{N-1} r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{N}) - V^*(\mathbf{s}_{0}).\) Assuming we reach a terminal state, \(V^*(\mathbf{s}_{N}) = 0\) and assuming all responses begin at the same state \(V^*(\mathbf{s}_{0})\) is cancelled from the preference model. \looseness=-1 \end{proof} Interestingly, in practice, the potential function \(\Phi(\mathbf{s}_t)\) represents the free parameter in the logits of the language model. An equal shift along all logits yields the same policy, but different Q-functions and corresponding rewards. The above Theorem proves that all of these are in the same equivalence class and induce the same set of preferences.</p> <p>Moreover, this Theorem implies that we can use DPO to learn the optimal policy for any per-token reward function, provided preference queries start at the same state and end at a terminal state. In addition, DPO \emph{always} fits an optimal advantage function for \emph{some} reward which is responsible for credit assignment. Thus, the training data determines how close the learned advantage corresponds to that of the true reward. This is in contrast to methods that estimate the reward function and then additionally employ some policy improvement mechanism. Which algorithm performs better remains largely an open or empirical question.</p> <p>The above derivations cast a language model as a Q function in the discrete token-level MDP. While this interpretation does not generally hold in continuous spaces, we can extend many of our results to other specially structured MDPs, like those present in diffusion. See Appendix \ref{appendix:diffusion} for more thorough treatment.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Runze Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 23, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-blogs",title:"Blogs",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-iclr",title:"ICLR",description:"A blog for Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters (arXiv 2408)",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-LLM-Test-Time/"}},{id:"post-iclr",title:"ICLR",description:"A blog for STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning (NeurIPS 22)",section:"Posts",handler:()=>{window.location.href="/blog/STaR/"}},{id:"post-rlhf-xpo",title:"RLHF - XPO",description:"A blog for Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF (ArXiv 2405.21046)",section:"Posts",handler:()=>{window.location.href="/blog/XPO/"}},{id:"post-rlhf-vpo",title:"RLHF - VPO",description:"A blog for Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF (ArXiv 2405.19320)",section:"Posts",handler:()=>{window.location.href="/blog/VPO/"}},{id:"post-rlhf-selm",title:"RLHF - SELM",description:"A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)",section:"Posts",handler:()=>{window.location.href="/blog/SELM/"}},{id:"post-rlhf-tdpo",title:"RLHF - TDPO",description:"A blog for Token-level Direct Preference Optimization (ICML 24)",section:"Posts",handler:()=>{window.location.href="/blog/TDPO/"}},{id:"post-rlhf-rto",title:"RLHF - RTO",description:"A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF",section:"Posts",handler:()=>{window.location.href="/blog/RTO/"}},{id:"post-rlhf-dpo-q",title:"RLHF - DPO-Q*",description:"A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)",section:"Posts",handler:()=>{window.location.href="/blog/DPO-Q/"}},{id:"post-rlhf-ipl",title:"RLHF - IPL",description:"A blog for Inverse Preference Learning: Preference-based RL without a Reward Function",section:"Posts",handler:()=>{window.location.href="/blog/IPL/"}},{id:"post-rlhf-cpl",title:"RLHF - CPL",description:"A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",section:"Posts",handler:()=>{window.location.href="/blog/CPL/"}},{id:"post-rlhf-dpo",title:"RLHF - DPO",description:"A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)",section:"Posts",handler:()=>{window.location.href="/blog/DPO/"}},{id:"post-rlhf-inverse-rlignment",title:"RLHF - Inverse-RLignment",description:"A blog for Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment",section:"Posts",handler:()=>{window.location.href="/blog/Inverse-RLignment/"}},{id:"post-rlhf",title:"RLHF -",description:"A blog for",section:"Posts",handler:()=>{window.location.href="/blog/template/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/tabs/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/code-diff/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/sidebar-table-of-contents/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/tables/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/distill/"}},{id:"news-sparkles-our-paper-meta-reward-net-implicitly-differentiable-reward-learning-for-preference-based-reinforcement-learning-is-accepted-by-neurips-2022",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning is...',description:"",section:"News"},{id:"news-i-guaduated-from-shandong-university-the-email-address-runzeliu-mail-sdu-edu-cn-is-no-longer-used-my-new-email-address-is-lrz23-mails-tsinghua-edu-cn",title:"I guaduated from Shandong University! The email address runzeliu@mail.sdu.edu.cn is no longer used....",description:"",section:"News"},{id:"news-sparkles-our-paper-seabo-a-simple-search-based-method-for-offline-imitation-learning-is-accepted-by-iclr-2024",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper SEABO: A Simple Search-Based Method for Offline Imitation Learning is...',description:"",section:"News"},{id:"news-sparkles-our-paper-pearl-zero-shot-cross-task-preference-alignment-and-robust-reward-learning-for-robotic-manipulation-is-accepted-by-icml-2024",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for...',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%72%7A%32%33@%6D%61%69%6C%73.%74%73%69%6E%67%68%75%61.%65%64%75.%63%6E","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0007-4784-5333","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=LiIfGakAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/rz-liu","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/235/0682","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
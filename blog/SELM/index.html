<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RLHF - SELM | Runze Liu </title> <meta name="author" content="Runze Liu"> <meta name="description" content="A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%91%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rz-liu.github.io/blog/SELM/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">mjx-container[jax="CHTML"][display="true"]{margin-top:0!important;margin-bottom:1em!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RLHF - SELM",
            "description": "A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)",
            "paper_url": "https://arxiv.org/abs/2405.19332",
            "published": "June 16, 2024",
            "authors": [
              
              {
                "author": "Runze Liu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Runze</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RLHF - SELM</h1> <p>A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332) <a href="https://arxiv.org/abs/2405.19332" class="paper-url" target="_blank" rel="noopener noreferrer">[URL]</a> </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#summary">Summary</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#key-idea">Key Idea</a> </li> <li> <a href="#contributions">Contributions</a> </li> </ul> <div> <a href="#preliminaries">Preliminaries</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#implementation">Implementation</a> </div> <div> <a href="#experiments">Experiments</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="summary">Summary</h2> <p>Code:</p> <p>这篇论文提出了 <d-cite key="SELM"></d-cite></p> <h3 id="motivation">Motivation</h3> <ul> <li>Online RLHF 可以主动迭代式探索 out-of-distribution (OOD) 区域，但是随机采样的方式可能会导致采样效率低下，高奖励的区域可能会被忽略，因此需要一种更加高效的采样方式。</li> <li> </li> </ul> <h3 id="key-idea">Key Idea</h3> <ul> <li>加一个 optimism bonus，鼓励 agent 探索未知区域，同时也能够保证 agent 在已知区域的收敛性。</li> </ul> <h3 id="contributions">Contributions</h3> <ol> <li> </li> <li> </li> <li> </li> </ol> <h2 id="preliminaries">Preliminaries</h2> <h3 id="large-language-models">Large Language Models</h3> <p>A language model \(\pi\in\Delta_\mathcal{Y}^\mathcal{X}\) typically takes the prompt \(x\in\mathcal{X}\) as input and outputs the response \(y\in\mathcal{Y}\). Here, \(\mathcal{X}\) and \(\mathcal{Y}\) are finite spaces of prompts and responses, respectively. Given the prompt \(x\in\mathcal{X}\), a discrete probability distribution \(\pi(\cdot \mid x)\in\Delta_\mathcal{Y}\) is generated, where \(\Delta_\mathcal{Y}\) is the set of discrete distributions over \(\mathcal{Y}\). Modern recipes for training LLMs consist of pre-training and post-training procedures, where during pre-training, LLMs learn to predict the next word on a huge and diverse dataset of text sequences in order to understand the underlying patterns and structures of natural language in an unsupervised manner. The post-training procedure aims to align better to end tasks and human preferences with two phases happening in order: Supervised Fine-Tuning (SFT) and human preference alignment. Here, SFT fine-tunes the pre-trained LLM with supervised learning on high-quality data to follow instructions on downstream tasks and obtain a model \(\pi^{\text{SFT}}\). In the following of this paper, we focus mainly on preference alignment.</p> <p>\subsection{Reward Modeling and Preference Optimization} \paragraph{Reinforcement Learning from Human Feedback (RLHF).} Standard RLHF frameworks consist of learning a reward model and then optimizing the LLM policy using the learned reward.</p> <p>Specifically, a point-wise reward \(r(x, y): \mathcal{X}\times\mathcal{Y}\rightarrow \mathcal{R}\) represents the Elo score \citep{elo1978rating} of the response \(y\) given the prompt \(x\). Then the preference distribution can be expressed by the Bradley-Terry model that distinguishes between the preferred response \(y_w\) and the dispreferred response \(y_l\) given prompt \(x\), denoted as \(y_w\succ y_l \mid x\), using the logistic function \(\sigma\):</p> \[\begin{equation}\label{bt_model} \begin{aligned} p(y_w\succ y_l \mid x) &amp;:= \mathbb{E}_{h}\bigl[\mathbb{1}(h \text{ prefers } y_w \text{ over } y_l \text{ given } x)\bigr] \notag\\ &amp;\,= \sigma\bigl(r(x, y_w) - r(x, y_l)\bigr) = \frac{\exp\bigl(r(x, y_w)\bigr)}{\exp\bigl(r(x, y_w)\bigr) + \exp\bigl(r(x, y_l)\bigr)}, \end{aligned} \end{equation}\] <p>where \(h\) denotes the human rater and the expectation is over \(h\) to account for the randomness of the choices of human raters we ask for their preference. When provided a static dataset of \(N\) comparisons \(\mathcal{D}=\{x_i, y_{w,i}, y_{l,i}\}_{i=1}^N\), the parameterized reward model can be learned by minimizing the following logistic regression loss:</p> \[\begin{equation}\label{lr_loss} \mathcal{L}_{\text{lr}}(r; \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\bigl[\log\sigma\bigl(r(x, y_w) - r(x, y_l)\bigr)\bigr]. \end{equation}\] <p>Using the learned reward, the LLM policy \(\pi\in\Delta_\mathcal{Y}^\mathcal{X}\) is optimized with reinforcement learning (RL) to maximize the expected reward while maintaining a small deviation from some base reference policy \(\pi_{\text{ref}}\), i.e., maximizing the following objective</p> \[\begin{equation}\label{rlhf_kl} \mathcal{J}(\pi; \mathcal{D}) = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi(\cdot \mid x)}\bigl[r(x, y)\bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}), \end{equation}\] <p>where \(\beta\) is a hyperparameter and \(\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}}) := \mathbb{E}_{x\sim\mathcal{D}} [\text{KL}(\pi(\cdot \mid x) \, \mid \mid \pi_{\text{ref}}(\cdot \mid x))]\) is the expected Kullback-Leibler (KL) divergence. An ideal \(\pi_{\text{ref}}\) is the policy that helps mitigate the distribution shift issue \citep{rafailov2024direct,guo2024direct} between the true preference distribution and the policy \(\pi\) during the off-policy RL training. Since we only have access to the dataset \(\mathcal{D}\) sampled from the unavailable true preference distribution, \(\pi_{\text{ref}}\) can be obtained by fine-tuning on the preferred responses in \(\mathcal{D}\) or simply setting \(\pi_{\text{ref}}=\pi^{\text{SFT}}\) and performing RLHF based on the SFT model.</p> <h3 id="direct-alignment-from-preference">Direct Alignment from Preference</h3> <p>With the motivation to get rid of a separate reward model, which is computationally costly to train, recent works \citep{rafailov2024direct,azar2023general,zhao2023slic,tunstall2023zephyr,ethayarajh2024kto} derived the preference loss as a function of the policy by changing of variables. Among them, DPO \citep{rafailov2024direct} shows that when the BT model in \eqref{bt_model} can perfectly fit the preference, the global optimizers of the RLHF objective in \eqref{rlhf_kl} and the following loss are equivalent:</p> \[\begin{equation} \mathcal{L}_{\text{DPO}}(\pi;\mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\biggl[\log\sigma\biggl(\beta\log\frac{\pi(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta\log\frac{\pi(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\biggr)\biggr]. \end{equation}\] <h2 id="method">Method</h2> <h3 id="rm-free-objective-for-active-exploration">RM-Free Objective for Active Exploration</h3> <p>\label{sec_der}</p> <p>In this section, we present several modifications to the optimistically biased objective \eqref{eq_intro} motivated in the introduction. Then we derive an RM-free objective for the LLM policy and analyze how active exploration works by examining its gradient.</p> <p>First, we consider the equivalence of \eqref{eq_intro}: \(\max_r-\mathcal{L}_{\text{lr}}(r; \mathcal{D}) + \alpha\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]\), where the inner \(\pi\) is deterministic when optimal. To account for the change of \(\pi\) relative to the reference policy \(\pi_{\text{ref}}\), we introduce two modifications: (1) replacing the optimistic bias term \(\max_\pi\mathbb{E}_{y\sim\pi}[r(x, y)]\) with \(\max_\pi\mathbb{E}_{y\sim\pi, y'\sim\pi_{\text{ref}}}[r(x, y)-r(x, y')]\), and (2) incorporating a KL-divergence loss term between \(\pi\) and \(\pi_{\text{ref}}\). These changes ensure that the resulting optimistic RM elicits responses with high potential unknown to the reference policy \(\pi_{\text{ref}}\) while minimizing the deviation between \(\pi\) and \(\pi_{\text{ref}}\).</p> <p>Formally, for the reward function \(r\), the bilevel optimization problem with optimism is formulated as:</p> \[\begin{equation}\label{our_obj} \max_r -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) +\alpha\max_\pi\Biggl(\underbrace{\mathbb{E}_{\substack{x\sim\mathcal{D}_t, y\sim\pi(\cdot \mid x)\\ y'\sim\pi_{\text{ref}}(\cdot \mid x)}}\Bigl[r(x, y) - r(x, y')\Bigr] - \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_{\text{ref}})}_{\mathcal{F}(\pi; r)}\Biggr), \end{equation}\] <p>where \(\mathcal{D}_t=\{x_i, y_{w,i}^t, y_{l,i}^t\}_{i=1}^N\) is the associated dataset at iteration \(t\) and \(\mathcal{L}_{\text{lr}}\) is the logistic regression loss defined in \eqref{lr_loss}. The nested optimization in \eqref{our_obj} can be handled by first solving the inner optimization \(\mathcal{F}(\pi; r)\) to obtain \(\pi_r\) that is optimal under \(r\). The solution is as follows and we defer all the derivations in this section to Appendix \ref{derivation_1}.</p> \[\begin{equation} \pi_r(y \mid x) := \mathop{\mathrm{argmax}}_\pi\mathcal{F}(\pi; r) = \frac{1}{Z(x)}\pi_{\text{ref}}(y \mid x)\exp\bigl(r(x, y) / \beta\bigr), \end{equation}\] <p>where the partition function \(Z(x) = \sum_y\pi_{\text{ref}}(y \mid x)\exp(r(x, y)/\beta)\). By substituting \(\pi=\pi_r\) into \(\mathcal{F}(\pi; r)\), we can rewrite the bilevel objective in \eqref{our_obj} as a single-level one:</p> \[\begin{equation} \max_r -\mathcal{L}_{\text{lr}}(r; \mathcal{D}_t) + \alpha\mathcal{F}(\pi_r; r). \end{equation}\] <p>Following the implicit reward formulation in DPO, we reparameterize the reward function with \(\theta\in\Theta\) as \(\hat{r}_\theta(x, y)=\beta(\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x))\), which is the optimal solution of \eqref{rlhf_kl} and can express \textit{all} reward classes consistent with the BT model as proved in \citep{rafailov2024direct}. With this change of variable, we obtain the RM-free objective for direct preference alignment with optimism:</p> \[\begin{equation}\label{final_obj} \max_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr]. \end{equation}\] <p>We now analyze how this new objective encourages active exploration. Specifically, we derive the gradient of \eqref{final_obj} with respect to \(\theta\) as</p> \[\begin{equation}\label{eq_grad} \begin{aligned} &amp;\underbrace{-\beta\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}_t}\Bigl[\sigma\bigl(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\bigr)\bigl(\nabla_\theta\log\pi_\theta(y_w \mid x) - \nabla_\theta\log\pi_\theta(y_l \mid x)\bigr)\Bigr]}_{\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)} \notag\\ &amp;\qquad\qquad\qquad\qquad\qquad\qquad - \alpha\beta\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_\theta(\cdot \mid x)}\bigl[\exp\bigl(-\hat{r}_\theta(x, y)/\beta\bigr)\nabla_\theta\log\pi_\theta(y \mid x)\bigr]. \end{aligned} \end{equation}\] <p>We note that the second line, corresponding to the gradient of the optimism term, decreases the log-likelihood of response \(y\) generated by \(\pi_\theta\) that has a low value of \(\exp(-\hat{r}_\theta(x, y)/\beta)\). Therefore, the added optimism term biases the gradient toward parameter regions that can elicit responses \(y\) with high implicit reward \(\hat{r}_\theta\), consistent with our intuition outlined in Figure \ref{urm_illu}.</p> <p>This also explains why \(\mathbb{E}_{\pi_{\text{ref}}}[\log\pi_\theta]\) is minimized in our objective \eqref{final_obj}, which is equivalent to maximizing the KL divergence between \(\pi_{\text{ref}}\) and \(\pi_\theta\), while the reverse KL in the policy optimization objective \eqref{rlhf_kl} is minimized. For the DPO gradient \(\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t)\), the degree of deviation of policy \(\pi_\theta\) from \(\pi_{\text{ref}}\) only affects the preference estimated with \(\hat{r}_\theta\). In other words, \(\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))\) is a scalar value and the policy deviation only determines the \textit{step size} of the policy gradient, instead of its \textit{direction}. On the other hand, our added exploration term directly controls the direction of the gradient toward potentially more rewarding areas while still fitting the preference data in \(\mathcal{D}_t\). As more feedback data is collected iteratively, deviating from the unbiasedly fitted model incurs a higher DPO loss, which ultimately dominates our objective at convergence. This mechanism ensures that the resulting LLM effectively balances between exploring novel responses and exploiting previously observed ones, leading to a more accurate and aligned model.</p> <h3 id="algorithm">Algorithm</h3> <p>\label{sec_algo}</p> <p>With the optimistically biased objective derived above, the language model can actively generate OOD responses worth exploring. Human or AI feedback follows to reduce the uncertainty in these regions. These two steps are executed iteratively to get a more and more aligned model.</p> <p>In practice, we split the offline preference dataset into three portions with equal sizes, one for each iteration. Besides, we use AI rankers, such as external RMs, to provide feedback on the model-generated response and the original chosen, rejected responses. The complete pseudocode of our algorithm, named \textit{Self-Exploring Language Models} (SELM), is outlined in Algorithm \ref{alg_se}. \begin{algorithm}[H] \caption{Self-Exploring Language Models (SELM)} \begin{algorithmic}[1]\label{alg_se} \REQUIRE Reference model \(\pi_{\text{ref}}\), preference dataset \(\mathcal{D}\), online iterations \(T\), optimism coefficient \(\alpha\). \FOR{iteration \(t = 1, 2, \ldots, T\)} \STATE Set \(\mathcal{D}_{t}\) as the \(t\)-th portion of \(\mathcal{D}\) and generate \(y\sim\pi_{\text{ref}}(\cdot \mid x)\) for each prompt \(x\) in \(\mathcal{D}_t\). \STATE Rank \(\{y, y_w, y_l\}\) and update \(\mathcal{D}_t\) to contain the best (chosen) and worst (rejected) responses. \STATE Train the LLM \(\pi_{\theta_t} = \mathop{\mathrm{argmax}}_{\pi_\theta} -\mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_t) - \alpha \mathbb{E}_{x\sim\mathcal{D}_t}[\log\pi_{\theta}(y \mid x)]\) and let \(\pi_{\text{ref}}=\pi_{\theta_t}\). \ENDFOR \end{algorithmic} \end{algorithm}</p> <p>\subsection{Self-Exploration Reduces Indiscriminate Favor of Unseen Extrapolations} It has been observed recently \citep{rafailov2024r,pal2024smaug,xu2024dpo} that DPO decreases the likelihood of responses generated by the reference policy. It is because for any prompt \(x\), at convergence when \(\pi_\theta \neq \pi_{\text{ref}}\), it holds that</p> \[\begin{equation} \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\hat{r}_\theta(x, y)/\beta\bigr] = \mathbb{E}_{y\sim\pi_{\text{ref}}}\bigl[\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x)\bigr] = -\text{KL}\bigl(\pi_{\text{ref}}(\cdot \mid x) \mid \mid \pi_\theta(\cdot \mid x)\bigr) &lt; 0, \end{equation}\] <p>while at the beginning of training when \(\pi_\theta = \pi_{\text{ref}}\), the above terms are zero. Thus, the expected implicit reward \(\hat{r}_\theta\) as well as the likelihood of \(\pi_\theta\) will decrease on the reference model’s responses. This indicates that DPO stimulates a biased distribution favoring unseen extrapolated responses. In the online iterative setting that we consider, the LLM policy generates responses and receives preference feedback alternately, where biasing towards OOD regions may sometimes help discover outstanding novel responses. However, DPO \textit{indiscriminately} favors unseen extrapolations and \textit{passively} explores based purely on the randomness inherent in sampling from the LLM. As a consequence, the vast space of natural language makes it almost impossible to exhaustively explore all the possible responses and identify those that most effectively benefit alignment.</p> <p>Next, we demonstrate that SELM mitigates this issue by performing guided exploration. Specifically, consider the proposed self-exploration objective in \eqref{final_obj}, which, in addition to the standard DPO loss, also minimizes \(\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]\). We now investigate how the probability distribution changes with this term incorporated. \begin{theorem} \label{thm} For any \(\rho\in\Theta\) in the policy parameter space, let \(\hat{r}_\rho(x, y) = \beta(\log\pi_\rho(y \mid x) - \log\pi_{\text{ref}}(y \mid x))\) be the reparameterized implicit reward. Denote \(\pi^{\min}_\rho\) as the policy that minimizes the expected implicit reward under the KL constraint, i.e.,</p> \[\begin{equation}\label{eq_pi_rho} \pi^{\min}_\rho(\cdot \mid x) := \mathop{\mathrm{argmin}}_\pi\mathbb{E}_{x, y\sim\pi(\cdot \mid x)}\bigl[\hat{r}_\rho(x, y)\bigr] + \beta\mathbb{D}_{\text{KL}}(\pi \mid \mid \pi_\rho). \end{equation}\] <p>Then minimizing \(\mathbb{E}_{x, y\sim\pi_{\text{ref}}}[\log\pi_\theta(y \mid x)]\) decreases the likelihood of responses sampled from \(\pi^{\min}_\rho\):</p> \[\begin{equation} \min_{\pi_\theta}\mathbb{E}_{x, y\sim\pi_{\text{ref}}(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr] = \min_{\pi_\theta}\mathbb{E}_{x,y\sim\pi^{\min}_\rho(\cdot \mid x)}\bigl[\log\pi_\theta(y \mid x)\bigr]. \end{equation}\] <p>\end{theorem}</p> <p>The above theorem states that maximizing the divergence between \(\pi_\theta\) and \(\pi_{\text{ref}}\) is essentially reducing the probability of generating responses with low implicit rewards reparameterized by any policy parameter \(\rho\) during training. In other words, the policy not only exploits the existing preference data but also learns to avoid generating the text \(y\) that is assigned a low reward value. This process occurs in every iteration with updated reference models. Consequently, responses with high potential rewards are selectively preferred and many commonplace responses receive a small probability mass, thus mitigating the indiscriminate favoring of unseen responses and improving exploration efficiency.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> <h2 id="experiments">Experiments</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Runze Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 23, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
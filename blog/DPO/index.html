<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RLHF - DPO | Runze Liu </title> <meta name="author" content="Runze Liu"> <meta name="description" content="A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%91%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rz-liu.github.io/blog/DPO/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">mjx-container[jax="CHTML"][display="true"]{margin-top:0!important;margin-bottom:1em!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RLHF - DPO",
            "description": "A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)",
            "paper_url": "",
            "published": "June 04, 2024",
            "authors": [
              
              {
                "author": "Runze Liu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Tsinghua University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Runze</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RLHF - DPO</h1> <p>A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23) </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#summary">Summary</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#key-idea">Key Idea</a> </li> <li> <a href="#contributions">Contributions</a> </li> </ul> <div> <a href="#preliminaries">Preliminaries</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#implementation">Implementation</a> </div> <div> <a href="#experiments">Experiments</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="summary">Summary</h2> <p>这篇论文提出了 Direct Preference Optimization (DPO) <d-cite key="DPO"></d-cite>，使用偏好数据直接优化策略，而不需要先学奖励模型再学策略。</p> <h3 id="motivation">Motivation</h3> <ul> <li>之前的方法流程复杂，先学 reward model，再学 policy，效率低</li> <li>RL (PPO) 难以训练</li> </ul> <h3 id="key-idea">Key Idea</h3> <h3 id="contributions">Contributions</h3> <ol> <li>D</li> <li>D</li> <li>D</li> </ol> <h2 id="preliminaries">Preliminaries</h2> <h3 id="dataset">Dataset</h3> <h3 id="sft">SFT</h3> <p>Supervised Fine-Tuning (SFT) 用于在预训练模型上进行微调，使模型在下游任务上表现更好（例如：对话、总结等）。在 SFT 阶段，我们使用一个包含 prompt \(x\) 和高质量回答 \(y\) 的数据集，对模型进行微调，使得模型在 prompt \(x\) 上生成的回答接近 \(y\)，最终得到模型 \(\pi_{\text{SFT}}\)。</p> <h3 id="奖励建模">奖励建模</h3> <p>对于一个输入 \(x\)，经过 SFT 的模型可以产生成对的回答 \((y_1, y_2) \sim \pi_{\text{SFT}}(y \mid x)\)。我们使用 \(y_w\) 和 \(y_l\) 分别表示 \((y_1, y_2)\) 中更好的回答和更差的回答，则偏好关系可以定义为 \(y_w \succ y_l \mid x\)。假设 ground-truth reward function 为 \(r^*(x, y)\)，按照 RLHF 的传统，使用 Bradley-Terry model <d-cite key="BTModel"></d-cite> 建模偏好，human preference distribution \(p^*\) 可以表示为</p> \[\begin{equation}\label{eq:bradley-terry} p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}. \end{equation}\] <p>假设我们有一个偏好数据集 \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\)，其中 \(y_w^{(i)} \succ y_l^{(i)} \mid x^{(i)}\)，我们可以将 reward learning 建模为一个二分类问题：</p> \[\begin{equation}\label{eq:reward_model} \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr] \end{equation}\] <h3 id="rl-fine-tuning">RL Fine-Tuning</h3> <p>使用 learned reward function 进行 fine-tuning，使得模型在下游任务上表现更好。</p> \[\begin{equation}\label{eq:RL} \max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)} \bigl[r_{\phi}(x, y)\bigr] - \beta \mathbb{D}_{\textrm{KL}} \bigl[\pi_{\theta}(y \mid x) \mid \mid \pi_{\text{ref}}(y \mid x)\bigr] \end{equation}\] <h2 id="method">Method</h2> <p>受到在大规模问题上应用强化学习算法的挑战的启发，我们的目标是推导一种直接使用偏好数据进行策略优化的简单方法。与以前的 RLHF 方法不同，这些方法首先学习奖励，然后通过 RL 进行优化，我们的方法绕过了奖励建模步骤，直接使用偏好数据优化语言模型。我们的 <strong>key insight</strong> 是利用从奖励函数到最优策略的分析映射，这使我们能够将对奖励函数的损失函数转换为对策略的损失函数。这种变量变换方法允许我们跳过显式奖励建模步骤，同时仍然在现有的人类偏好模型下进行优化，例如 Bradley-Terry 模型。本质上，策略网络既代表语言模型，也代表隐式奖励。</p> <h3 id="deriving-the-dpo-objective">Deriving the DPO objective</h3> <p>我们从与以前的工作相同的 RL 目标 \eqref{eq:RL} 开始，其中奖励函数 \(r\) 是一个通用的函数。接下来推导KL 约束奖励最大化目标的最优解 (KL-constrained reward maximization objective) 的最优解。对于 \eqref{eq:RL}，我们可以将其重写为：</p> \[\begin{equation}\label{eq:RL_proof} \begin{aligned} &amp; \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(y \mid x)} \Big[ r(x, y) \Big] - \beta \mathbb{D}_{\textrm{KL}} \Big[ \pi(y \mid x) \mid\mid \pi_{\text{ref}}(y \mid x) \Big] \\ =&amp; \max_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ r(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} - \frac{1}{\beta} r(x, y) \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right)} - \log Z(x) \right], \end{aligned} \end{equation}\] <p>其中</p> \[\begin{equation} Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>可以看出 partition function \(Z(x)\) 只与 \(x\) 和 \(\pi_{\text{ref}}\) 有关，而与 \(\pi\) 无关。我们定义：</p> \[\begin{equation} \pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(\pi^*(y \mid x)\) 满足 \(\pi^*(y \mid x) \ge 0\) 且 \(\sum_y \pi^*(y \mid x) = 1\)。由于 \(Z(x)\) 不是 \(y\) 的函数，我们可以将 \eqref{eq:RL_proof} 重写为：</p> \[\begin{equation} \begin{aligned} &amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log\frac{\pi(y \mid x)}{\pi^*(y \mid x)} \right] - \log Z(x) \right] \\ =&amp; \min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \Big[ \mathbb{D}_{\text{KL}}(\pi(y \mid x) \mid\mid \pi^*(y \mid x)) - \log Z(x) \Big] \end{aligned} \end{equation}\] <p>由于 \(Z(x)\) 不依赖于 \(\pi\)，上式的最小值在 KL 散度最小时取得，根据 Gibbs’ inequality，当 \(\pi = \pi^*\) 时，KL 散度为 \(0\)。因此，KL 约束奖励最大化目标的最优解形式为：</p> \[\begin{equation}\label{eq:op_policy} \pi_r(y\mid x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right), \end{equation}\] <p>其中 \(Z(x)\) 是 partition function。即使我们使用奖励函数 \(r_{\phi}\) 的 MLE 估计值，估计 partition function \(Z(x)\) 仍然是昂贵的~\citep{korbak2022reinforcement, go2023aligning}，这使得这种表示在实践中难以利用。然而，我们可以重新排列 \eqref{eq:op_policy} 以将奖励函数表示为其对应的最优策略 \(\pi_r\)、参考策略 \(\pi_{\text{ref}}\) 和未知的 partition function \(Z(\cdot)\)。具体来说，我们首先对 \eqref{eq:op_policy} 两边取对数，然后通过一些代数运算，我们得到：</p> \[\begin{equation}\label{eq:main_eq} r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \beta \log Z(x). \end{equation}\] <p>我们可以将这种重新参数化应用于 ground-truth reward \(r^*\) 和相应的最优策略 \(\pi^*\)。幸运的是，Bradley-Terry 模型仅取决于两个回答之间的奖励差异，即 \(p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))\)。将 \eqref{eq:main_eq} 中的重新参数化代入到 \(r^*(x,y)\) 的偏好模型 \eqref{eq:bradley-terry} 中：</p> \[\begin{equation} \begin{aligned} p^*(y_1 \succ y_2 \mid x) &amp;= \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right)}{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} + \beta \log Z(x)\right)} \\ &amp;= \frac{1}{1+\exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}-\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)} \\ &amp;= \sigma \left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right), \end{aligned} \end{equation}\] <p>其中 \(\sigma(z) = 1/(1+\exp(-z))\) 是 sigmoid 函数。通过上式，partition function 消除了，我们可以将人类偏好概率表示为仅关于最优策略 \(\pi^*\) 和参考策略 \(\pi_{\text{ref}}\)。因此，Bradley-Terry 模型下的最优 RLHF 策略 \(\pi^*\) 满足偏好模型：</p> \[\begin{equation}\label{eq:objective} p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi_{\text{ref}}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi_{\text{ref}}(y_1\mid x)}\right)} \end{equation}\] <p>虽然 \eqref{eq:objective} 使用了 BT 模型，我们可以类似地推导出 Plackett-Luce 模型 <d-cite key="plackett1975analysis, luce2005individual"></d-cite> 的表达式，详见附录。</p> <p>注意到我们现在可以将人类偏好数据的概率表示为最优策略而不是奖励模型，我们可以为参数化策略 \(\pi_{\theta}\) 制定一个最大似然目标。类似于奖励建模方法（即 \eqref{eq:reward_model}），我们的策略目标变为：</p> \[\begin{equation}\label{eq:optimum_model} \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}\right)\right]. \end{equation}\] <p>这样，我们既绕过了显式奖励建模步骤，又避免了执行强化学习优化。此外，由于我们的过程等价于拟合重新参数化的 Bradley-Terry 模型，因此它具有某些理论性质，例如在适当假设下，偏好数据分布的一致性~\citep{bong2022generalized}。</p> <h3 id="what-does-the-dpo-update-do">What does the DPO update do?</h3> <p>为了理解 DPO 的机制，我们可以分析损失函数 \(\mathcal{L}_\text{DPO}\) 的梯度。对于参数 \(\theta\) 的梯度可以写成：</p> \[\begin{equation}\label{eq:gradient} \begin{aligned} &amp; \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi_{\text{ref}}) = \\ &amp;-\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[ \underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong} \bigg[ \underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$} \bigg] \bigg], \end{aligned} \end{equation}\] <p>其中，\(\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\) 是由语言模型 \(\pi_\theta\) 和参考模型 \(\pi_{\text{ref}}\) 隐式定义的奖励。直观地说，损失函数 \(\mathcal{L}_\text{DPO}\) 的梯度增加了更好回答 \(y_w\) 的可能性，降低了更差回答 \(y_l\) 的可能性。重要的是，这些样本的权重取决于隐式奖励模型 \(\hat{r}_\theta\) 如何评价更差的回答，由 \(\beta\) 缩放，即隐式奖励模型对回答的排序有多么不正确，考虑到 KL 约束的强度。我们的实验表明这种加权的重要性，因为没有加权系数的这种方法可能会导致语言模型退化。</p> <h3 id="dpo-outline">DPO outline</h3> <p>DPO 的一般流程如下：1) 为每个 prompt \(x\) 采样完成 \(y_1, y_2 \sim \pi_{\text{ref}}(\cdot \mid x)\)，并使用人类偏好标记构建偏好离线数据集 \(\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N\)；2) 为给定的 \(\pi_{\text{ref}}\) 和 \(\mathcal{D}\) 以及所需的 \(\beta\)，优化语言模型 \(\pi_\theta\) 以最小化 \(\mathcal{L}_\text{DPO}\)。在实践中，我们希望重用公开可用的偏好数据集，而不是生成样本并收集人类偏好。由于偏好数据集是使用 \(\pi_{\text{ref}}\) 进行采样的，我们在可能的情况下初始化 \(\pi_{\text{ref}} = \pi_{\text{SFT}}\)。然而，当 \(\pi_{\text{SFT}}\) 不可用时，我们通过最大化更好回答的似然来初始化 \(\pi_{\text{ref}}\)，即 \(\pi_{\text{ref}} = \mathop{\mathrm{argmax}}_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]\)。这个过程有助于减轻真实参考分布和 DPO 使用的 \(\pi_{\text{ref}}\) 之间的分布偏移。</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">def</span> <span class="nf">dpo_loss</span><span class="p">(</span><span class="n">pi_logps</span><span class="p">,</span> <span class="n">ref_logps</span><span class="p">,</span> <span class="n">yw_idxs</span><span class="p">,</span> <span class="n">yl_idxs</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    pi_logps: policy logprobs, shape (B,)
    ref_logps: reference model logprobs, shape (B,)
    yw_idxs: preferred completion indices in [0, B-1], shape (T,)
    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
    beta: temperature controlling strength of KL penalty

    Each pair of (yw_idxs[i], yl_idxs[i]) represents the
      indices of a single preference pair.
    </span><span class="sh">"""</span>

    <span class="n">pi_yw_logps</span><span class="p">,</span> <span class="n">pi_yl_logps</span> <span class="o">=</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>
    <span class="n">ref_yw_logps</span><span class="p">,</span> <span class="n">ref_yl_logps</span> <span class="o">=</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>

    <span class="n">pi_logratios</span> <span class="o">=</span> <span class="n">pi_yw_logps</span> <span class="o">-</span> <span class="n">pi_yl_logps</span>
    <span class="n">ref_logratios</span> <span class="o">=</span> <span class="n">ref_yw_logps</span> <span class="o">-</span> <span class="n">ref_yl_logps</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">logsigmoid</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logratios</span> <span class="o">-</span> <span class="n">ref_logratios</span><span class="p">))</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logps</span> <span class="o">-</span> <span class="n">ref_logps</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">rewards</span>
</code></pre></div></div> <h2 id="experiments">Experiments</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Runze Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 23, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-blogs",title:"Blogs",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-iclr",title:"ICLR",description:"A blog for Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters (arXiv 2408)",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-LLM-Test-Time/"}},{id:"post-iclr",title:"ICLR",description:"A blog for STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning (NeurIPS 22)",section:"Posts",handler:()=>{window.location.href="/blog/STaR/"}},{id:"post-rlhf-xpo",title:"RLHF - XPO",description:"A blog for Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF (ArXiv 2405.21046)",section:"Posts",handler:()=>{window.location.href="/blog/XPO/"}},{id:"post-rlhf-vpo",title:"RLHF - VPO",description:"A blog for Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF (ArXiv 2405.19320)",section:"Posts",handler:()=>{window.location.href="/blog/VPO/"}},{id:"post-rlhf-selm",title:"RLHF - SELM",description:"A blog for Self-Exploring Language Models: Active Preference Elicitation for Online Alignment (ArXiv 2405.19332)",section:"Posts",handler:()=>{window.location.href="/blog/SELM/"}},{id:"post-rlhf-tdpo",title:"RLHF - TDPO",description:"A blog for Token-level Direct Preference Optimization (ICML 24)",section:"Posts",handler:()=>{window.location.href="/blog/TDPO/"}},{id:"post-rlhf-rto",title:"RLHF - RTO",description:"A blog for DPO Meets PPO: Reinforced Token Optimization for RLHF",section:"Posts",handler:()=>{window.location.href="/blog/RTO/"}},{id:"post-rlhf-dpo-q",title:"RLHF - DPO-Q*",description:"A blog for From r to Q*: Your Language Model is Secretly a Q-Function (ArXiv)",section:"Posts",handler:()=>{window.location.href="/blog/DPO-Q/"}},{id:"post-rlhf-ipl",title:"RLHF - IPL",description:"A blog for Inverse Preference Learning: Preference-based RL without a Reward Function",section:"Posts",handler:()=>{window.location.href="/blog/IPL/"}},{id:"post-rlhf-cpl",title:"RLHF - CPL",description:"A blog for Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",section:"Posts",handler:()=>{window.location.href="/blog/CPL/"}},{id:"post-rlhf-dpo",title:"RLHF - DPO",description:"A blog for Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 23)",section:"Posts",handler:()=>{window.location.href="/blog/DPO/"}},{id:"post-rlhf-inverse-rlignment",title:"RLHF - Inverse-RLignment",description:"A blog for Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment",section:"Posts",handler:()=>{window.location.href="/blog/Inverse-RLignment/"}},{id:"post-rlhf",title:"RLHF -",description:"A blog for",section:"Posts",handler:()=>{window.location.href="/blog/template/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/tabs/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/code-diff/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/sidebar-table-of-contents/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/tables/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/distill/"}},{id:"news-sparkles-our-paper-meta-reward-net-implicitly-differentiable-reward-learning-for-preference-based-reinforcement-learning-is-accepted-by-neurips-2022",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning is...',description:"",section:"News"},{id:"news-i-guaduated-from-shandong-university-the-email-address-runzeliu-mail-sdu-edu-cn-is-no-longer-used-my-new-email-address-is-lrz23-mails-tsinghua-edu-cn",title:"I guaduated from Shandong University! The email address runzeliu@mail.sdu.edu.cn is no longer used....",description:"",section:"News"},{id:"news-sparkles-our-paper-seabo-a-simple-search-based-method-for-offline-imitation-learning-is-accepted-by-iclr-2024",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper SEABO: A Simple Search-Based Method for Offline Imitation Learning is...',description:"",section:"News"},{id:"news-sparkles-our-paper-pearl-zero-shot-cross-task-preference-alignment-and-robust-reward-learning-for-robotic-manipulation-is-accepted-by-icml-2024",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> Our paper PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for...',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%72%7A%32%33@%6D%61%69%6C%73.%74%73%69%6E%67%68%75%61.%65%64%75.%63%6E","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0007-4784-5333","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=LiIfGakAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/rz-liu","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/235/0682","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>